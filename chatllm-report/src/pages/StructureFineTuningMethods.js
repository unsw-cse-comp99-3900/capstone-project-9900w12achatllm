
import React from 'react';
import { Box, Heading, Text, Image } from '@chakra-ui/react';

const LiteratureReviewFineTuning = () => {
  return (
    <Box p={4}>
      <Heading as="h1" size="xl" mb={6}>
        The Structure of Fine-tuning Methods
      </Heading>
      <Text fontSize="lg" mb={4} textAlign="justify">
        The effectiveness of large language models is not only due to their complex structure and large parameters, but also relies on the fine-tuning strategies employed (Wolf, Thomas et al., 2019). By fine-tuning the model for a specific task or domain, it can be made better adapted to the specific needs of the application, which in turn improves its performance in real-world tasks. Fine-tuning large pre-trained Transformer-based language models (LLMs) has become a common practice for handling natural language processing (NLP) tasks. While modern pre-trained language models perform well in terms of transfer learning (e.g., zero-sample inference capability), fine-tuning on specific downstream datasets can still significantly enhance the performance and generalisation of the model. Performing full-parameter fine-tuning, i.e., tuning all parameters of the model, can further optimise performance.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        As model sizes increase, it has become impractical to perform full parameter fine-tuning on common consumer-grade hardware, which can lead to loss of model diversity and catastrophic forgetting problems. Therefore, finding efficient model fine-tuning methods has become a hot research topic in the industry and has driven the rapid development of Parametric Efficient Fine-Tuning Technique (PEFT). PEFT significantly reduces computation and storage requirements by fine-tuning only a small number of or additional parameters while fixing most of the pre-trained model's parameters while achieving similar or even better results than full-parameter fine-tuning. The PEFT technique is mainly divided into three approaches: adding additional parameters (A), using selective methods (S) and using reparameterization techniques (R).
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        There are two main types of Additional parameters (A): adapter-like methods and soft prompt methods. The adapter-like method adds a small adapter layer within the Transformer block. Such as Adapter Tuning method (Houlsby et al., 2019) adds adapter structures after the multi-head attention projection and after the second feed-forward layer in each transformer layer and adjusts only these added adapters and the layer normalisation layer during training, keeping the original model parameters unchanged. This strategy achieves similar results to full parameter fine-tuning by increasing the number of parameters by 0.5% to 5%. Adapter Fusion (Pfeiffer et al., 2020) is designed to optimise performance by combining the parameters of multitasking adapters and solves the problem by dividing training into two phases, knowledge extraction and knowledge combination for the multitasking problem, but the addition of adapters also increases the total number of parameters in the model, which may affect inference performance. Adapter Drop (Rücklé et al., 2020) is an innovative technique designed to increase the computational speed of models by dynamically removing adapters during the inference process. Another technique to improve model efficiency is Soft Prompt, which enables efficient parameter fine-tuning by adding learnable vectors at the beginning of the input sequence. For example, in the Prefix Tuning (Li & Leung, 2021), the system constructs task-related virtual token segments as prefixes before the input tokens. In this setup, the training process only involves updating the parameters of the prefix segment, while keeping the parameters of the rest of the pre-trained language model (PLM) unchanged. This approach not only effectively reduces the training burden, but also ensures the flexibility and adaptability of the model, allowing it to quickly adapt to new tasks without retraining the entire model. The Power of Scale for Parameter Efficient Prompt Tuning (Lester, Al-Rfou & Constant, 2021) is a simplified version of Prefix Tuning, where specific prompts are defined for each task as part of the input data. This method simplifies the training process by adding cue tokens only at the input layer, without the need to tune the MLP, and the effectiveness of cue tuning progressively increases as the parameters of the pre-trained model increases, the effect of prompt Tuning gradually approaches that of full parameter fine-tuning.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Selective methods (S) optimize the model by freezing a portion of the parameters and updating only the parameters that need to be adjusted to better adapt to a specific task and maintain performance. This strategy can be mainly divided into two categories: unstructured and structured masking. Unstructured masking selects the parameters that need to be fine-tuned by adding a learnable binary mask to the model parameters. For example, Diff Pruning (Guo, Rush, Kim, 2020) adjusts the pre-trained parameters by creating a task-specific "diff" vector and dynamically prunes the diff vector using fine-tuning with an L0 norm penalty to promote parameter sparsity. This method does not need to process all tasks during training, but only needs to store small difference vectors for each task, which makes it practical for industrial applications and has shown comparable performance to full parameter fine-tuning on the GLUE benchmark. Another method, such as transformer-based BitFit (Zaken et al., 2021), is a sparse fine-tuning technique that only updates the bias parameters during training to improve parameter efficiency.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Reparameterisation fine-tuning (R) consists of methods to optimise the entire parameter space using various low-dimensional reparameterised representations of the pre-trained weight matrix. During the training phase, in order to reduce the additional delay, these parameters modified during training can be efficiently transformed and integrated back into the original parameter structure of the pre-trained model. Taking Low Rank Adaptive (LoRA) (Hu et al., 2021) as an example, the core idea is to simulate changes to model parameters by means of a low-rank decomposition technique, to train large models indirectly at a small computational cost. Specifically, a low-rank representation of the original model parameters is constructed by introducing projection matrices. These projection matrices are adjusted during the training process, while after training is completed, these additional parameters can be seamlessly integrated into the original model weights, thus significantly improving the inference efficiency without sacrificing the model performance. Adaptive Budget Allocation for Efficient Parameter Fine-tuning (AdaLoRA) (Zhang et al., 2023) improves LoRA by dynamically allocating parameter budgets to weight matrices based on importance scores. This method adjusts the incremental updates through singular value decomposition, prunes unimportant singular values to improve computational speed, and adds additional penalty terms to the training loss to stabilise the training process. Quantized Large Language Models for Efficient Fine-tuning (QLoRA) (Dettmers et al., 2024) is an innovative approach that achieves optimization by quantizing the pre-trained model to 4-bit values ​​with high-precision techniques. On this basis, QLoRA further adds a set of learnable low-rank adapter weights specifically for fine-tuning the model. To ensure high fidelity during this process, QLoRA uses two advanced techniques for 4-bit fine-tuning, which helps to accurately adjust the model without losing performance. In addition, it introduces a new paging optimizer technology that effectively prevents memory spikes that may occur during the gradient checkpoint process, thereby avoiding memory overflow errors. The application of these techniques not only improves the operating efficiency of the model, but also ensures that the model performance and stability are maintained under strict resource constraints.
      </Text>


      <Image src="pictures/report_pic_5.png" alt="An illustration of main components of the transformer model from the ‘Attention is All You Need’ paper" mb={4} />
    </Box>
  );
};

export default LiteratureReviewFineTuning;
