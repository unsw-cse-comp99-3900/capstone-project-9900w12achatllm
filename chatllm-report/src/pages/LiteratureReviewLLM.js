// src/pages/LiteratureReviewLLM.js
import React from 'react';
import { Box, Heading, Text, Image } from '@chakra-ui/react';

const LiteratureReviewLLM = () => {
  return (
    <Box p={4}>
      <Heading as="h1" size="xl" mb={6}>
        Literature Review of Large Language Models
      </Heading>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Large Language Models (LLMs) are large-scale neural networks trained based on deep learning techniques, particularly the transformer architecture, designed to process and generate natural language text. The transformer architecture is a deep learning model proposed by Vaswani et al. in 2017 (Vaswani et al, 2017) and is designed to capture the relationships between elements in a sequence through a method called self-attention mechanism. The transformer architecture can be divided into three main types: Encoder-Only architecture, Encoder-Decoder architecture and Decoder-Only architecture.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        The encoder is a stack of multiple identical encoder layers, each encoder layer includes a multi-head self-attention sublayer and a feed-forward neural network sublayer, which encodes the input sequence into a fixed-length context vector. The decoder also consists of multiple identical decoder layers stacked on top of each other, each of which consists of a multi-head self-attention sublayer, a multi-head attention sublayer interacting with the encoder, and a feed-forward neural network sublayer, which decodes the context vectors generated by the encoder into a target sequence.
      </Text>
      <Image src="pictures/report_pic_3.png" alt="An illustration of main components of the transformer model from the ‘Attention is All You Need’ paper" mb={4} />
      <Text fontSize="lg" mb={4} textAlign="justify">
        Encoder-Decoder architecture is typically used for tasks that require converting one sequence to another, is usually trained using supervised learning, i.e., given an input sequence and a corresponding target sequence, the model is trained by minimizing the gap between the predicted sequence and the target sequence. In the inference phase, the input sequence is passed through the encoder to obtain a context vector, which is then used by the decoder to generate the target sequence, such as earlier seq2seq models and ChatGLM (Zeng et al, 2022) are used this architecture.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Encoder-Only contains only the encoder part and not the decoder part. Encoder-Only is mainly used to handle tasks that require only encoding the input sequence without generating the output sequence, such as text categorization, sentiment analysis, semantic representation, etc. The training process is also usually a supervised learning approach, where the model is trained by minimizing the gap between the model output and the labels. In the inference phase, the input sequence is processed by the encoder to produce a fixed-dimension context vector that contains information about the input sequence, the context vector can be used directly as inputs for subsequent tasks, such as BERT（Bidirectional Encoder Representations from Transformers）is used this architecture (Devlin et al, 2018).
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Decoder-Only architecture which contains only the decoder part is typically used for generative tasks such as text generation, language modeling, etc. The training of Decoder-Only is also usually supervised learning, i.e., given an input sequence and a corresponding target sequence, the model is trained by minimizing the gap between the predicted sequence and the target sequence. In the inference phase, the model accepts a fixed-length vector as input, and then generates the output sequence step by step through the decoder. During the process of generating the sequence, the model can use part of the sequence generated in the previous step to help generate the next unit. For example, Llama2 (Hugo et al, 2023), Qwen2 (Jinze Bai et al, 2023) and a series of GPT (Brown, 2020) are used Decoder-Only architecture.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Therefore, because it does not rely on sequence order, the Transformer architecture enables efficient parallelization during training and inference, greatly improving computational efficiency, and the self-attention mechanism enables the model to efficiently capture the relationships between distant elements in the sequence, and more effectively process long texts. Encoder-Only and Decoder-Only more simplifies the structure of the model and achieves better performance in certain generative tasks.
      </Text>
      <Image src="pictures/report_pic_4.jpg" alt="An illustration of main components of the transformer model from the ‘Attention is All You Need’ paper" mb={4} />
    </Box>
  );
};

export default LiteratureReviewLLM;
