// src/pages/LiteratureReviewFineTuning.js
import React from 'react';
import { Box, Heading, Text, Image } from '@chakra-ui/react';

const LiteratureReviewFineTuning = () => {
  return (
    <Box p={4}>
      <Heading as="h1" size="xl" mb={6}>
        Literature Review of Fine-Tuning Methods
      </Heading>
      <Text fontSize="lg" mb={4} textAlign="justify">
        The success of large language models depends not only on their complex architectures and large parameter scales, but also on the use of fine-tuning strategies (Wolf & Thomas et al, 2019), whereby by fine-tuning to a specific task or domain, the models can be adapted to specific application scenarios, thus improving their performance in real-world tasks. Fine-tuning large pre-trained language models (LLMs) based on the Transformer architecture has become a paradigm for processing NLP tasks. Despite the powerful transfer learning capabilities of modern pre-trained LLMs (e.g., zero-sample inference), fine-tuning on downstream datasets can still significantly improve model performance and generalization. Full-parameter fine-tuning involves all parameters of the model and can further improve performance.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        However, as the model grows larger, full-parameter fine-tuning on consumer-grade hardware becomes infeasible and can lead to loss of diversity and catastrophic forgetting. Therefore, how to efficiently perform model fine-tuning has become the focus of industry research and has spawned the rapid development of Parameter Efficient Fine-Tuning Techniques (PEFT). PEFT freezes most of the pre-trained model parameters by fine-tuning a small number of or additional model parameters, drastically reducing the computational and storage costs while being able to achieve comparable or even better performance than full-parameter fine-tuning. PEFT techniques are broadly categorized into three types: additions of additional parameters (A), selective methods (S), and introducing reparameterization (R).
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Additional parameters (A) are mainly divided into two subcategories: adapter-like methods and soft prompt methods. The adapter-like method achieves PEFT by adding a small adapter layer within the Transformer block, such as Adapter Tuning method (Houlsby et al., 2019) adds adapter structures to each Transformer layer after the multi-head attentional projection and after the second feed-forward layer and fixes the original model parameters by tuning only these added adapters and layer normalisation layers during training, this approach achieves similar performance to full parameter fine-tuning by adding between 0.5% and 5% of parameters; Adapter Fusion (Pfeiffer, Kamath, Rücklé, Cho & Gurevych, 2020) is a variant of adapters that optimises performance by combining multi-task adapter parameters and addresses the inter-multi-task problem by splitting training into two phases: knowledge extraction and knowledge combination, but the addition of adapters also increases the total number of parameters in the model, which may affect inference performance; Adapter Drop (Rücklé et al., 2020) improves inference speed by dynamically removing adapters and reduces computational overhead at runtime while maintaining task performance.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Soft Prompt achieves efficient parameter fine-tuning by adding learnable vectors at the head of the input sequence, such as Prefix Tuning (Li & Liang, 2021) constructs task-related virtual token segments as prefixes before the input tokens and updates only the parameters in the prefixed portion during the training period, while the parameters in the rest of the PLM portion remain fixed; The Power of Scale for Parameter Efficient Prompt Tuning (Lester, Al-Rfou & Constant, 2021) is a simplified version of Prefix Tuning, where a specific Prompt is defined for each task and used as part of the input data, this approach simplifies the training process by adding prompt tokens to the input layer only, without the need to adjust the MLP, and as the number of pre-trained model parameters increases, the effect of Prompt Tuning gradually approaches that of full parameter fine-tuning.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Selective method(S) optimizes the model by freezing some of the parameters and updating only those that need to be fine-tuned, adapting to the specific task while maintaining performance. This method is divided into two types: unstructured and structured masking. Unstructured masking determines the fine-tuning parameters by adding learnable binary masks to the model parameters. For example, Diff Pruning (Guo, Rush & Kim, 2020) tunes pre-training parameters by learning task-specific ‘diff’ vectors and encourages sparsity by dynamically cropping the diff vectors through fine-tuned approximations with L0-paradigm penalties. This approach, which does not require all tasks to be accessed during training and only stores small diff vectors for each task, has practical industrial applications and has shown comparable performance to baseline fine-tuning in GLUE benchmarks. Another approach, such as simple parameter efficient fine-tuning or transformer based masked language models BitFit (Zaken, Ravfogel & Goldberg, 2021), is a sparse fine-tuning method that updates bias parameters only during training, improving parameter efficiency.
      </Text>
      <Text fontSize="lg" mb={4} textAlign="justify">
        Reparameterization fine-tuning (R) involves finding various low dimensional re-parameterization forms of the pre-trained weight matrix to represent the entire parameter space for fine-tuning. During inference, the parameters will be equivalently transformed into a pre-trained model parameter structure to avoid introducing additional inference delays. The core idea of LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS (LoRA) (Hu et al., 2021) is to simulate indirect training of large models is achieved by simulating parameter changes through low-rank decomposition. During training, a low-rank representation of the original model parameters is constructed by adding upper and lower projection matrices, and after training, these additional parameters can be seamlessly fused into the pre-training weights; Adaptive budget allocation for parameter-efficient fine-tuning (AdaLoRA) (Zhang et al., 2023) is an improvement on LoRA that dynamically allocates parameter budgets to weight matrices based on importance scores. This method adjusts the incremental update by singular value decomposition, trims unimportant singular values to speed up the computation, and adds an additional penalty term to the training loss to stabilize the training; Efficient Fine Tuning of Quantized LLMs (QLoRA) (Dettmers, Pagnoni, Holtzman & Zettlemoyer, 2024) uses a high-precision technique to quantize a pre-trained model to 4 bits, and then adds a set of learnable low-rank adapter weights for fine-tuning, QLoRA employs two techniques to achieve high-fidelity 4-bit fine-tuning and introduces a paging optimizer to prevent memory spikes during gradient checkpoints and avoid memory overflow errors.
      </Text>

      <Image src="pictures/report_pic_5.png" alt="An illustration of main components of the transformer model from the ‘Attention is All You Need’ paper" mb={4} />
    </Box>
  );
};

export default LiteratureReviewFineTuning;
