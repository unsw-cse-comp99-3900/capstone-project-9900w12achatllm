{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /root/miniconda3/lib/python3.10/site-packages (4.42.4)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /root/miniconda3/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: peft>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: trl>=0.8.6 in /root/miniconda3/lib/python3.10/site-packages (0.9.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/miniconda3/lib/python3.10/site-packages (from trl>=0.8.6) (0.8.5)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (1.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=d7247b7fdec115baeacda05dd7cd87a1587196e4886d7d9601561003573456cc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1btbiv_8/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.8.2.dev0\n",
      "    Uninstalling llamafactory-0.8.2.dev0:\n",
      "      Successfully uninstalled llamafactory-0.8.2.dev0\n",
      "Successfully installed llamafactory-0.8.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (17.0.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: optimum in /root/miniconda3/lib/python3.10/site-packages (1.21.2)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.24.2)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping apex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41ddc7-fe57-45b6-87db-5ee83877d3a6",
   "metadata": {},
   "source": [
    "LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 05:13:27,495] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 05:13:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:13:49,517 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 05:13:50,182 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 05:13:50 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 05:13:50 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 05:13:56 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:03<00:00, 276.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:14:01,291 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:14:01,299 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 05:14:01,695 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 05:14:10,873 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:14:10,877 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 05:15:09,894 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 05:15:09,895 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 05:15:09,922 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:15:09,923 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 05:15:10 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 05:15:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 05:15:10 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 05:15:10 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 05:15:10 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,q_proj,up_proj,down_proj,v_proj,o_proj\n",
      "07/26/2024 05:15:11 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-26 05:15:12,006 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 05:15:13,233 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 05:15:13,233 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 05:15:13,233 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 05:15:13,233 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 05:15:13,233 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 05:15:13,233 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 05:15:13,233 >>   Total optimization steps = 168\n",
      "[INFO|trainer.py:2137] 2024-07-26 05:15:13,241 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.8179, 'grad_norm': 1.432814359664917, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5014, 'grad_norm': 0.914472758769989, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:45<08:07,  3.09s/it][INFO|trainer.py:3788] 2024-07-26 05:15:58,850 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:15:58,850 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:15:58,850 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.24it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.94it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.12it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.89it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.95it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.03it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.83it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.56it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.80it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 10.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.92it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.24it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.34it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 10.58it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:01, 10.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 11.25it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.46it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 11.42it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.56it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.50it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00, 11.08it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 10.90it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4945244789123535, 'eval_runtime': 4.8764, 'eval_samples_per_second': 20.507, 'eval_steps_per_second': 10.253, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:50<08:07,  3.09s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.45it/s]\u001b[A\n",
      "{'loss': 1.5178, 'grad_norm': 0.8078789710998535, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.3393, 'grad_norm': 0.6754911541938782, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [01:17<06:59,  2.84s/it][INFO|trainer.py:3788] 2024-07-26 05:16:31,294 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:16:31,294 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:16:31,294 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.30it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.62it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.54it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.68it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.98it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.11it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.40it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.35it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.86it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.29it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.31it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.51it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.67it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.61it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.66it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.26it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 10.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.15it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.35it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.90it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3544299602508545, 'eval_runtime': 5.0463, 'eval_samples_per_second': 19.817, 'eval_steps_per_second': 9.908, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [01:22<06:59,  2.84s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.13it/s]\u001b[A\n",
      "{'loss': 1.2123, 'grad_norm': 0.6294285655021667, 'learning_rate': 4.731745523109029e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.2087, 'grad_norm': 0.6217873096466064, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [01:53<06:59,  3.04s/it][INFO|trainer.py:3788] 2024-07-26 05:17:06,707 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:17:06,708 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:17:06,708 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.34it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.50it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.32it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.16it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.14it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.54it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.56it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.60it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.24it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.11it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 10.24it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.44it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.60it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 10.65it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:01, 10.25it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 10.56it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.65it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 10.34it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01, 10.49it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 10.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00, 10.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 11.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.60it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2999094724655151, 'eval_runtime': 5.077, 'eval_samples_per_second': 19.697, 'eval_steps_per_second': 9.848, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [01:58<06:59,  3.04s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.72it/s]\u001b[A\n",
      "{'loss': 1.4197, 'grad_norm': 0.5905656218528748, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.0936, 'grad_norm': 0.6161296963691711, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [02:26<06:10,  2.90s/it][INFO|trainer.py:3788] 2024-07-26 05:17:40,157 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:17:40,157 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:17:40,157 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.53it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.40it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.29it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 12.21it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.49it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.45it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.49it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.68it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.44it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.26it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.55it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.62it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.17it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.71it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.63it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.45it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00,  9.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2853233814239502, 'eval_runtime': 4.8889, 'eval_samples_per_second': 20.454, 'eval_steps_per_second': 10.227, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [02:31<06:10,  2.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.93it/s]\u001b[A\n",
      "{'loss': 1.0736, 'grad_norm': 0.6493117809295654, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.2823, 'grad_norm': 0.6221295595169067, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [03:00<05:44,  2.92s/it][INFO|trainer.py:3788] 2024-07-26 05:18:13,519 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:18:13,519 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:18:13,519 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.11it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 11.00it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.61it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.86it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.97it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.78it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.72it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 10.24it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.03it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.49it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.37it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 10.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 10.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 10.76it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.57it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 10.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01, 10.92it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 10.62it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00, 10.64it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 10.54it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.280711054801941, 'eval_runtime': 4.9107, 'eval_samples_per_second': 20.364, 'eval_steps_per_second': 10.182, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [03:05<05:44,  2.92s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.30it/s]\u001b[A\n",
      "{'loss': 1.3417, 'grad_norm': 0.6733303070068359, 'learning_rate': 3.790265684518767e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.3243, 'grad_norm': 0.6745858192443848, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [03:33<05:18,  2.95s/it][INFO|trainer.py:3788] 2024-07-26 05:18:46,997 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:18:46,998 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:18:46,998 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.62it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.23it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.86it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.87it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.50it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.33it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.11it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.16it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.34it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.30it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.26it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.03it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.37it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.11it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.15it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 11.03it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.81it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2745226621627808, 'eval_runtime': 4.8033, 'eval_samples_per_second': 20.819, 'eval_steps_per_second': 10.409, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [03:38<05:18,  2.95s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.68it/s]\u001b[A\n",
      "{'loss': 1.326, 'grad_norm': 0.8121852278709412, 'learning_rate': 3.369677161463068e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.2455, 'grad_norm': 0.6784561276435852, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [04:06<04:34,  2.80s/it][INFO|trainer.py:3788] 2024-07-26 05:19:19,677 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:19:19,678 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:19:19,678 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.94it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.09it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.52it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.28it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.41it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.78it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.17it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.08it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.63it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.72it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.68it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.93it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.31it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.95it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.18it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.27it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.28it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.60it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.273045301437378, 'eval_runtime': 4.6615, 'eval_samples_per_second': 21.452, 'eval_steps_per_second': 10.726, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [04:11<04:34,  2.80s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.64it/s]\u001b[A\n",
      "{'loss': 1.3392, 'grad_norm': 0.7260016798973083, 'learning_rate': 2.918765558261841e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.1804, 'grad_norm': 0.6402546763420105, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [04:39<04:09,  2.83s/it][INFO|trainer.py:3788] 2024-07-26 05:19:52,899 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:19:52,899 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:19:52,899 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.82it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.71it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.97it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.10it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.75it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.10it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.81it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.80it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.05it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.27it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.14it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.42it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.58it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2704930305480957, 'eval_runtime': 4.6227, 'eval_samples_per_second': 21.633, 'eval_steps_per_second': 10.816, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [04:44<04:09,  2.83s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.65it/s]\u001b[A\n",
      "{'loss': 1.2022, 'grad_norm': 0.5780544877052307, 'learning_rate': 2.4532528339227452e-05, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.3017, 'grad_norm': 0.6323313117027283, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [05:11<03:43,  2.86s/it][INFO|trainer.py:3788] 2024-07-26 05:20:24,308 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:20:24,308 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:20:24,308 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.86it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.48it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.75it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.48it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.38it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.46it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.39it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.72it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.60it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.42it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.72it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.16it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.12it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.03it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.09it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.05it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 11.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2681248188018799, 'eval_runtime': 4.7551, 'eval_samples_per_second': 21.03, 'eval_steps_per_second': 10.515, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [05:15<03:43,  2.86s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.44it/s]\u001b[A\n",
      "{'loss': 1.1513, 'grad_norm': 0.44850224256515503, 'learning_rate': 1.9893700455257996e-05, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.2015, 'grad_norm': 0.7188655734062195, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [05:43<03:09,  2.78s/it][INFO|trainer.py:3788] 2024-07-26 05:20:56,470 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:20:56,471 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:20:56,471 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.74it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.09it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.15it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.16it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.15it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.39it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.65it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.94it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.66it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.88it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.70it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.51it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 12.08it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.64it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.57it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.57it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.38it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.68it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.72it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.59it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.84it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2660099267959595, 'eval_runtime': 4.6478, 'eval_samples_per_second': 21.516, 'eval_steps_per_second': 10.758, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [05:47<03:09,  2.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.01it/s]\u001b[A\n",
      "{'loss': 1.143, 'grad_norm': 0.6192137002944946, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.4648, 'grad_norm': 0.648077666759491, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [06:15<02:43,  2.83s/it][INFO|trainer.py:3788] 2024-07-26 05:21:28,889 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:21:28,889 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:21:28,889 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.18it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.42it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.84it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.65it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.88it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.79it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.98it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.09it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.49it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.46it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.67it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.42it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.21it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 10.98it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.16it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.20it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.26it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 10.34it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.13it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.18it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2660202980041504, 'eval_runtime': 4.8953, 'eval_samples_per_second': 20.428, 'eval_steps_per_second': 10.214, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [06:20<02:43,  2.83s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.04it/s]\u001b[A\n",
      "{'loss': 1.2721, 'grad_norm': 0.6751638054847717, 'learning_rate': 1.130570401955322e-05, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.242, 'grad_norm': 0.47067514061927795, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [06:48<02:17,  2.86s/it][INFO|trainer.py:3788] 2024-07-26 05:22:01,826 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:22:01,826 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:22:01,826 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.93it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.86it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.85it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.46it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.50it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.50it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.51it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.84it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.14it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.83it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.11it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.82it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.01it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.23it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.07it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.19it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.81it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.27it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.265830397605896, 'eval_runtime': 4.7131, 'eval_samples_per_second': 21.217, 'eval_steps_per_second': 10.609, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [06:53<02:17,  2.86s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.36it/s]\u001b[A\n",
      "{'loss': 1.0739, 'grad_norm': 0.6438608765602112, 'learning_rate': 7.65597359928646e-06, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.2866, 'grad_norm': 0.6319846510887146, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [07:22<01:51,  2.94s/it][INFO|trainer.py:3788] 2024-07-26 05:22:35,612 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:22:35,612 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:22:35,612 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.77it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.26it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.19it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.75it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.94it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.08it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.34it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.59it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.36it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.22it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.44it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:02,  9.69it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.11it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.26it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.43it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.62it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 10.83it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.26it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.59it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2659791707992554, 'eval_runtime': 4.9076, 'eval_samples_per_second': 20.377, 'eval_steps_per_second': 10.188, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [07:27<01:51,  2.94s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.80it/s]\u001b[A\n",
      "{'loss': 1.3005, 'grad_norm': 0.6457785367965698, 'learning_rate': 4.610978276018496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.2914, 'grad_norm': 0.5953301787376404, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [07:55<01:19,  2.84s/it][INFO|trainer.py:3788] 2024-07-26 05:23:08,543 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:23:08,543 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:23:08,543 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.76it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.12it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.21it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.98it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.71it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.75it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.06it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.01it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.76it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.69it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.98it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.68it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.76it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 10.96it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.80it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.90it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.13it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.13it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.29it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2660574913024902, 'eval_runtime': 4.6561, 'eval_samples_per_second': 21.477, 'eval_steps_per_second': 10.739, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [07:59<01:19,  2.84s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.77it/s]\u001b[A\n",
      "{'loss': 1.1193, 'grad_norm': 0.5690096020698547, 'learning_rate': 2.2768880646947268e-06, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.5222, 'grad_norm': 0.6410197615623474, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [08:28<00:53,  2.95s/it][INFO|trainer.py:3788] 2024-07-26 05:23:42,005 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:23:42,006 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:23:42,006 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.40it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.97it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.25it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.24it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.26it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.30it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.73it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.31it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.10it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.85it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.89it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 11.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 11.53it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.67it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 11.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.49it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.57it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 11.65it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 11.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 11.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.265943169593811, 'eval_runtime': 4.676, 'eval_samples_per_second': 21.386, 'eval_steps_per_second': 10.693, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [08:33<00:53,  2.95s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.07it/s]\u001b[A\n",
      "{'loss': 1.3707, 'grad_norm': 0.6048609018325806, 'learning_rate': 7.350858136652261e-07, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 0.9835, 'grad_norm': 0.48941630125045776, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [09:01<00:21,  2.74s/it][INFO|trainer.py:3788] 2024-07-26 05:24:15,006 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:24:15,007 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:24:15,007 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.30it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.64it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.95it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.22it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.66it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.47it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.57it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.71it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.66it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.55it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.44it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.61it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.43it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.30it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.61it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.75it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.50it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.72it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.33it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.50it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 10.91it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2659859657287598, 'eval_runtime': 4.6038, 'eval_samples_per_second': 21.721, 'eval_steps_per_second': 10.861, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [09:06<00:21,  2.74s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.77it/s]\u001b[A\n",
      "{'loss': 1.1232, 'grad_norm': 0.8412803411483765, 'learning_rate': 3.9329624554584884e-08, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "100%|█████████████████████████████████████████| 168/168 [09:29<00:00,  3.08s/it][INFO|trainer.py:3478] 2024-07-26 05:24:42,479 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/checkpoint-168\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:24:42,568 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:24:42,571 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:24:43,476 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/checkpoint-168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:24:43,491 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/checkpoint-168/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 05:24:46,026 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 572.7852, 'train_samples_per_second': 4.714, 'train_steps_per_second': 0.293, 'train_loss': 1.2859125449543907, 'epoch': 2.99, 'num_input_tokens_seen': 950512}\n",
      "100%|█████████████████████████████████████████| 168/168 [09:32<00:00,  3.41s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 05:24:46,037 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:24:46,092 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:24:46,093 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:24:47,031 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:24:47,044 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =     950512\n",
      "  total_flos               =  7008779GF\n",
      "  train_loss               =     1.2859\n",
      "  train_runtime            = 0:09:32.78\n",
      "  train_samples_per_second =      4.714\n",
      "  train_steps_per_second   =      0.293\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 05:24:48,335 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:24:48,336 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:24:48,336 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.44it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.2659\n",
      "  eval_runtime            = 0:00:04.47\n",
      "  eval_samples_per_second =     22.333\n",
      "  eval_steps_per_second   =     11.166\n",
      "  num_input_tokens_seen   =     950512\n",
      "[INFO|modelcard.py:449] 2024-07-26 05:24:52,846 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb61405-dc34-4737-a316-306fd9cb7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 05:25:49,279] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 05:26:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,439 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,440 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,440 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,440 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,440 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:26:10,440 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 05:26:11,117 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 05:26:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 05:26:11 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 05:26:16 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:26:18,059 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:26:18,068 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 05:26:18,535 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 05:26:20,250 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:26:20,254 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 05:27:17,549 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 05:27:17,549 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 05:27:17,560 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:27:17,561 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 05:27:17 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 05:27:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 05:27:17 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 05:27:17 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 05:27:17 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,gate_proj,o_proj,k_proj,v_proj,q_proj\n",
      "07/26/2024 05:27:19 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-26 05:27:19,162 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 05:27:19,929 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 05:27:19,929 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 05:27:19,929 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 05:27:19,929 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-26 05:27:19,929 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-26 05:27:19,929 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 05:27:19,929 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-26 05:27:19,937 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.8402, 'grad_norm': 1.052909255027771, 'learning_rate': 4.975009271054409e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.7069, 'grad_norm': 0.7210690975189209, 'learning_rate': 4.9005367134442235e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:45<06:03,  3.60s/it][INFO|trainer.py:3788] 2024-07-26 05:28:05,345 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:28:05,346 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:28:05,346 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.15it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.11it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.37it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.88it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.36it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.45it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.60it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.21it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.61it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.49it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.73it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.25it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.97it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.90it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.20it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.08it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.46it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.24it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.30it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.15it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.478330373764038, 'eval_runtime': 4.8599, 'eval_samples_per_second': 20.576, 'eval_steps_per_second': 10.288, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:50<06:03,  3.60s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.66it/s]\u001b[A\n",
      "{'loss': 1.5715, 'grad_norm': 0.8214752674102783, 'learning_rate': 4.77807122597034e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.5351, 'grad_norm': 0.5896456837654114, 'learning_rate': 4.6100612100748765e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [01:24<05:27,  3.60s/it][INFO|trainer.py:3788] 2024-07-26 05:28:44,968 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:28:44,969 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:28:44,969 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.03it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.28it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.79it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.11it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.19it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.53it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.61it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:03,  9.86it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02,  9.88it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02,  9.80it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.00it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.97it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 11.05it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.78it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.61it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.33it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 10.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.63it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3422460556030273, 'eval_runtime': 5.0212, 'eval_samples_per_second': 19.916, 'eval_steps_per_second': 9.958, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [01:30<05:27,  3.60s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.84it/s]\u001b[A\n",
      "{'loss': 1.4784, 'grad_norm': 0.4981657862663269, 'learning_rate': 4.3998656199717435e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2502, 'grad_norm': 0.5592352151870728, 'learning_rate': 4.151686808475204e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [02:05<04:45,  3.52s/it][INFO|trainer.py:3788] 2024-07-26 05:29:25,066 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:29:25,067 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:29:25,067 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.37it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.14it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.54it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.45it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 12.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.60it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.47it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.47it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.58it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.07it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.37it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.25it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.57it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.69it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.46it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.50it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.73it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.71it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.64it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.67it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2953282594680786, 'eval_runtime': 4.6038, 'eval_samples_per_second': 21.721, 'eval_steps_per_second': 10.861, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [02:09<04:45,  3.52s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.57it/s]\u001b[A\n",
      "{'loss': 1.5371, 'grad_norm': 0.5624285340309143, 'learning_rate': 3.8704865111117746e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.4045, 'grad_norm': 0.4942658245563507, 'learning_rate': 3.56188664821012e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [02:46<04:16,  3.62s/it][INFO|trainer.py:3788] 2024-07-26 05:30:06,336 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:30:06,336 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:30:06,336 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.70it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.95it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.75it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.51it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.47it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.98it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.35it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.69it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.72it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.58it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.53it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.13it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.95it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.52it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.58it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.26it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.20it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.36it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.28it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.32it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2842893600463867, 'eval_runtime': 4.6707, 'eval_samples_per_second': 21.41, 'eval_steps_per_second': 10.705, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [02:51<04:16,  3.62s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.54it/s]\u001b[A\n",
      "{'loss': 1.4782, 'grad_norm': 0.5834718942642212, 'learning_rate': 3.232056928191376e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.4293, 'grad_norm': 0.544355571269989, 'learning_rate': 2.8875914991604948e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [03:26<03:39,  3.60s/it][INFO|trainer.py:3788] 2024-07-26 05:30:46,872 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:30:46,872 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:30:46,872 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.20it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.03it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.37it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.39it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.64it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.39it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.52it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.76it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.10it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.18it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.79it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.63it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.22it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.19it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.45it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.23it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.59it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.84it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.26it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.25it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.277307152748108, 'eval_runtime': 4.6399, 'eval_samples_per_second': 21.552, 'eval_steps_per_second': 10.776, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [03:31<03:39,  3.60s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.57it/s]\u001b[A\n",
      "{'loss': 1.3676, 'grad_norm': 0.5403192043304443, 'learning_rate': 2.5353771148519057e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3461, 'grad_norm': 0.5374765992164612, 'learning_rate': 2.182455450632803e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [04:06<03:00,  3.53s/it][INFO|trainer.py:3788] 2024-07-26 05:31:26,520 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:31:26,521 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:31:26,521 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.18it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.83it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.30it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.30it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.54it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.77it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.12it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.27it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.77it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.69it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.15it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.98it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.52it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.64it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.31it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.11it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.26it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.31it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2738879919052124, 'eval_runtime': 4.6168, 'eval_samples_per_second': 21.66, 'eval_steps_per_second': 10.83, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [04:11<03:00,  3.53s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.93it/s]\u001b[A\n",
      "{'loss': 1.3445, 'grad_norm': 0.6283462047576904, 'learning_rate': 1.8358823222228097e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.272, 'grad_norm': 0.4806451201438904, 'learning_rate': 1.5025866217114592e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [04:45<02:26,  3.57s/it][INFO|trainer.py:3788] 2024-07-26 05:32:05,913 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:32:05,914 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:32:05,914 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.57it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.48it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.00it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.89it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.59it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.26it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.94it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.22it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.38it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.33it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.17it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.51it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.18it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.00it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.53it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.18it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 10.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.40it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.59it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2712937593460083, 'eval_runtime': 4.8127, 'eval_samples_per_second': 20.778, 'eval_steps_per_second': 10.389, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [04:50<02:26,  3.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.41it/s]\u001b[A\n",
      "{'loss': 1.5343, 'grad_norm': 0.705571174621582, 'learning_rate': 1.1892317911069212e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.4851, 'grad_norm': 0.6035999059677124, 'learning_rate': 9.020826029175384e-06, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [05:26<01:50,  3.56s/it][INFO|trainer.py:3788] 2024-07-26 05:32:46,653 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:32:46,653 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:32:46,653 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.00it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.54it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.47it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 12.32it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.71it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.99it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.27it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.26it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.38it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.26it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.62it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.85it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 11.43it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.47it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.56it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.53it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 11.53it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2701656818389893, 'eval_runtime': 4.5178, 'eval_samples_per_second': 22.135, 'eval_steps_per_second': 11.067, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [05:31<01:50,  3.56s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.59it/s]\u001b[A\n",
      "{'loss': 1.2037, 'grad_norm': 0.5288206338882446, 'learning_rate': 6.468799111665003e-06, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.4297, 'grad_norm': 0.5772798657417297, 'learning_rate': 4.2872587689039484e-06, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [06:05<01:13,  3.51s/it][INFO|trainer.py:3788] 2024-07-26 05:33:25,820 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:33:25,820 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:33:25,820 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.11it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 12.16it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.61it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.79it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.97it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.33it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.33it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 10.99it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 11.04it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.20it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.17it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.10it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.24it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.16it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.90it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.10it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.98it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2700059413909912, 'eval_runtime': 4.6374, 'eval_samples_per_second': 21.564, 'eval_steps_per_second': 10.782, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [06:10<01:13,  3.51s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.41it/s]\u001b[A\n",
      "{'loss': 1.332, 'grad_norm': 0.57441246509552, 'learning_rate': 2.5198196276040782e-06, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.4714, 'grad_norm': 0.6584528088569641, 'learning_rate': 1.201817361771837e-06, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [06:46<00:41,  3.80s/it][INFO|trainer.py:3788] 2024-07-26 05:34:06,513 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:34:06,513 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:34:06,513 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.08it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.97it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.56it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.96it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.52it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.78it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.66it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.79it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.96it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.96it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.45it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.58it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.35it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.25it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.19it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.49it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 11.48it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.92it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2698414325714111, 'eval_runtime': 4.7713, 'eval_samples_per_second': 20.959, 'eval_steps_per_second': 10.479, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [06:51<00:41,  3.80s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.25it/s]\u001b[A\n",
      "{'loss': 1.3554, 'grad_norm': 0.5844089388847351, 'learning_rate': 3.5960224130728857e-07, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.2867, 'grad_norm': 0.6541716456413269, 'learning_rate': 1.0012322041960676e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [07:25<00:03,  3.57s/it][INFO|trainer.py:3788] 2024-07-26 05:34:45,696 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:34:45,696 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:34:45,696 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.98it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.80it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.06it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.29it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.54it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.38it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.13it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.21it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.38it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.27it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.61it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.62it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.64it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.76it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 10.62it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.41it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.86it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.52it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.49it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 10.81it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.92it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.269773244857788, 'eval_runtime': 4.8726, 'eval_samples_per_second': 20.523, 'eval_steps_per_second': 10.261, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [07:30<00:03,  3.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.36it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [07:33<00:00,  4.98s/it][INFO|trainer.py:3478] 2024-07-26 05:34:53,896 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/checkpoint-111\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:34:53,964 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:34:53,966 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:34:54,735 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:34:54,748 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 05:34:57,058 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 457.1222, 'train_samples_per_second': 5.907, 'train_steps_per_second': 0.243, 'train_loss': 1.4412054714855846, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [07:37<00:00,  4.12s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 05:34:57,071 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:34:57,204 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:34:57,205 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:34:58,077 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:34:58,113 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8142145GF\n",
      "  train_loss               =     1.4412\n",
      "  train_runtime            = 0:07:37.12\n",
      "  train_samples_per_second =      5.907\n",
      "  train_steps_per_second   =      0.243\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 05:34:59,340 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:34:59,340 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:34:59,340 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.19it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.2697\n",
      "  eval_runtime            = 0:00:04.58\n",
      "  eval_samples_per_second =     21.794\n",
      "  eval_steps_per_second   =     10.897\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-26 05:35:03,962 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_lora_bz=3 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f615a5-b235-4205-adad-1cf4014cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 05:36:04,145] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 05:36:23 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/26/2024 05:36:23 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,129 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,129 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,129 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,129 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,130 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 05:36:23,130 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 05:36:23,538 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 05:36:23 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 05:36:23 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 05:36:29 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 05:36:30,439 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 05:36:30,441 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/26/2024 05:36:30 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 05:36:30,882 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 05:36:32,082 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:36:32,088 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 05:37:10,741 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 05:37:10,741 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 05:37:10,753 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 05:37:10,754 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,gate_proj,v_proj,down_proj,o_proj,up_proj\n",
      "07/26/2024 05:37:11 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-26 05:37:11,560 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 05:37:12,279 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 05:37:12,279 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 05:37:12,279 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 05:37:12,279 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 05:37:12,279 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 05:37:12,279 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 05:37:12,279 >>   Total optimization steps = 168\n",
      "[INFO|trainer.py:2137] 2024-07-26 05:37:12,291 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.8158, 'grad_norm': 1.4046882390975952, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5023, 'grad_norm': 0.9097433686256409, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [01:38<23:48,  9.04s/it][INFO|trainer.py:3788] 2024-07-26 05:38:51,188 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:38:51,188 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:38:51,188 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.54it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  3.03it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.68it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.57it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.59it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.61it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.70it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.69it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:14,  2.61it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:15,  2.46it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:15,  2.38it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.39it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:14,  2.45it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:13,  2.49it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:13,  2.42it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.38it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:12,  2.39it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:12,  2.37it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.37it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:11,  2.43it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.46it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:10,  2.50it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.47it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:09,  2.31it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:09,  2.24it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:09,  2.23it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:09,  2.16it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:12<00:08,  2.15it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:08,  2.22it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:13<00:07,  2.23it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:07,  2.22it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:06,  2.22it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:14<00:06,  2.17it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:06,  2.12it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:15<00:05,  2.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:04,  2.22it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:16<00:04,  2.17it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:04,  2.11it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:17<00:03,  2.06it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:03,  2.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:18<00:02,  2.19it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:19<00:02,  2.29it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.29it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:20<00:01,  2.15it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.08it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4963645935058594, 'eval_runtime': 21.9963, 'eval_samples_per_second': 4.546, 'eval_steps_per_second': 2.273, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [02:00<23:48,  9.04s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.01it/s]\u001b[A\n",
      "{'loss': 1.5215, 'grad_norm': 0.8077029585838318, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.3421, 'grad_norm': 0.6781370043754578, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [03:32<23:23,  9.49s/it][INFO|trainer.py:3788] 2024-07-26 05:40:44,338 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:40:44,338 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:40:44,338 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.01it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  3.01it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.71it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.54it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.50it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.43it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.47it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.46it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.37it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.32it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.25it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:16,  2.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.37it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:14,  2.39it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.39it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.30it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:14,  2.25it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.31it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.43it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:11,  2.42it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:11,  2.39it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:11,  2.25it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:10,  2.28it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:10,  2.29it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.23it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:09,  2.23it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:09,  2.30it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:08,  2.31it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:08,  2.23it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:08,  2.17it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:13<00:07,  2.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.29it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:06,  2.36it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:06,  2.26it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:05,  2.25it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:16<00:05,  2.19it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:05,  2.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:17<00:04,  2.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:04,  2.20it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:18<00:03,  2.06it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:03,  2.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:19<00:02,  2.14it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:19<00:02,  2.19it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:20<00:01,  2.17it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.21it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3552623987197876, 'eval_runtime': 22.3564, 'eval_samples_per_second': 4.473, 'eval_steps_per_second': 2.236, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [03:54<23:23,  9.49s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  1.99it/s]\u001b[A\n",
      "{'loss': 1.2136, 'grad_norm': 0.6343222856521606, 'learning_rate': 4.731745523109029e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.2108, 'grad_norm': 0.6301788091659546, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [05:21<20:48,  9.05s/it][INFO|trainer.py:3788] 2024-07-26 05:42:34,179 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:42:34,179 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:42:34,179 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.57it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.32it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.87it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.50it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.32it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.17it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.18it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.31it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:16,  2.37it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.37it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.32it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.37it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.40it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.26it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:14,  2.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:14,  2.21it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.23it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:13,  2.15it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:13,  2.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.20it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:12,  2.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.07it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:11,  2.16it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.22it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:09,  2.26it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:09,  2.30it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:08,  2.28it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:08,  2.34it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:07,  2.43it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:07,  2.37it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.38it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:15<00:06,  2.39it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:05,  2.42it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:05,  2.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:16<00:05,  2.25it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:04,  2.25it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:17<00:04,  2.24it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:04,  2.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:18<00:03,  2.25it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:03,  2.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:19<00:02,  2.24it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:19<00:02,  2.29it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.34it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:20<00:01,  2.30it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.35it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3020707368850708, 'eval_runtime': 22.3783, 'eval_samples_per_second': 4.469, 'eval_steps_per_second': 2.234, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [05:44<20:48,  9.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.23it/s]\u001b[A\n",
      "{'loss': 1.4204, 'grad_norm': 0.5929824113845825, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.0944, 'grad_norm': 0.6009731888771057, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [07:13<19:08,  8.97s/it][INFO|trainer.py:3788] 2024-07-26 05:44:26,013 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:44:26,014 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:44:26,014 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  3.12it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:19,  2.41it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:19,  2.29it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:20,  2.19it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:20,  2.13it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.20it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.34it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:16,  2.38it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:17,  2.15it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.14it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.11it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:16,  2.18it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:16,  2.05it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.08it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:15,  2.10it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:14,  2.12it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:14,  2.10it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:14,  2.03it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:14,  1.94it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:13,  1.99it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:13,  1.99it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:12,  2.07it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:12,  1.99it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:10,  2.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.10it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  2.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.19it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:07,  2.03it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  2.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.08it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:05,  2.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.18it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.20it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.22it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:02,  2.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.59it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:01,  2.80it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:20<00:01,  2.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:21<00:00,  2.80it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.287041187286377, 'eval_runtime': 22.6171, 'eval_samples_per_second': 4.421, 'eval_steps_per_second': 2.211, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [07:36<19:08,  8.97s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.85it/s]\u001b[A\n",
      "{'loss': 1.0742, 'grad_norm': 0.6462177634239197, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.2838, 'grad_norm': 0.6137668490409851, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [08:58<18:37,  9.47s/it][INFO|trainer.py:3788] 2024-07-26 05:46:11,194 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:46:11,194 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:46:11,194 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.70it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.18it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.67it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.41it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.34it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.31it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:18,  2.26it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:17,  2.28it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.09it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.17it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.27it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.24it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.15it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:15,  2.07it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:14,  2.14it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:13,  2.26it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.24it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.24it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.25it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:11,  2.21it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:11,  2.23it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.24it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.21it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:11,  2.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:10,  2.02it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  1.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:09,  2.05it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:08,  2.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:07,  2.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:15<00:07,  2.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.14it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:16<00:06,  2.04it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:06,  1.94it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:17<00:05,  2.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.11it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.05it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  1.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.00it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:02,  1.97it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  1.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:01,  1.98it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:22<00:00,  1.98it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2822883129119873, 'eval_runtime': 23.8125, 'eval_samples_per_second': 4.199, 'eval_steps_per_second': 2.1, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [09:22<18:37,  9.47s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  1.93it/s]\u001b[A\n",
      "{'loss': 1.3425, 'grad_norm': 0.6773917078971863, 'learning_rate': 3.790265684518767e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.3259, 'grad_norm': 0.6532480716705322, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [11:02<18:19, 10.18s/it][INFO|trainer.py:3788] 2024-07-26 05:48:14,918 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:48:14,918 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:48:14,918 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.42it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.25it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.74it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:19,  2.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:19,  2.28it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:19,  2.15it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:17,  2.24it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.20it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:17,  2.11it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.07it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:16,  2.14it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.19it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:15,  2.15it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.14it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:14,  2.23it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.28it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:13,  2.30it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.26it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.25it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:11,  2.26it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:11,  2.20it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.09it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:10,  2.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.10it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:09,  2.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:09,  2.07it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.07it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  1.99it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:07,  2.03it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:15<00:06,  2.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.24it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:16<00:05,  2.26it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:05,  2.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:17<00:04,  2.23it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.17it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.09it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  2.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:19<00:02,  2.24it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.28it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:20<00:01,  2.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:21<00:00,  2.10it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:22<00:00,  2.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.275002121925354, 'eval_runtime': 23.2267, 'eval_samples_per_second': 4.305, 'eval_steps_per_second': 2.153, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [11:25<18:19, 10.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.19it/s]\u001b[A\n",
      "{'loss': 1.3286, 'grad_norm': 0.795462965965271, 'learning_rate': 3.369677161463068e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.2466, 'grad_norm': 0.7234498262405396, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [13:05<16:37, 10.18s/it][INFO|trainer.py:3788] 2024-07-26 05:50:17,907 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:50:17,907 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:50:17,907 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.24it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.14it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.47it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.43it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.42it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.34it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:17,  2.27it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.17it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.02it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:18,  2.00it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:18,  1.98it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:18,  1.90it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:17,  1.97it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:16,  2.02it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:14,  2.17it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.26it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.31it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.27it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.28it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:11,  2.32it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:11,  2.33it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:10,  2.34it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.15it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:11,  2.04it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:11,  1.99it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:10,  2.02it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  1.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  1.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:09,  1.95it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  1.94it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:08,  1.88it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  1.92it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:07,  1.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  1.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:06,  1.85it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  1.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:05,  1.86it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  1.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:04,  1.82it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  1.87it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:03,  1.88it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  1.85it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:02,  1.89it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  1.91it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:01,  1.94it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  1.93it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.272301197052002, 'eval_runtime': 24.7482, 'eval_samples_per_second': 4.041, 'eval_steps_per_second': 2.02, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [13:30<16:37, 10.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:24<00:00,  1.86it/s]\u001b[A\n",
      "{'loss': 1.3405, 'grad_norm': 0.7124834060668945, 'learning_rate': 2.918765558261841e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.1835, 'grad_norm': 0.651094913482666, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [15:05<13:55,  9.49s/it][INFO|trainer.py:3788] 2024-07-26 05:52:17,642 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:52:17,642 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:52:17,643 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.64it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:16,  2.83it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:19,  2.38it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.41it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.42it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.40it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.48it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.32it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.33it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.37it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:14,  2.40it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.41it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.40it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.36it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.42it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.35it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:12,  2.25it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.20it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:12,  2.13it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:12,  2.08it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.21it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:09,  2.21it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:09,  2.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:08,  2.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:07,  2.38it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:07,  2.41it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:07,  2.35it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.37it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:06,  2.43it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:05,  2.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:05,  2.26it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:16<00:05,  2.25it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:04,  2.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:17<00:04,  2.34it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:04,  2.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:18<00:03,  2.14it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:03,  2.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:18<00:02,  2.25it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:19<00:02,  2.26it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:20<00:01,  2.16it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.20it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2702782154083252, 'eval_runtime': 22.1317, 'eval_samples_per_second': 4.518, 'eval_steps_per_second': 2.259, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [15:27<13:55,  9.49s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.18it/s]\u001b[A\n",
      "{'loss': 1.2045, 'grad_norm': 0.5776326656341553, 'learning_rate': 2.4532528339227452e-05, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.3013, 'grad_norm': 0.6337026953697205, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [17:00<12:21,  9.50s/it][INFO|trainer.py:3788] 2024-07-26 05:54:12,564 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:54:12,564 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:54:12,564 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.77it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.30it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.76it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.60it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.49it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.42it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.37it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.35it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:17,  2.27it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.21it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:17,  2.16it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:16,  2.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.28it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.29it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.12it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:14,  2.13it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.23it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.34it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:12,  2.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:11,  2.38it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:12,  2.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.14it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:11,  2.15it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.19it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:10,  2.13it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.16it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:09,  2.21it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:08,  2.24it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:07,  2.29it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:07,  2.43it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.60it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:05,  2.77it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:05,  2.76it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:04,  2.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:15<00:04,  2.74it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:03,  2.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:16<00:03,  2.95it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:16<00:03,  2.92it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:17<00:02,  2.84it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:17<00:02,  2.78it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:18<00:02,  2.66it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:18<00:01,  2.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:18<00:01,  2.83it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:19<00:01,  2.85it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:19<00:00,  2.85it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:19<00:00,  2.90it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.267563819885254, 'eval_runtime': 20.5567, 'eval_samples_per_second': 4.865, 'eval_steps_per_second': 2.432, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [17:20<12:21,  9.50s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:20<00:00,  2.86it/s]\u001b[A\n",
      "{'loss': 1.1511, 'grad_norm': 0.44246700406074524, 'learning_rate': 1.9893700455257996e-05, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.2049, 'grad_norm': 0.5868773460388184, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [18:52<10:16,  9.07s/it][INFO|trainer.py:3788] 2024-07-26 05:56:04,397 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:56:04,397 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:56:04,397 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.28it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.31it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.76it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.56it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.42it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.15it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:19,  2.16it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:18,  2.17it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:19,  2.04it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.97it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:18,  2.03it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:16,  2.13it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.19it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.32it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.37it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.40it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:11,  2.44it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:11,  2.39it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:11,  2.31it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:10,  2.37it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:10,  2.43it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.39it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:09,  2.43it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:09,  2.44it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:08,  2.50it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:08,  2.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:08,  2.29it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:07,  2.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:07,  2.31it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.37it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:06,  2.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:06,  2.26it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:05,  2.29it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:16<00:05,  2.22it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:04,  2.26it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:17<00:04,  2.35it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:03,  2.38it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:17<00:03,  2.39it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:02,  2.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:18<00:02,  2.42it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:19<00:02,  2.47it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:20<00:01,  2.32it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.23it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.267587661743164, 'eval_runtime': 22.0682, 'eval_samples_per_second': 4.531, 'eval_steps_per_second': 2.266, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [19:14<10:16,  9.07s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.11it/s]\u001b[A\n",
      "{'loss': 1.1451, 'grad_norm': 0.6352258324623108, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.4685, 'grad_norm': 0.648061990737915, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [20:47<09:17,  9.60s/it][INFO|trainer.py:3788] 2024-07-26 05:58:00,071 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:58:00,072 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:58:00,072 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.36it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  3.03it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:18,  2.45it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.45it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.41it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.17it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:20,  2.03it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:18,  2.17it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.08it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.15it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:16,  2.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.32it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.27it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:13,  2.30it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.36it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:11,  2.39it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:11,  2.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:10,  2.40it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:10,  2.42it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:09,  2.45it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:09,  2.41it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:09,  2.40it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:12<00:08,  2.46it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:12<00:08,  2.45it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:13<00:07,  2.42it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:13<00:07,  2.47it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:06,  2.48it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:14<00:06,  2.47it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:05,  2.52it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:15<00:05,  2.54it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:15<00:05,  2.51it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:16<00:04,  2.49it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:16<00:04,  2.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:16<00:03,  2.54it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:17<00:03,  2.34it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:17<00:03,  2.36it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:18<00:02,  2.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:18<00:02,  2.43it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:18<00:02,  2.42it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:19<00:01,  2.45it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:19<00:01,  2.39it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:20<00:00,  2.40it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:20<00:00,  2.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2665338516235352, 'eval_runtime': 21.5656, 'eval_samples_per_second': 4.637, 'eval_steps_per_second': 2.319, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [21:09<09:17,  9.60s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:21<00:00,  2.40it/s]\u001b[A\n",
      "{'loss': 1.2735, 'grad_norm': 0.6959377527236938, 'learning_rate': 1.130570401955322e-05, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.2416, 'grad_norm': 0.4771650433540344, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [22:43<07:35,  9.49s/it][INFO|trainer.py:3788] 2024-07-26 05:59:56,193 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:59:56,194 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:59:56,194 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.10it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:19,  2.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:19,  2.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:19,  2.24it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.10it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:20,  2.01it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:18,  2.14it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:18,  2.14it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.09it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:18,  2.03it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:16,  2.14it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.23it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:15,  2.25it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:13,  2.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:12,  2.41it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.44it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.31it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.18it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.18it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:11,  2.19it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:10,  2.29it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.23it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:10,  2.13it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:11,  1.95it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:11,  1.90it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  2.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  2.05it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:07,  2.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:08,  1.99it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  2.05it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.13it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:06,  1.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.04it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.12it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.17it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  2.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.16it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:01,  2.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.10it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:00,  2.12it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:22<00:00,  2.13it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2655471563339233, 'eval_runtime': 23.8184, 'eval_samples_per_second': 4.198, 'eval_steps_per_second': 2.099, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [23:07<07:35,  9.49s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  2.11it/s]\u001b[A\n",
      "{'loss': 1.0756, 'grad_norm': 0.6572810411453247, 'learning_rate': 7.65597359928646e-06, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.2876, 'grad_norm': 0.6251938939094543, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [24:29<04:57,  7.83s/it][INFO|trainer.py:3788] 2024-07-26 06:01:41,467 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:01:41,468 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:01:41,468 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.51it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  3.97it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.28it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.12it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.88it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.60it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.67it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.60it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.54it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.67it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.71it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.65it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.65it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:12,  2.64it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:11,  2.68it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:11,  2.65it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.66it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.52it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.51it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:09,  2.60it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.69it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:08,  2.71it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:08,  2.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.50it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:08,  2.57it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:06,  2.76it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:06,  2.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:05,  2.94it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  2.85it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:05,  2.62it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:05,  2.59it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.56it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:04,  2.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.65it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.50it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  2.74it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.92it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:16<00:01,  3.07it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  2.92it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.80it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:17<00:01,  2.66it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  2.51it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:18<00:00,  2.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2667313814163208, 'eval_runtime': 18.8415, 'eval_samples_per_second': 5.307, 'eval_steps_per_second': 2.654, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [24:47<04:57,  7.83s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  2.52it/s]\u001b[A\n",
      "{'loss': 1.3028, 'grad_norm': 0.6494864225387573, 'learning_rate': 4.610978276018496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.2928, 'grad_norm': 0.5987901091575623, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [26:20<04:41, 10.05s/it][INFO|trainer.py:3788] 2024-07-26 06:03:32,603 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:03:32,604 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:03:32,604 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:12,  3.94it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:17,  2.66it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:21,  2.17it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:22,  2.02it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:22,  1.93it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:21,  1.97it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:20,  2.06it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:20,  2.04it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.03it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:05<00:19,  1.98it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.99it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:18,  2.01it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:19,  1.87it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:18,  1.92it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:17,  1.89it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:17,  1.84it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:16,  1.91it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:16,  1.89it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:10<00:16,  1.86it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:10<00:14,  1.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:11<00:12,  2.12it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.13it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:11,  2.22it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:11,  2.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:10,  2.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:09,  2.20it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:09,  2.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:09,  2.11it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:09,  2.06it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:08,  2.17it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:07,  2.21it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:07,  2.08it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  2.03it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.08it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:05,  2.18it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.12it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:04,  2.06it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  2.02it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  2.17it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:02,  2.23it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  2.16it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:01,  2.19it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  2.27it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:00,  2.28it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  2.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2646934986114502, 'eval_runtime': 24.4212, 'eval_samples_per_second': 4.095, 'eval_steps_per_second': 2.047, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [26:44<04:41, 10.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  2.15it/s]\u001b[A\n",
      "{'loss': 1.1204, 'grad_norm': 0.574306845664978, 'learning_rate': 2.2768880646947268e-06, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.5232, 'grad_norm': 0.6434810757637024, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [28:11<02:49,  9.43s/it][INFO|trainer.py:3788] 2024-07-26 06:05:24,195 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:05:24,195 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:05:24,195 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.25it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  2.97it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.76it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.66it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:16,  2.62it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:21,  1.97it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:20,  2.02it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.11it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:17,  2.19it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:17,  2.15it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.06it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.06it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:16,  2.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:16,  2.04it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:17,  1.94it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:16,  1.94it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:15,  1.95it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.01it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:13,  2.11it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:13,  2.01it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:13,  1.99it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.12it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.26it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.28it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:10,  2.20it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.15it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  1.99it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:10,  1.84it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:10,  1.79it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:09,  1.72it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:08,  1.79it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  1.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  1.97it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:06,  1.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  1.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:04,  2.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  2.03it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:03,  2.09it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  2.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:02,  2.17it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  2.24it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:01,  2.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  2.27it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:00,  2.16it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  2.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2658426761627197, 'eval_runtime': 24.5649, 'eval_samples_per_second': 4.071, 'eval_steps_per_second': 2.035, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [28:36<02:49,  9.43s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  1.99it/s]\u001b[A\n",
      "{'loss': 1.3711, 'grad_norm': 0.6113659143447876, 'learning_rate': 7.350858136652261e-07, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 0.9847, 'grad_norm': 0.5000937581062317, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [30:13<01:19,  9.98s/it][INFO|trainer.py:3788] 2024-07-26 06:07:25,821 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:07:25,821 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:07:25,821 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:12,  3.96it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:17,  2.68it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:20,  2.27it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:21,  2.10it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:21,  2.04it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.15it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:18,  2.18it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.01it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:05<00:20,  1.95it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.97it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:18,  2.04it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.02it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:18,  1.94it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:18,  1.87it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:17,  1.91it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:16,  1.91it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:15,  1.95it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:10<00:14,  2.02it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:11<00:12,  2.08it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:12,  1.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:12,  1.86it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:11,  1.94it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:10,  2.01it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:10,  1.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:09,  2.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:08,  2.14it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:16<00:07,  2.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:07,  2.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:17<00:07,  2.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:07,  1.96it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:18<00:06,  1.89it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:05,  2.01it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:19<00:05,  2.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:05,  1.96it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:20<00:04,  1.84it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:04,  1.92it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:21<00:03,  2.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:02,  2.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:22<00:02,  2.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:01,  2.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:23<00:01,  2.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:00,  2.15it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  2.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2665982246398926, 'eval_runtime': 25.014, 'eval_samples_per_second': 3.998, 'eval_steps_per_second': 1.999, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [30:38<01:19,  9.98s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:24<00:00,  2.07it/s]\u001b[A\n",
      "{'loss': 1.1219, 'grad_norm': 0.8359614014625549, 'learning_rate': 3.9329624554584884e-08, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "100%|█████████████████████████████████████████| 168/168 [31:54<00:00, 10.25s/it][INFO|trainer.py:3478] 2024-07-26 06:09:06,962 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:09:07,036 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:09:07,037 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:09:08,010 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:09:08,023 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 06:09:10,751 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1918.4606, 'train_samples_per_second': 1.407, 'train_steps_per_second': 0.088, 'train_loss': 1.2872277725310552, 'epoch': 2.99, 'num_input_tokens_seen': 950512}\n",
      "100%|█████████████████████████████████████████| 168/168 [31:58<00:00, 11.42s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 06:09:10,768 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:09:10,843 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:09:10,845 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:09:11,757 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:09:11,773 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =     950512\n",
      "  total_flos               =  7008779GF\n",
      "  train_loss               =     1.2872\n",
      "  train_runtime            = 0:31:58.46\n",
      "  train_samples_per_second =      1.407\n",
      "  train_steps_per_second   =      0.088\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 06:09:13,074 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:09:13,074 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:09:13,074 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.19it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.2671\n",
      "  eval_runtime            = 0:00:23.30\n",
      "  eval_samples_per_second =      4.292\n",
      "  eval_steps_per_second   =      2.146\n",
      "  num_input_tokens_seen   =     950512\n",
      "[INFO|modelcard.py:449] 2024-07-26 06:09:36,497 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 06:10:32,170] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 06:10:52 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/26/2024 06:10:52 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,847 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,848 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,848 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,848 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,848 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:10:52,848 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 06:10:53,431 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 06:10:53 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 06:10:53 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 06:10:59 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:11:00,276 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:11:00,283 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/26/2024 06:11:00 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 06:11:00,736 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 06:11:02,250 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:11:02,254 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 06:11:34,658 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 06:11:34,658 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 06:11:34,685 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:11:34,686 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,v_proj,q_proj,gate_proj,up_proj,down_proj,k_proj\n",
      "07/26/2024 06:11:35 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-26 06:11:35,773 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 06:11:36,676 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 06:11:36,677 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 06:11:36,677 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 06:11:36,677 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-26 06:11:36,677 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-26 06:11:36,677 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 06:11:36,677 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-26 06:11:36,685 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.8368, 'grad_norm': 1.0149093866348267, 'learning_rate': 4.975009271054409e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.7066, 'grad_norm': 0.7163776755332947, 'learning_rate': 4.9005367134442235e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [01:52<16:55, 10.05s/it][INFO|trainer.py:3788] 2024-07-26 06:13:29,071 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:13:29,071 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:13:29,071 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:13,  3.47it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:17,  2.67it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:18,  2.54it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.50it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.48it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.44it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:16,  2.48it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.52it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.45it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.34it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.37it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.37it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.43it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:14,  2.40it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.37it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.29it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.29it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:14,  2.04it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:15,  1.87it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:15,  1.87it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:13,  1.97it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:12,  2.05it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.06it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:11,  2.04it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.02it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:11,  1.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  1.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:10,  1.86it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:09,  1.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:09,  1.89it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:08,  1.91it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  1.98it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.09it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.12it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:05,  2.09it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.08it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  2.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.05it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  2.01it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:03,  1.91it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  1.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:02,  1.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  1.97it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:01,  1.96it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  1.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4798986911773682, 'eval_runtime': 24.4977, 'eval_samples_per_second': 4.082, 'eval_steps_per_second': 2.041, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [02:16<16:55, 10.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  1.87it/s]\u001b[A\n",
      "{'loss': 1.5731, 'grad_norm': 0.8016355633735657, 'learning_rate': 4.77807122597034e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.5358, 'grad_norm': 0.5954598784446716, 'learning_rate': 4.6100612100748765e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [04:00<16:10, 10.66s/it][INFO|trainer.py:3788] 2024-07-26 06:15:37,501 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:15:37,501 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:15:37,501 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:12,  3.91it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:16,  2.85it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:19,  2.32it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:20,  2.10it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:21,  1.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:21,  1.99it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:21,  1.95it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:20,  1.94it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:05<00:20,  1.94it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.93it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:18,  2.02it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:18,  2.00it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:16,  2.09it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:17,  1.96it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:16,  1.98it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:15,  2.01it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:15,  1.99it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.00it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:10<00:15,  1.92it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:14,  1.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:11<00:13,  1.95it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:13,  1.94it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:12,  1.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:12,  1.99it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:11,  1.95it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:11,  1.94it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:11,  1.84it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:11,  1.80it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:16<00:09,  1.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:16<00:09,  1.81it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:17<00:08,  1.95it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:17<00:07,  2.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:18<00:06,  2.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:18<00:06,  1.90it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:19<00:06,  1.91it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:19<00:05,  1.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:20<00:05,  1.92it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:20<00:04,  1.90it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:21<00:04,  1.90it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:21<00:03,  1.94it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:22<00:02,  2.05it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:22<00:02,  2.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:23<00:01,  2.12it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:23<00:01,  2.13it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:24<00:00,  2.10it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:24<00:00,  2.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3422577381134033, 'eval_runtime': 25.5857, 'eval_samples_per_second': 3.908, 'eval_steps_per_second': 1.954, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [04:26<16:10, 10.66s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:25<00:00,  2.07it/s]\u001b[A\n",
      "{'loss': 1.4789, 'grad_norm': 0.4891670048236847, 'learning_rate': 4.3998656199717435e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2515, 'grad_norm': 0.5603682994842529, 'learning_rate': 4.151686808475204e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [06:04<13:27,  9.96s/it][INFO|trainer.py:3788] 2024-07-26 06:17:41,704 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:17:41,704 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:17:41,704 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:12,  3.80it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:17,  2.70it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:21,  2.13it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:21,  2.09it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:21,  2.06it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:20,  2.11it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.14it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:19,  2.10it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:20,  1.99it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:05<00:19,  1.99it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.03it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:18,  2.00it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:18,  1.93it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:17,  1.97it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:16,  2.04it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:15,  2.15it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:14,  2.16it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:14,  2.10it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.12it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:13,  2.17it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:12,  2.19it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.21it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.20it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:11,  2.15it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:10,  2.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:10,  2.15it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:09,  2.17it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:10,  1.99it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  1.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:08,  2.08it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  2.10it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:07,  2.07it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  2.05it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:06,  2.03it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.04it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:05,  2.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.13it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.09it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  2.00it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:03,  2.02it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.07it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  1.93it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:02,  1.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  1.94it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:01,  1.99it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  2.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.295053243637085, 'eval_runtime': 24.53, 'eval_samples_per_second': 4.077, 'eval_steps_per_second': 2.038, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [06:29<13:27,  9.96s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:24<00:00,  2.03it/s]\u001b[A\n",
      "{'loss': 1.5394, 'grad_norm': 0.556931734085083, 'learning_rate': 3.8704865111117746e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.4065, 'grad_norm': 0.4964650869369507, 'learning_rate': 3.56188664821012e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [08:11<12:14, 10.35s/it][INFO|trainer.py:3788] 2024-07-26 06:19:47,893 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:19:47,893 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:19:47,893 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.39it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:15,  3.01it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:19,  2.24it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.18it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:19,  2.08it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:20,  1.96it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:20,  1.93it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.94it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:19,  1.88it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:19,  1.82it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:18,  1.87it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:17,  1.92it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:16,  1.93it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:15,  1.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:10<00:14,  2.02it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:11<00:13,  1.98it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:12,  2.04it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:11,  2.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:10,  2.01it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:11,  1.87it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:10,  1.86it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:11,  1.67it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:09,  1.81it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:16<00:09,  1.82it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:08,  1.90it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:17<00:07,  2.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:06,  2.01it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:18<00:06,  1.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:06,  1.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:19<00:05,  2.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:04,  2.03it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:20<00:04,  2.02it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:04,  1.96it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:21<00:03,  1.95it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:03,  1.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:22<00:02,  1.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:01,  2.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:23<00:01,  1.97it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:01,  1.98it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:24<00:00,  1.99it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2860509157180786, 'eval_runtime': 25.6361, 'eval_samples_per_second': 3.901, 'eval_steps_per_second': 1.95, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [08:36<12:14, 10.35s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:25<00:00,  1.93it/s]\u001b[A\n",
      "{'loss': 1.479, 'grad_norm': 0.6081446409225464, 'learning_rate': 3.232056928191376e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.4326, 'grad_norm': 0.5431793332099915, 'learning_rate': 2.8875914991604948e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [10:20<10:58, 10.79s/it][INFO|trainer.py:3788] 2024-07-26 06:21:57,058 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:21:57,058 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:21:57,059 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.03it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.59it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.48it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.41it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.16it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:20,  2.07it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:20,  2.04it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.06it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:18,  2.12it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.91it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:20,  1.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:21,  1.70it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:20,  1.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:19,  1.74it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:18,  1.81it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:16,  1.89it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:15,  1.94it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:15,  1.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:10<00:15,  1.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:14,  1.90it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:11<00:14,  1.84it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:12<00:14,  1.81it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:14,  1.78it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:13<00:12,  1.86it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:11,  1.95it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:14<00:10,  2.03it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:11,  1.91it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:15<00:11,  1.81it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:10,  1.87it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:16<00:09,  1.98it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:16<00:08,  1.97it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:17<00:08,  1.88it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:17<00:07,  1.92it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:18<00:07,  1.96it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:18<00:06,  1.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:19<00:06,  1.82it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:20<00:06,  1.82it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:21<00:04,  1.88it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:21<00:04,  1.83it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:22<00:03,  1.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:22<00:03,  1.85it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:23<00:02,  1.84it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:23<00:02,  1.92it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:24<00:01,  1.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:24<00:01,  1.89it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:25<00:00,  1.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2793575525283813, 'eval_runtime': 26.385, 'eval_samples_per_second': 3.79, 'eval_steps_per_second': 1.895, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [10:46<10:58, 10.79s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:25<00:00,  1.85it/s]\u001b[A\n",
      "{'loss': 1.3703, 'grad_norm': 0.5450782179832458, 'learning_rate': 2.5353771148519057e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3472, 'grad_norm': 0.5307835936546326, 'learning_rate': 2.182455450632803e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [12:33<09:18, 10.95s/it][INFO|trainer.py:3788] 2024-07-26 06:24:09,975 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:24:09,976 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:24:09,976 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:13,  3.62it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:16,  2.83it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:21,  2.08it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:21,  2.08it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:20,  2.13it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.15it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:19,  2.09it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:18,  2.12it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:18,  2.16it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.10it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.12it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.04it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:17,  2.03it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:17,  2.00it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:08<00:17,  1.94it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:17,  1.85it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:09<00:15,  1.95it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.07it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:14,  2.04it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:13,  2.03it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:13,  2.00it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.03it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:12<00:12,  1.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:12,  1.97it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:13<00:11,  1.95it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:11,  1.96it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:14<00:10,  2.02it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:09,  2.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:15<00:09,  1.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:08,  2.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  2.04it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:07,  2.02it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:17<00:07,  1.97it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:07,  1.99it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:18<00:06,  1.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:06,  1.85it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:19<00:05,  1.84it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:05,  1.90it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:20<00:04,  1.90it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:21<00:03,  1.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:03,  1.94it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:22<00:02,  1.95it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:01,  2.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:23<00:01,  1.94it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:01,  1.85it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:24<00:00,  1.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2750126123428345, 'eval_runtime': 25.5989, 'eval_samples_per_second': 3.906, 'eval_steps_per_second': 1.953, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [12:58<09:18, 10.95s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:25<00:00,  1.84it/s]\u001b[A\n",
      "{'loss': 1.3448, 'grad_norm': 0.6413324475288391, 'learning_rate': 1.8358823222228097e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.2739, 'grad_norm': 0.4880099892616272, 'learning_rate': 1.5025866217114592e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [14:41<07:09, 10.48s/it][INFO|trainer.py:3788] 2024-07-26 06:26:18,321 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:26:18,321 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:26:18,321 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.89it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.34it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.83it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.41it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:19,  2.26it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.29it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:18,  2.30it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.37it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.50it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.57it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.45it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.45it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:14,  2.37it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.14it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:15,  2.08it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:15,  2.03it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:14,  2.02it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:14,  2.00it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:13,  2.03it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:12,  2.13it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:11,  2.13it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.09it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:11<00:11,  1.98it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:11,  1.91it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:11,  1.90it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:10,  1.90it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  1.92it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:09,  1.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:09,  1.83it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:08,  1.98it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  1.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:07,  1.91it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  1.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:06,  1.87it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  1.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:05,  1.84it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  1.81it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:04,  1.88it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  1.87it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:21<00:03,  1.85it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  1.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:22<00:02,  1.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  1.86it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:23<00:01,  1.87it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  1.78it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2706373929977417, 'eval_runtime': 24.8628, 'eval_samples_per_second': 4.022, 'eval_steps_per_second': 2.011, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [15:06<07:09, 10.48s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:24<00:00,  1.73it/s]\u001b[A\n",
      "{'loss': 1.5365, 'grad_norm': 0.6935701370239258, 'learning_rate': 1.1892317911069212e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.4884, 'grad_norm': 0.6055086255073547, 'learning_rate': 9.020826029175384e-06, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [16:51<05:31, 10.71s/it][INFO|trainer.py:3788] 2024-07-26 06:28:27,910 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:28:27,910 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:28:27,910 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:14,  3.29it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:18,  2.55it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:21,  2.16it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:02<00:21,  2.08it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:25,  1.75it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:03<00:24,  1.75it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:04<00:23,  1.77it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:04<00:22,  1.85it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.02it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:05<00:18,  2.11it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:17,  2.17it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:06<00:16,  2.20it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:15,  2.27it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:07<00:15,  2.31it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:15,  2.25it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:14,  2.28it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:13,  2.33it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:12,  2.40it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:12,  2.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:12,  2.27it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.19it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:11,  2.23it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:10,  2.30it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:09,  2.36it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:10,  2.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:10,  2.05it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.07it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  2.06it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.20it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:07,  2.27it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:06,  2.30it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:06,  2.39it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:05,  2.39it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:16<00:05,  2.42it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:04,  2.46it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:17<00:04,  2.43it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.33it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:03,  2.33it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:18<00:03,  2.35it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  2.33it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:19<00:02,  2.28it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.19it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:20<00:01,  2.25it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.28it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:21<00:00,  2.34it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:21<00:00,  2.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2705477476119995, 'eval_runtime': 23.1113, 'eval_samples_per_second': 4.327, 'eval_steps_per_second': 2.163, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [17:14<05:31, 10.71s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.30it/s]\u001b[A\n",
      "{'loss': 1.2076, 'grad_norm': 0.5326998233795166, 'learning_rate': 6.468799111665003e-06, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.4327, 'grad_norm': 0.586708128452301, 'learning_rate': 4.2872587689039484e-06, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [18:53<03:34, 10.20s/it][INFO|trainer.py:3788] 2024-07-26 06:30:29,760 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:30:29,761 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:30:29,761 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.95it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:16,  2.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:18,  2.55it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:19,  2.36it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:19,  2.28it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.16it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:19,  2.14it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:19,  2.11it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:19,  2.04it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:19,  2.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:18,  2.02it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:18,  2.04it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.05it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:17,  2.03it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:16,  2.08it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.13it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:15,  2.08it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:14,  2.12it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.12it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:13,  2.15it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:12,  2.20it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.18it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:12,  2.15it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.17it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:11,  2.03it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:11,  1.97it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:10,  2.04it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:09,  2.13it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:08,  2.17it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.19it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:07,  2.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:07,  2.27it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:06,  2.27it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.27it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:16<00:05,  2.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:05,  2.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:17<00:05,  2.19it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.12it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.16it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  2.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.09it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.15it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:01,  2.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.21it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:00,  2.16it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:22<00:00,  2.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2705161571502686, 'eval_runtime': 23.4746, 'eval_samples_per_second': 4.26, 'eval_steps_per_second': 2.13, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [19:16<03:34, 10.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.26it/s]\u001b[A\n",
      "{'loss': 1.3347, 'grad_norm': 0.5749366283416748, 'learning_rate': 2.5198196276040782e-06, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.473, 'grad_norm': 0.6375912427902222, 'learning_rate': 1.201817361771837e-06, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [20:55<01:54, 10.38s/it][INFO|trainer.py:3788] 2024-07-26 06:32:32,682 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:32:32,682 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:32:32,682 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:13,  3.60it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:16,  2.83it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:18,  2.54it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:18,  2.45it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.32it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:19,  2.24it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:18,  2.23it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:17,  2.31it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:18,  2.14it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:05<00:19,  1.97it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:18,  1.98it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:06<00:17,  2.02it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:17,  2.03it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:07<00:16,  2.11it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.10it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:08<00:15,  2.03it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:15,  2.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:09<00:14,  2.10it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:10<00:14,  1.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:15,  1.80it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:11<00:13,  1.90it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:13,  1.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:11,  2.01it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:13<00:10,  2.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:10,  2.03it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:14<00:09,  2.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  2.08it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:15<00:08,  2.11it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:15<00:08,  1.96it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:16<00:07,  2.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:16<00:07,  2.14it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:17<00:06,  2.20it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:17<00:06,  2.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:18<00:05,  2.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:18<00:05,  2.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:19<00:04,  2.13it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:19<00:04,  2.09it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:19<00:03,  2.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:20<00:03,  2.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:20<00:02,  2.09it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:21<00:02,  2.07it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:21<00:01,  2.12it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:22<00:01,  2.10it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:22<00:00,  2.00it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:23<00:00,  2.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2707175016403198, 'eval_runtime': 24.4165, 'eval_samples_per_second': 4.096, 'eval_steps_per_second': 2.048, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [21:20<01:54, 10.38s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:23<00:00,  2.02it/s]\u001b[A\n",
      "{'loss': 1.3578, 'grad_norm': 0.5911695957183838, 'learning_rate': 3.5960224130728857e-07, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.2889, 'grad_norm': 0.6686175465583801, 'learning_rate': 1.0012322041960676e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [22:56<00:09,  9.92s/it][INFO|trainer.py:3788] 2024-07-26 06:34:33,190 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:34:33,190 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:34:33,190 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:11,  4.10it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:17,  2.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.56it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.52it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.39it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:18,  2.31it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:04<00:17,  2.33it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.36it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.24it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:17,  2.14it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:17,  2.12it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:16,  2.16it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.15it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:07<00:15,  2.16it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:15,  2.04it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:08<00:15,  1.96it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:14,  2.07it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:09<00:13,  2.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:09<00:12,  2.18it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:10<00:12,  2.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:10<00:12,  2.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:11<00:11,  2.10it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:11<00:11,  2.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:12<00:11,  2.03it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:12<00:11,  1.99it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:13<00:10,  2.01it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:13<00:09,  2.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:14<00:09,  2.04it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:14<00:08,  2.16it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:14<00:07,  2.21it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:15<00:07,  2.19it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:15<00:06,  2.23it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:16<00:06,  2.20it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:16<00:05,  2.25it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:17<00:05,  2.30it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:17<00:04,  2.30it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:18<00:04,  2.25it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:18<00:04,  2.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:18<00:03,  2.22it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:19<00:03,  2.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:19<00:02,  2.32it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:20<00:02,  2.30it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:20<00:01,  2.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:21<00:01,  2.26it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:21<00:00,  2.20it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:22<00:00,  2.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.272571086883545, 'eval_runtime': 23.1469, 'eval_samples_per_second': 4.32, 'eval_steps_per_second': 2.16, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [23:19<00:09,  9.92s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:22<00:00,  2.14it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [23:29<00:00, 16.78s/it][INFO|trainer.py:3478] 2024-07-26 06:35:05,899 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/checkpoint-111\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:35:05,967 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:35:05,968 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:35:06,901 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:35:06,915 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 06:35:09,571 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1412.8866, 'train_samples_per_second': 1.911, 'train_steps_per_second': 0.079, 'train_loss': 1.4428381812465083, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [23:32<00:00, 12.73s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 06:35:09,584 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:35:09,766 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:35:09,768 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:35:10,753 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:35:10,767 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8142145GF\n",
      "  train_loss               =     1.4428\n",
      "  train_runtime            = 0:23:32.88\n",
      "  train_samples_per_second =      1.911\n",
      "  train_steps_per_second   =      0.079\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 06:35:12,021 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:35:12,022 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:35:12,022 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:20<00:00,  2.39it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.2717\n",
      "  eval_runtime            = 0:00:21.40\n",
      "  eval_samples_per_second =      4.671\n",
      "  eval_steps_per_second   =      2.336\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-26 06:35:33,458 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_qlora_bz=3 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a719ba66-c0b7-4012-b47e-812699f754e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 06:36:28,954] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 06:36:47 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:36:47,328 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 06:36:47,816 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 06:36:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 06:36:47 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 06:36:53 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:36:54,672 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:36:54,678 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 06:36:54,949 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 06:36:56,091 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:36:56,096 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 06:37:23,882 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 06:37:23,883 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 06:37:23,904 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:37:23,905 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/26/2024 06:37:24 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-26 06:37:24,345 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 06:37:25,140 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 06:37:25,140 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 06:37:25,140 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-26 06:37:25,140 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 06:37:25,140 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 06:37:25,140 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 06:37:25,140 >>   Total optimization steps = 112\n",
      "[INFO|trainer.py:2137] 2024-07-26 06:37:25,141 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                   | 0/112 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.6987, 'grad_norm': 3.6136441230773926, 'learning_rate': 4.975452813341114e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.4009, 'grad_norm': 4.305347442626953, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:19<01:39,  1.03it/s][INFO|trainer.py:3788] 2024-07-26 06:37:44,681 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:37:44,681 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:37:44,681 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.37it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.39it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.30it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.70it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.01it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.03it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 14.09it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.36it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.56it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.49it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.78it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.49it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.20it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.74it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.79it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.84it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.32it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.55it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.42it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.478911280632019, 'eval_runtime': 3.3899, 'eval_samples_per_second': 29.5, 'eval_steps_per_second': 14.75, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:22<01:39,  1.03it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.85it/s]\u001b[A\n",
      "{'loss': 1.5409, 'grad_norm': 4.40507698059082, 'learning_rate': 4.781958162653297e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.403, 'grad_norm': 2.926417827606201, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:31<01:18,  1.17it/s][INFO|trainer.py:3788] 2024-07-26 06:37:56,193 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:37:56,193 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:37:56,193 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 18.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.40it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.22it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 17.53it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.84it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.64it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.84it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.93it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.02it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.63it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.49it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.90it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 15.12it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.53it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.65it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.61it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.88it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.58it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.442366123199463, 'eval_runtime': 3.3208, 'eval_samples_per_second': 30.113, 'eval_steps_per_second': 15.057, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:34<01:18,  1.17it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.95it/s]\u001b[A\n",
      "{'loss': 1.2843, 'grad_norm': 2.4479119777679443, 'learning_rate': 4.410093439554019e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3173, 'grad_norm': 3.2825984954833984, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:42<01:07,  1.21it/s][INFO|trainer.py:3788] 2024-07-26 06:38:07,516 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:38:07,516 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:38:07,517 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.50it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.26it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.86it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.77it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 14.17it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 14.03it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 14.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.92it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.51it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.27it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 16.08it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 16.56it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 16.27it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.62it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 16.24it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 16.16it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 16.11it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.81it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.25it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.65it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4335461854934692, 'eval_runtime': 3.3859, 'eval_samples_per_second': 29.534, 'eval_steps_per_second': 14.767, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:45<01:07,  1.21it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.02it/s]\u001b[A\n",
      "{'loss': 1.556, 'grad_norm': 2.9635426998138428, 'learning_rate': 3.888925582549006e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.1921, 'grad_norm': 1.9418880939483643, 'learning_rate': 3.5847093477938956e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [00:53<00:59,  1.22it/s][INFO|trainer.py:3788] 2024-07-26 06:38:18,971 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:38:18,971 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:38:18,972 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.08it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 15.40it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 14.22it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.72it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.75it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.76it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.83it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.23it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.90it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.99it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.86it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.93it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 14.87it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.50it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.23it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.97it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 14.93it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.05it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.92it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 15.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4224892854690552, 'eval_runtime': 3.5034, 'eval_samples_per_second': 28.544, 'eval_steps_per_second': 14.272, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [00:57<00:59,  1.22it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.79it/s]\u001b[A\n",
      "{'loss': 1.178, 'grad_norm': 2.5585756301879883, 'learning_rate': 3.259191862774037e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.3973, 'grad_norm': 2.236363649368286, 'learning_rate': 2.918765558261841e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:05<00:54,  1.13it/s][INFO|trainer.py:3788] 2024-07-26 06:38:30,662 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:38:30,663 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:38:30,663 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.71it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.13it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.89it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.93it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.58it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.36it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.58it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.47it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.67it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 16.03it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 15.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.64it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.76it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.69it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.70it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.17it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4145240783691406, 'eval_runtime': 3.3318, 'eval_samples_per_second': 30.014, 'eval_steps_per_second': 15.007, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:08<00:54,  1.13it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.79it/s]\u001b[A\n",
      "{'loss': 1.4481, 'grad_norm': 2.9054512977600098, 'learning_rate': 2.5701156406896725e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.2983, 'grad_norm': 2.0652692317962646, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:17<00:45,  1.14it/s][INFO|trainer.py:3788] 2024-07-26 06:38:42,414 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:38:42,414 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:38:42,414 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.80it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.35it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.72it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.43it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.50it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.41it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.70it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.52it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 16.14it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 16.50it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 15.83it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 16.24it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.42it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 16.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 16.07it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 16.21it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.78it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.413862943649292, 'eval_runtime': 3.2928, 'eval_samples_per_second': 30.369, 'eval_steps_per_second': 15.185, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:20<00:45,  1.14it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.87it/s]\u001b[A\n",
      "{'loss': 1.2111, 'grad_norm': 2.5204575061798096, 'learning_rate': 1.8755588045819327e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.1324, 'grad_norm': 2.6571505069732666, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [01:28<00:35,  1.19it/s][INFO|trainer.py:3788] 2024-07-26 06:38:53,997 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:38:53,997 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:38:53,997 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.91it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.64it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 16.01it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.66it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.65it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.73it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.58it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.86it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.33it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.72it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 15.10it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.78it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 16.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.54it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 16.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.73it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.95it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.07it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.89it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4069103002548218, 'eval_runtime': 3.3513, 'eval_samples_per_second': 29.839, 'eval_steps_per_second': 14.92, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [01:32<00:35,  1.19it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.02it/s]\u001b[A\n",
      "{'loss': 1.1687, 'grad_norm': 2.53182053565979, 'learning_rate': 1.229811636883677e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.0506, 'grad_norm': 2.520047426223755, 'learning_rate': 9.412754953531663e-06, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [01:40<00:27,  1.16it/s][INFO|trainer.py:3788] 2024-07-26 06:39:05,646 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:39:05,646 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:39:05,646 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.94it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.43it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.55it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 17.01it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.71it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.46it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.74it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.80it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 15.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.46it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 15.27it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.34it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.70it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.22it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.79it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.80it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.43it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.16it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.84it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.40836501121521, 'eval_runtime': 3.3191, 'eval_samples_per_second': 30.129, 'eval_steps_per_second': 15.064, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [01:43<00:27,  1.16it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.17it/s]\u001b[A\n",
      "{'loss': 1.0788, 'grad_norm': 2.3158247470855713, 'learning_rate': 6.833491949149329e-06, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.1438, 'grad_norm': 2.532451868057251, 'learning_rate': 4.610978276018496e-06, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [01:52<00:19,  1.15it/s][INFO|trainer.py:3788] 2024-07-26 06:39:17,191 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:39:17,192 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:39:17,192 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.73it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.23it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.01it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 17.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.97it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.74it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.78it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.86it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.92it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.79it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.95it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 16.47it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 15.81it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 16.46it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.39it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 16.00it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.86it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.20it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.11it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.406510591506958, 'eval_runtime': 3.3169, 'eval_samples_per_second': 30.148, 'eval_steps_per_second': 15.074, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [01:55<00:19,  1.15it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.52it/s]\u001b[A\n",
      "{'loss': 1.022, 'grad_norm': 2.04714298248291, 'learning_rate': 2.788859100528196e-06, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.0838, 'grad_norm': 2.278449773788452, 'learning_rate': 1.4029167422908107e-06, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:03<00:10,  1.19it/s][INFO|trainer.py:3788] 2024-07-26 06:39:28,442 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:39:28,442 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:39:28,442 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.47it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.76it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.59it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.13it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.12it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.35it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.27it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.30it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.36it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 14.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 14.50it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 14.51it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 13.99it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 14.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 14.80it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.05it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 14.76it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 15.45it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 14.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.406561255455017, 'eval_runtime': 3.4341, 'eval_samples_per_second': 29.12, 'eval_steps_per_second': 14.56, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:06<00:10,  1.19it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.55it/s]\u001b[A\n",
      "{'loss': 0.9905, 'grad_norm': 2.6947457790374756, 'learning_rate': 4.803679899192392e-07, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.302, 'grad_norm': 2.602675199508667, 'learning_rate': 3.9329624554584884e-08, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [02:15<00:01,  1.11it/s][INFO|trainer.py:3788] 2024-07-26 06:39:40,508 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:39:40,508 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:39:40,509 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.70it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.72it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.01it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.97it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.18it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 14.17it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.15it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.41it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.96it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.73it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.06it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.33it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.67it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.73it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.66it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.18it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.48it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.34it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.83it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4067269563674927, 'eval_runtime': 3.3972, 'eval_samples_per_second': 29.436, 'eval_steps_per_second': 14.718, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [02:18<00:01,  1.11it/s]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.85it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 112/112 [02:20<00:00,  1.60s/it][INFO|trainer.py:3478] 2024-07-26 06:39:45,650 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:39:45,674 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:39:45,691 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:40:04,356 >> Model weights saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:40:04,371 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:40:04,387 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/checkpoint-112/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 06:40:09,250 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 164.1092, 'train_samples_per_second': 10.968, 'train_steps_per_second': 0.682, 'train_loss': 1.2667636679751533, 'epoch': 1.99, 'num_input_tokens_seen': 631296}\n",
      "100%|█████████████████████████████████████████| 112/112 [02:44<00:00,  1.47s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 06:40:09,261 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:40:09,285 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:40:09,298 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:40:25,088 >> Model weights saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:40:25,104 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:40:25,118 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9911\n",
      "  num_input_tokens_seen    =     631296\n",
      "  total_flos               =  4622411GF\n",
      "  train_loss               =     1.2668\n",
      "  train_runtime            = 0:02:44.10\n",
      "  train_samples_per_second =     10.968\n",
      "  train_steps_per_second   =      0.682\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 06:40:26,087 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:40:26,087 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:40:26,087 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.29it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9911\n",
      "  eval_loss               =     1.4068\n",
      "  eval_runtime            = 0:00:03.35\n",
      "  eval_samples_per_second =      29.82\n",
      "  eval_steps_per_second   =      14.91\n",
      "  num_input_tokens_seen   =     631296\n",
      "[INFO|modelcard.py:449] 2024-07-26 06:40:29,456 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1b8de4-5757-4496-a21a-ab4029fe2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 06:41:26,575] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 06:41:46 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:41:46,979 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 06:41:47,342 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 06:41:47 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 06:41:47 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 06:41:53 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:41:54,208 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:41:54,217 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 06:41:54,524 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 06:41:55,774 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:41:55,780 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 06:42:21,953 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 06:42:21,954 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 06:42:21,965 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:42:21,966 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/26/2024 06:42:22 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-26 06:42:22,822 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 06:42:23,698 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 06:42:23,698 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 06:42:23,698 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-26 06:42:23,698 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-26 06:42:23,698 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-26 06:42:23,698 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 06:42:23,698 >>   Total optimization steps = 74\n",
      "[INFO|trainer.py:2137] 2024-07-26 06:42:23,699 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.6976, 'grad_norm': 3.230889081954956, 'learning_rate': 4.943888097369216e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.6376, 'grad_norm': 3.3261911869049072, 'learning_rate': 4.77807122597034e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [00:28<01:28,  1.39s/it][INFO|trainer.py:3788] 2024-07-26 06:42:51,735 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:42:51,736 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:42:51,736 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.73it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:02, 15.36it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 14.71it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 13.61it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.65it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.13it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.21it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 13.88it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.55it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.29it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.66it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 12.95it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 13.75it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.22it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.81it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.19it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 14.95it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.54it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.19it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 15.59it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4779486656188965, 'eval_runtime': 3.7497, 'eval_samples_per_second': 26.668, 'eval_steps_per_second': 13.334, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [00:31<01:28,  1.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.26it/s]\u001b[A\n",
      "{'loss': 1.5939, 'grad_norm': 2.987774610519409, 'learning_rate': 4.5099928259173516e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.5955, 'grad_norm': 2.6501498222351074, 'learning_rate': 4.151686808475204e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [00:43<01:05,  1.21s/it][INFO|trainer.py:3788] 2024-07-26 06:43:07,164 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:43:07,165 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:43:07,165 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.16it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.70it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 14.90it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 14.78it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.80it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.26it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.14it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.20it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.80it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.16it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.55it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.63it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.19it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.46it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.98it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.68it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.37it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.37it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.36it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.22it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 15.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4313430786132812, 'eval_runtime': 3.4965, 'eval_samples_per_second': 28.6, 'eval_steps_per_second': 14.3, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [00:46<01:05,  1.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.14it/s]\u001b[A\n",
      "{'loss': 1.5741, 'grad_norm': 2.005483627319336, 'learning_rate': 3.719237359534087e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.3477, 'grad_norm': 2.4047937393188477, 'learning_rate': 3.232056928191376e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [00:58<00:52,  1.19s/it][INFO|trainer.py:3788] 2024-07-26 06:43:22,320 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:43:22,320 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:43:22,320 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.18it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.23it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.81it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.41it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.96it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.93it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.53it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.94it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 14.78it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.17it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.19it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.28it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.24it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.18it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.41it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 14.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 14.80it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.27it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 15.52it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4159976243972778, 'eval_runtime': 3.4908, 'eval_samples_per_second': 28.647, 'eval_steps_per_second': 14.323, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [01:02<00:52,  1.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.93it/s]\u001b[A\n",
      "{'loss': 1.6597, 'grad_norm': 2.2994868755340576, 'learning_rate': 2.7120148111887732e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.4172, 'grad_norm': 1.8010220527648926, 'learning_rate': 2.182455450632803e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [01:14<00:42,  1.26s/it][INFO|trainer.py:3788] 2024-07-26 06:43:38,256 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:43:38,256 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:43:38,256 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.37it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 15.39it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 14.03it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 14.33it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.95it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.48it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.31it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.20it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.78it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.72it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.40it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.22it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.33it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.01it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.08it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 14.66it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.00it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.30it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.83it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.90it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4149752855300903, 'eval_runtime': 3.5614, 'eval_samples_per_second': 28.079, 'eval_steps_per_second': 14.04, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [01:18<00:42,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.21it/s]\u001b[A\n",
      "{'loss': 1.3857, 'grad_norm': 2.1703524589538574, 'learning_rate': 1.667150513144856e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.2944, 'grad_norm': 2.083082675933838, 'learning_rate': 1.1892317911069212e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [01:30<00:29,  1.25s/it][INFO|trainer.py:3788] 2024-07-26 06:43:54,022 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:43:54,022 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:43:54,022 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.52it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.00it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.85it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.98it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.57it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.22it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.14it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.42it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.41it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.43it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 13.31it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.75it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.56it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.21it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.77it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 16.03it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.80it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.85it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 16.03it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.49it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 16.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.404972791671753, 'eval_runtime': 3.3643, 'eval_samples_per_second': 29.724, 'eval_steps_per_second': 14.862, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [01:33<00:29,  1.25s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.19it/s]\u001b[A\n",
      "{'loss': 1.2658, 'grad_norm': 2.4684736728668213, 'learning_rate': 7.701528275561348e-06, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.2306, 'grad_norm': 2.0337209701538086, 'learning_rate': 4.2872587689039484e-06, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [01:45<00:16,  1.21s/it][INFO|trainer.py:3788] 2024-07-26 06:44:09,033 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:44:09,034 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:44:09,034 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.54it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.84it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.52it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 16.07it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.98it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.91it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.36it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.68it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 13.24it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.61it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.05it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.62it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.88it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.18it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.97it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.01it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.17it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.69it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.34it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.84it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4025338888168335, 'eval_runtime': 3.4961, 'eval_samples_per_second': 28.603, 'eval_steps_per_second': 14.302, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [01:48<00:16,  1.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.78it/s]\u001b[A\n",
      "{'loss': 1.2373, 'grad_norm': 2.2731473445892334, 'learning_rate': 1.8027743175872664e-06, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.1914, 'grad_norm': 1.9548585414886475, 'learning_rate': 3.5960224130728857e-07, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [02:00<00:04,  1.19s/it][INFO|trainer.py:3788] 2024-07-26 06:44:23,887 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:44:23,887 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:44:23,887 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.64it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.56it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.39it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.08it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.09it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.89it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.06it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.47it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.82it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.01it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.95it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.42it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.67it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.12it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.62it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.01it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.35it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.24it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.87it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.402878999710083, 'eval_runtime': 3.4561, 'eval_samples_per_second': 28.935, 'eval_steps_per_second': 14.467, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [02:03<00:04,  1.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.96it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 74/74 [02:08<00:00,  1.63s/it][INFO|trainer.py:3478] 2024-07-26 06:44:32,568 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:44:32,596 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:44:32,629 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:44:49,542 >> Model weights saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:44:49,560 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:44:49,575 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/checkpoint-74/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 06:44:54,281 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 150.5829, 'train_samples_per_second': 11.954, 'train_steps_per_second': 0.491, 'train_loss': 1.4384628244348474, 'epoch': 1.97, 'num_input_tokens_seen': 739920}\n",
      "100%|███████████████████████████████████████████| 74/74 [02:30<00:00,  2.03s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 06:44:54,295 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:44:54,326 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:44:54,337 >> Configuration saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:45:10,453 >> Model weights saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:45:10,469 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:45:10,482 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9733\n",
      "  num_input_tokens_seen    =     739920\n",
      "  total_flos               =  5417767GF\n",
      "  train_loss               =     1.4385\n",
      "  train_runtime            = 0:02:30.58\n",
      "  train_samples_per_second =     11.954\n",
      "  train_steps_per_second   =      0.491\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 06:45:11,574 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:45:11,574 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:45:11,574 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.14it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9733\n",
      "  eval_loss               =     1.4028\n",
      "  eval_runtime            = 0:00:03.38\n",
      "  eval_samples_per_second =     29.508\n",
      "  eval_steps_per_second   =     14.754\n",
      "  num_input_tokens_seen   =     739920\n",
      "[INFO|modelcard.py:449] 2024-07-26 06:45:14,995 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/bz/train_freeze_bz=3 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9722ed7e-41a1-4ef0-84a0-7ee05fdb555b",
   "metadata": {},
   "source": [
    "epoch=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4287379-91d2-4b74-90d4-9ae91c72aa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 06:46:15,987] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 06:46:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 06:46:35,304 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 06:46:35,686 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 06:46:35 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 06:46:35 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 06:46:41 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 06:46:42,927 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 06:46:42,934 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 06:46:43,475 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 06:46:44,559 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:46:44,564 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 06:47:11,630 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 06:47:11,631 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 06:47:11,637 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 06:47:11,637 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/26/2024 06:47:12 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-26 06:47:12,468 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 06:47:13,305 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 06:47:13,305 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 06:47:13,305 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-26 06:47:13,305 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-26 06:47:13,305 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-26 06:47:13,305 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 06:47:13,305 >>   Total optimization steps = 74\n",
      "[INFO|trainer.py:2137] 2024-07-26 06:47:13,307 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.8412, 'grad_norm': 3.713322162628174, 'learning_rate': 9.887776194738433e-06, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.7646, 'grad_norm': 3.075308322906494, 'learning_rate': 9.55614245194068e-06, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [00:28<01:28,  1.38s/it][INFO|trainer.py:3788] 2024-07-26 06:47:41,597 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:47:41,597 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:47:41,597 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.48it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:02, 16.12it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 13.52it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 13.09it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.99it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.61it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.79it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.65it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.97it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 14.51it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 13.35it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.00it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 13.69it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.06it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.41it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 14.42it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 14.63it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 14.38it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 14.26it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.78it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.98it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5495399236679077, 'eval_runtime': 3.885, 'eval_samples_per_second': 25.74, 'eval_steps_per_second': 12.87, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [00:32<01:28,  1.38s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.03it/s]\u001b[A\n",
      "{'loss': 1.6582, 'grad_norm': 3.1955184936523438, 'learning_rate': 9.019985651834703e-06, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.6529, 'grad_norm': 2.691596031188965, 'learning_rate': 8.303373616950408e-06, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [00:43<01:05,  1.20s/it][INFO|trainer.py:3788] 2024-07-26 06:47:57,077 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:47:57,077 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:47:57,077 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.20it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.27it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.33it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.58it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.13it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.95it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.92it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.37it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.31it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 15.09it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.81it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.54it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.15it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.52it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.06it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.66it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.78it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.47it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.71it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.18it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 15.74it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.475682020187378, 'eval_runtime': 3.4095, 'eval_samples_per_second': 29.33, 'eval_steps_per_second': 14.665, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [00:47<01:05,  1.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.46it/s]\u001b[A\n",
      "{'loss': 1.6209, 'grad_norm': 2.3638107776641846, 'learning_rate': 7.438474719068174e-06, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.3845, 'grad_norm': 2.9368844032287598, 'learning_rate': 6.464113856382752e-06, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [00:58<00:51,  1.17s/it][INFO|trainer.py:3788] 2024-07-26 06:48:11,887 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:48:11,887 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:48:11,888 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 18.12it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.55it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.55it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.39it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.93it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.02it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.88it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.15it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 13.93it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.25it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 14.69it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 14.98it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.18it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.41it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:01, 13.28it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 14.36it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 13.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 14.05it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 13.66it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 14.15it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 13.19it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.457442045211792, 'eval_runtime': 3.545, 'eval_samples_per_second': 28.208, 'eval_steps_per_second': 14.104, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [01:02<00:51,  1.17s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.55it/s]\u001b[A\n",
      "{'loss': 1.702, 'grad_norm': 2.5998871326446533, 'learning_rate': 5.4240296223775465e-06, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.5449, 'grad_norm': 2.422102689743042, 'learning_rate': 4.364910901265607e-06, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [01:14<00:43,  1.27s/it][INFO|trainer.py:3788] 2024-07-26 06:48:27,964 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:48:27,965 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:48:27,965 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.52it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.54it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.97it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 16.11it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.05it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.89it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.62it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.22it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.50it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.07it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.54it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.46it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.05it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 14.32it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.52it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.21it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.21it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 14.73it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 14.80it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.55it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.85it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4467990398406982, 'eval_runtime': 3.5514, 'eval_samples_per_second': 28.158, 'eval_steps_per_second': 14.079, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [01:18<00:43,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.99it/s]\u001b[A\n",
      "{'loss': 1.6021, 'grad_norm': 2.4365267753601074, 'learning_rate': 3.3343010262897125e-06, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.5363, 'grad_norm': 2.5469677448272705, 'learning_rate': 2.3784635822138424e-06, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [01:30<00:30,  1.26s/it][INFO|trainer.py:3788] 2024-07-26 06:48:43,869 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:48:43,869 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:48:43,869 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.19it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.16it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.19it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.65it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.07it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.06it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.19it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.70it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.22it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.54it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 14.45it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.05it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 14.43it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 15.26it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.62it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.94it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.13it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.13it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.27it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.69it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 14.53it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 15.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 13.84it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4410812854766846, 'eval_runtime': 3.4447, 'eval_samples_per_second': 29.03, 'eval_steps_per_second': 14.515, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [01:33<00:30,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.25it/s]\u001b[A\n",
      "{'loss': 1.4805, 'grad_norm': 2.719722270965576, 'learning_rate': 1.5403056551122697e-06, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.4429, 'grad_norm': 2.37050724029541, 'learning_rate': 8.574517537807897e-07, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [01:45<00:17,  1.22s/it][INFO|trainer.py:3788] 2024-07-26 06:48:59,099 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:48:59,100 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:48:59,100 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.75it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.21it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.77it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.67it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 14.23it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 14.06it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 14.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.96it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.14it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 14.37it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 13.85it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.61it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 16.01it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 15.07it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.50it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.83it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.20it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.32it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 14.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 14.74it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 14.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.65it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4385943412780762, 'eval_runtime': 3.4468, 'eval_samples_per_second': 29.012, 'eval_steps_per_second': 14.506, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [01:49<00:17,  1.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.00it/s]\u001b[A\n",
      "{'loss': 1.4498, 'grad_norm': 2.6749932765960693, 'learning_rate': 3.6055486351745327e-07, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.3876, 'grad_norm': 2.1444098949432373, 'learning_rate': 7.192044826145772e-08, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [02:00<00:04,  1.20s/it][INFO|trainer.py:3788] 2024-07-26 06:49:13,926 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:49:13,927 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:49:13,927 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.18it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 18.32it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.71it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.55it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.06it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.40it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.13it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.29it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 14.10it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 13.80it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 13.73it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 14.81it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 14.45it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 15.39it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.72it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.11it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.04it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.33it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 14.56it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 14.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 14.35it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 14.64it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 13.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4380507469177246, 'eval_runtime': 3.5326, 'eval_samples_per_second': 28.308, 'eval_steps_per_second': 14.154, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [02:04<00:04,  1.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.86it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 74/74 [02:09<00:00,  1.64s/it][INFO|trainer.py:3478] 2024-07-26 06:49:22,657 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:49:22,705 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:49:22,724 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:49:37,930 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:49:37,951 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:49:37,966 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/checkpoint-74/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 06:49:42,578 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 149.2713, 'train_samples_per_second': 12.059, 'train_steps_per_second': 0.496, 'train_loss': 1.582818772341754, 'epoch': 1.97, 'num_input_tokens_seen': 739920}\n",
      "100%|███████████████████████████████████████████| 74/74 [02:29<00:00,  2.02s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 06:49:42,590 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 06:49:42,614 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 06:49:42,628 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 06:49:58,899 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 06:49:58,919 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 06:49:58,933 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9733\n",
      "  num_input_tokens_seen    =     739920\n",
      "  total_flos               =  5417767GF\n",
      "  train_loss               =     1.5828\n",
      "  train_runtime            = 0:02:29.27\n",
      "  train_samples_per_second =     12.059\n",
      "  train_steps_per_second   =      0.496\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 06:50:00,022 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 06:50:00,022 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 06:50:00,022 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.80it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9733\n",
      "  eval_loss               =      1.438\n",
      "  eval_runtime            = 0:00:03.47\n",
      "  eval_samples_per_second =     28.742\n",
      "  eval_steps_per_second   =     14.371\n",
      "  num_input_tokens_seen   =     739920\n",
      "[INFO|modelcard.py:449] 2024-07-26 06:50:03,532 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea1204d-f1e9-4f7d-956e-dff0f8e305c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
