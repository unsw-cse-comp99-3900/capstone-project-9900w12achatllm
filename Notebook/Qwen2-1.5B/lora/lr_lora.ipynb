{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.41.2\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/20/5c/244db59e074e80248fdfa60495eeee257e4d97c3df3487df68be30cd60c8/transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /root/miniconda3/lib/python3.10/site-packages (0.32.1)\n",
      "Collecting peft>=0.11.1\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "Collecting trl>=0.8.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/8c/7d1007557b343d5cf18349802e94d3a14397121e9105b4661f8cd753f9bf/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tyro>=0.5.11 (from trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/ea/c8967a64769ec465a2d49bf81e1e135999741704a36993b6b51465ce8503/tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=31c653d8941d5de9c2179b9ebd258a144aaf48a8c91df35038778fd257f50ab2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-d_w0j0l3/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: shtab, docstring-parser, tiktoken, tyro, tokenizers, transformers, trl, peft, llamafactory\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.15\n",
      "    Uninstalling docstring-parser-0.15:\n",
      "      Successfully uninstalled docstring-parser-0.15\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchx 0.6.0 requires fsspec==2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 llamafactory-0.8.2.dev0 peft-0.11.1 shtab-1.7.1 tiktoken-0.7.0 tokenizers-0.19.1 transformers-4.42.3 trl-0.9.6 tyro-0.8.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting optimum\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/fa/e4/f832e42a1eb9d5ac4fa6379295e05aebeae507d171babc1786bfa0210299/optimum-1.21.2-py3-none-any.whl (424 kB)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.21.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping apex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd83f-942a-4b63-9e4e-e08578379499",
   "metadata": {},
   "source": [
    "batch size = 3, max_example = 500, lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 10:41:08,062] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 10:41:13 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:41:13,333 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 10:41:13,729 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 10:41:13 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 10:41:13 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "Generating train split: 10178 examples [00:00, 85205.26 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1204.9\n",
      "07/11/2024 10:41:20 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Generating train split: 900 examples [00:00, 9322.06 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1411.8\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:02<00:00, 378.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 10:41:25,902 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 10:41:25,904 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 10:41:25,951 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 10:41:33,514 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 10:41:33,517 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 10:43:49,045 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 10:43:49,045 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 10:43:49,065 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 10:43:49,065 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 10:43:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 10:43:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 10:43:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 10:43:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 10:43:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,v_proj,up_proj,down_proj,gate_proj,k_proj,q_proj\n",
      "07/11/2024 10:44:12 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-11 10:44:12,199 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 10:44:12,588 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 10:44:12,588 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 10:44:12,588 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-11 10:44:12,588 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 10:44:12,588 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 10:44:12,588 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 10:44:12,588 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-11 10:44:12,592 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.9278, 'grad_norm': 1.496383786201477, 'learning_rate': 9.950018542108818e-06, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.941, 'grad_norm': 1.3594516515731812, 'learning_rate': 9.801073426888447e-06, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [02:11<13:11,  7.84s/it][INFO|trainer.py:3788] 2024-07-11 10:46:24,485 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:46:24,485 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:46:24,485 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.46it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.49it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.32it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.54it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.40it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.86it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.58it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.72it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.59it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.43it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.57it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.75it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:02<00:05,  6.57it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.13it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.73it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.36it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.61it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.78it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  6.85it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.27it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  5.99it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.19it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.46it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.47it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.64it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.74it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.96it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.57it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.89it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.68it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.66it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7421492338180542, 'eval_runtime': 8.3538, 'eval_samples_per_second': 11.971, 'eval_steps_per_second': 5.985, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [02:20<13:11,  7.84s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.76it/s]\u001b[A\n",
      "{'loss': 1.8482, 'grad_norm': 1.46728515625, 'learning_rate': 9.55614245194068e-06, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.8289, 'grad_norm': 1.1372661590576172, 'learning_rate': 9.220122420149753e-06, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [03:26<10:14,  6.75s/it][INFO|trainer.py:3788] 2024-07-11 10:47:39,175 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:47:39,175 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:47:39,175 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.96it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.48it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.18it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.44it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.95it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.59it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.68it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.59it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.52it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.45it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.49it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.28it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.71it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.26it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.82it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.87it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.77it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.81it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.49it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.20it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.95it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:02,  6.35it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.36it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.72it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.85it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  6.01it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.66it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.74it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6384015083312988, 'eval_runtime': 8.2446, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 6.065, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [03:34<10:14,  6.75s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.83it/s]\u001b[A\n",
      "{'loss': 1.76, 'grad_norm': 0.9845370650291443, 'learning_rate': 8.799731239943488e-06, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.52, 'grad_norm': 1.1649439334869385, 'learning_rate': 8.303373616950408e-06, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [04:40<08:52,  6.57s/it][INFO|trainer.py:3788] 2024-07-11 10:48:52,904 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:48:52,904 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:48:52,904 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.90it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.21it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.46it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.96it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.60it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.67it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.57it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.46it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.50it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.28it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.71it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.25it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.82it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.74it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.14it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.47it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.15it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:02,  6.34it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.35it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.72it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.84it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.99it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.62it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.14it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.98it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.72it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.27it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.559340238571167, 'eval_runtime': 8.2769, 'eval_samples_per_second': 12.082, 'eval_steps_per_second': 6.041, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [04:48<08:52,  6.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.79it/s]\u001b[A\n",
      "{'loss': 1.7875, 'grad_norm': 0.9254412055015564, 'learning_rate': 7.74097302222355e-06, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.6449, 'grad_norm': 0.8236969113349915, 'learning_rate': 7.12377329642024e-06, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [06:00<08:27,  7.15s/it][INFO|trainer.py:3788] 2024-07-11 10:50:13,477 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:50:13,477 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:50:13,477 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.99it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.41it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.12it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.36it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.50it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.46it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.49it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.39it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.45it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.25it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.76it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.21it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.73it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.30it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.76it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.74it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.14it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.51it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.33it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.59it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.82it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.37it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  6.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.60it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5072085857391357, 'eval_runtime': 8.2611, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 6.052, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [06:09<08:27,  7.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.90it/s]\u001b[A\n",
      "{'loss': 1.7146, 'grad_norm': 0.7968530654907227, 'learning_rate': 6.464113856382752e-06, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.6462, 'grad_norm': 0.802116334438324, 'learning_rate': 5.77518299832099e-06, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [07:19<07:10,  7.06s/it][INFO|trainer.py:3788] 2024-07-11 10:51:32,332 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:51:32,332 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:51:32,332 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.39it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.10it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.59it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.46it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.87it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.56it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.29it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.46it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.36it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.83it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.21it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.77it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.50it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.95it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.69it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.71it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.20it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.58it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:02,  6.41it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.82it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.59it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.87it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  7.01it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.98it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.54it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.18it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.32it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.42it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4728500843048096, 'eval_runtime': 8.2511, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 6.06, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [07:27<07:10,  7.06s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.77it/s]\u001b[A\n",
      "{'loss': 1.5763, 'grad_norm': 0.6922920942306519, 'learning_rate': 5.070754229703811e-06, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.5172, 'grad_norm': 0.6497918367385864, 'learning_rate': 4.364910901265607e-06, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [08:34<05:42,  6.72s/it][INFO|trainer.py:3788] 2024-07-11 10:52:46,820 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:52:46,820 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:52:46,821 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.78it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.25it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.31it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.46it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.48it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.55it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.52it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.39it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.46it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.26it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.83it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.28it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.78it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.33it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.96it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.76it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.74it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.25it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.69it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.58it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.83it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.37it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  6.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.60it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.86it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.36it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.74it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4491424560546875, 'eval_runtime': 8.2484, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 6.062, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [08:42<05:42,  6.72s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.91it/s]\u001b[A\n",
      "{'loss': 1.5278, 'grad_norm': 0.7650100588798523, 'learning_rate': 3.6717646444456196e-06, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.4371, 'grad_norm': 0.6731665730476379, 'learning_rate': 3.0051732434229185e-06, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [09:41<03:48,  5.57s/it][INFO|trainer.py:3788] 2024-07-11 10:53:54,130 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:53:54,130 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:53:54,130 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.94it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:05,  8.93it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:04, 10.44it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:06,  6.95it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:06,  6.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  8.04it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  8.31it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  7.86it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  7.09it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:05,  6.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:05,  5.99it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  6.41it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.49it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.93it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.29it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  9.18it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  7.52it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.33it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:03,  7.30it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  7.24it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.85it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.58it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  7.67it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  7.63it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:01,  6.92it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  6.29it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:05<00:01,  6.79it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.30it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.33it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.02it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  6.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  5.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4330945014953613, 'eval_runtime': 7.1398, 'eval_samples_per_second': 14.006, 'eval_steps_per_second': 7.003, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [09:48<03:48,  5.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  5.82it/s]\u001b[A\n",
      "{'loss': 1.7196, 'grad_norm': 0.8806163668632507, 'learning_rate': 2.3784635822138424e-06, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.6633, 'grad_norm': 0.6505841016769409, 'learning_rate': 1.8041652058350768e-06, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [10:59<03:33,  6.87s/it][INFO|trainer.py:3788] 2024-07-11 10:55:12,378 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:55:12,378 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:55:12,378 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.15it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.95it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.54it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.52it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.90it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.43it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.49it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.63it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.34it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.43it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.35it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:02<00:04,  6.64it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.03it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.56it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.28it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.59it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:04,  5.62it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.55it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.14it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.97it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.38it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.95it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.53it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.90it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.71it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4224810600280762, 'eval_runtime': 8.2992, 'eval_samples_per_second': 12.049, 'eval_steps_per_second': 6.025, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [11:08<03:33,  6.87s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.77it/s]\u001b[A\n",
      "{'loss': 1.3697, 'grad_norm': 0.747351884841919, 'learning_rate': 1.2937598223330006e-06, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.6072, 'grad_norm': 0.7381699681282043, 'learning_rate': 8.574517537807897e-07, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [12:13<02:19,  6.63s/it][INFO|trainer.py:3788] 2024-07-11 10:56:26,498 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:56:26,498 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:56:26,498 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.47it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.39it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.43it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.82it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.54it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.54it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.47it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.36it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.35it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.52it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.30it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.72it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.16it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.34it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.67it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.79it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.10it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.45it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.07it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:02,  6.38it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.27it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.63it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.80it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.86it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.55it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.24it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.77it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.84it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4167343378067017, 'eval_runtime': 8.2976, 'eval_samples_per_second': 12.052, 'eval_steps_per_second': 6.026, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [12:22<02:19,  6.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.76it/s]\u001b[A\n",
      "{'loss': 1.4917, 'grad_norm': 0.8174755573272705, 'learning_rate': 5.039639255208156e-07, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.6479, 'grad_norm': 0.720298171043396, 'learning_rate': 2.403634723543674e-07, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [13:21<01:07,  6.13s/it][INFO|trainer.py:3788] 2024-07-11 10:57:34,039 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:57:34,040 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:57:34,040 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.57it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.65it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.47it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.26it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.30it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.65it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.19it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.85it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.28it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.13it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.06it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.27it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.39it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:02,  9.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01,  9.42it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.95it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.23it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.40it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.81it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.38it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.50it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.41it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  6.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.56it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.74it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.14it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.414535403251648, 'eval_runtime': 5.7883, 'eval_samples_per_second': 17.276, 'eval_steps_per_second': 8.638, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [13:27<01:07,  6.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  6.60it/s]\u001b[A\n",
      "{'loss': 1.5364, 'grad_norm': 0.7731712460517883, 'learning_rate': 7.192044826145772e-08, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.4545, 'grad_norm': 0.8761062622070312, 'learning_rate': 2.002464408392135e-09, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [14:20<00:05,  5.58s/it][INFO|trainer.py:3788] 2024-07-11 10:58:32,961 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:58:32,961 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:58:32,961 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 14.34it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.89it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.85it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.38it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.76it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.81it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.83it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.55it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.88it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.87it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.69it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.61it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.42it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.48it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.41it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.39it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.23it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.84it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.62it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.28it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  7.41it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  7.46it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  6.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  6.75it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.21it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.22it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.33it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  5.70it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.414078712463379, 'eval_runtime': 6.6504, 'eval_samples_per_second': 15.037, 'eval_steps_per_second': 7.518, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [14:27<00:05,  5.58s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  5.80it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [14:33<00:00,  7.80s/it][INFO|trainer.py:3478] 2024-07-11 10:58:45,940 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/checkpoint-111\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f723eb637f0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: c9f69918-e308-4038-a084-9de235a623d1)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 10:58:56,206 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 10:58:56,214 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 10:58:57,029 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 884.4372, 'train_samples_per_second': 3.053, 'train_steps_per_second': 0.126, 'train_loss': 1.645418643951416, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [14:44<00:00,  7.97s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 10:58:57,036 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f723ea8d4b0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 67e20adf-0eab-4b3c-b250-ad49388dbaf9)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 10:59:07,386 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 10:59:07,395 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8142145GF\n",
      "  train_loss               =     1.6454\n",
      "  train_runtime            = 0:14:44.43\n",
      "  train_samples_per_second =      3.053\n",
      "  train_steps_per_second   =      0.126\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 10:59:08,167 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 10:59:08,167 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 10:59:08,167 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.93it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.4141\n",
      "  eval_runtime            = 0:00:06.40\n",
      "  eval_samples_per_second =     15.623\n",
      "  eval_steps_per_second   =      7.811\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-11 10:59:14,596 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d5fa37-2d80-4197-ae5f-8e9e58eed058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 10:59:33,978] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 10:59:38 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 10:59:38,848 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 10:59:39,377 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 10:59:39 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 10:59:39 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "Generating train split: 1273 examples [00:00, 46065.41 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 247.10\n",
      "07/11/2024 10:59:46 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "Generating train split: 100 examples [00:00, 8381.73 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 297.88\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 200/200 [00:02<00:00, 80.36 \n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 10:59:50,761 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 10:59:50,762 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 10:59:50 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 10:59:50,809 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 10:59:52,759 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 10:59:52,763 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 11:01:11,002 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 11:01:11,002 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 11:01:11,007 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:01:11,007 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 11:01:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 11:01:59 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 11:01:59 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5\n",
      "07/11/2024 11:01:59 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:3788] 2024-07-11 11:01:59,841 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:01:59,841 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:01:59,841 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 11:02:00,602 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [02:21<00:00,  1.63it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.692 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [02:22<00:00,  1.42s/it]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =    24.7251\n",
      "  predict_rouge-1            =    31.8137\n",
      "  predict_rouge-2            =    15.4335\n",
      "  predict_rouge-l            =    27.6748\n",
      "  predict_runtime            = 0:02:23.31\n",
      "  predict_samples_per_second =      1.396\n",
      "  predict_steps_per_second   =      0.698\n",
      "07/11/2024 11:04:23 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-34_lr=1e-5/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-34_lr=1e-5 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_lr=1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "batch size = 3, max_exmaple = 500, _lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:04:40,672] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 11:04:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:04:45,260 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 11:04:45,555 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 11:04:45 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 11:04:45 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/11/2024 11:04:51 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 11:04:52,123 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 11:04:52,125 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 11:04:52,176 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 11:04:53,328 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:04:53,332 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 11:06:45,257 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 11:06:45,258 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 11:06:45,264 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:06:45,265 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 11:06:45 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 11:06:45 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 11:06:45 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 11:06:45 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 11:06:45 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,v_proj,o_proj,k_proj,up_proj,down_proj\n",
      "07/11/2024 11:07:07 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-11 11:07:07,889 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 11:07:08,312 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 11:07:08,312 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 11:07:08,312 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-11 11:07:08,312 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 11:07:08,312 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 11:07:08,312 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 11:07:08,312 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-11 11:07:08,316 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.7651, 'grad_norm': 0.8094133734703064, 'learning_rate': 9.950018542108818e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.594, 'grad_norm': 0.5813366770744324, 'learning_rate': 9.801073426888447e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [01:22<10:09,  6.03s/it][INFO|trainer.py:3788] 2024-07-11 11:08:30,662 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:08:30,662 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:08:30,662 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 14.61it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.73it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.51it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.61it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.78it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.85it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.63it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.60it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.86it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.93it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.76it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.78it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.65it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.80it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.38it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.45it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.23it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.11it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.12it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.75it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.64it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.19it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.52it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.70it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  6.64it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.28it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.25it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.49it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.56it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3613357543945312, 'eval_runtime': 6.4168, 'eval_samples_per_second': 15.584, 'eval_steps_per_second': 7.792, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [01:28<10:09,  6.03s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.92it/s]\u001b[A\n",
      "{'loss': 1.4653, 'grad_norm': 0.6813035607337952, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.4552, 'grad_norm': 0.5751915574073792, 'learning_rate': 9.220122420149753e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [02:21<07:50,  5.17s/it][INFO|trainer.py:3788] 2024-07-11 11:09:29,872 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:09:29,872 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:09:29,872 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.87it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.78it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.70it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.84it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.42it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.29it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.21it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.10it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.44it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.32it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.11it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.09it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.74it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.44it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.61it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  9.09it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.25it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.61it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.68it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  8.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.42it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  8.26it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  7.86it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.51it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.44it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2845146656036377, 'eval_runtime': 5.7642, 'eval_samples_per_second': 17.349, 'eval_steps_per_second': 8.674, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [02:27<07:50,  5.17s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.41it/s]\u001b[A\n",
      "{'loss': 1.4429, 'grad_norm': 0.45014914870262146, 'learning_rate': 8.799731239943487e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2188, 'grad_norm': 0.5129408240318298, 'learning_rate': 8.303373616950408e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [03:19<07:05,  5.25s/it][INFO|trainer.py:3788] 2024-07-11 11:10:27,822 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:10:27,823 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:10:27,823 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.38it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.45it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.22it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.40it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.85it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.75it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.29it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.85it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.93it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.27it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.25it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.45it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.29it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.41it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.71it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.96it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.08it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.24it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.44it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.94it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  7.22it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.04it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2750855684280396, 'eval_runtime': 5.936, 'eval_samples_per_second': 16.846, 'eval_steps_per_second': 8.423, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [03:25<07:05,  5.25s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.02it/s]\u001b[A\n",
      "{'loss': 1.5147, 'grad_norm': 0.5635349750518799, 'learning_rate': 7.740973022223549e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3766, 'grad_norm': 0.5143546462059021, 'learning_rate': 7.12377329642024e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [04:28<07:02,  5.95s/it][INFO|trainer.py:3788] 2024-07-11 11:11:36,606 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:11:36,606 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:11:36,606 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.25it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.95it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.54it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.90it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.36it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.03it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.68it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.63it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.11it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.15it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.79it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.09it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.86it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.78it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.56it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.42it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.58it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.88it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.14it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.84it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.39it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.53it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.81it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.74it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.59it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.11it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.81it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.03it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2715747356414795, 'eval_runtime': 6.1769, 'eval_samples_per_second': 16.189, 'eval_steps_per_second': 8.095, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [04:34<07:02,  5.95s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.99it/s]\u001b[A\n",
      "{'loss': 1.4396, 'grad_norm': 0.5926064252853394, 'learning_rate': 6.464113856382752e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.3966, 'grad_norm': 0.5103179216384888, 'learning_rate': 5.7751829983209896e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [05:33<06:00,  5.90s/it][INFO|trainer.py:3788] 2024-07-11 11:12:41,358 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:12:41,359 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:12:41,359 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.26it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.48it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.58it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.07it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:05,  6.92it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  6.58it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:05,  6.49it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:05,  6.08it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  6.46it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  6.70it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  6.21it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  6.74it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  6.29it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  7.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.27it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  9.92it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:01, 10.90it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 11.49it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01, 11.44it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01, 11.66it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:00, 12.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:00, 11.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 12.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 12.29it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 12.30it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00, 11.45it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.267932653427124, 'eval_runtime': 5.4961, 'eval_samples_per_second': 18.195, 'eval_steps_per_second': 9.097, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [05:38<06:00,  5.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00, 10.55it/s]\u001b[A\n",
      "{'loss': 1.3355, 'grad_norm': 0.5727438926696777, 'learning_rate': 5.0707542297038114e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3167, 'grad_norm': 0.531753420829773, 'learning_rate': 4.364910901265606e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [06:39<05:33,  6.54s/it][INFO|trainer.py:3788] 2024-07-11 11:13:48,225 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:13:48,225 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:13:48,225 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.20it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.44it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.14it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.06it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.57it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.04it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.38it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.23it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  9.26it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.88it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.57it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.95it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.01it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  9.06it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.47it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.28it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.17it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  8.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.24it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  6.69it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  5.83it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  6.43it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  5.59it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  5.59it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:01,  5.50it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  5.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.63it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2657134532928467, 'eval_runtime': 6.2995, 'eval_samples_per_second': 15.874, 'eval_steps_per_second': 7.937, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [06:46<05:33,  6.54s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.49it/s]\u001b[A\n",
      "{'loss': 1.3093, 'grad_norm': 0.5866958498954773, 'learning_rate': 3.6717646444456193e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.2442, 'grad_norm': 0.47562652826309204, 'learning_rate': 3.0051732434229184e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [07:46<04:06,  6.00s/it][INFO|trainer.py:3788] 2024-07-11 11:14:54,514 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:14:54,514 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:14:54,514 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.95it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:06,  7.58it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:04,  9.11it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:05,  8.48it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  7.21it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:06,  6.48it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  8.56it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:05,  7.19it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  7.26it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:05,  7.15it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:05,  6.39it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  6.64it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.12it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.05it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  6.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:07,  3.82it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:07,  3.38it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:07,  3.51it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:04,  4.96it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:04,  4.84it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:04,  5.01it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:03,  6.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.14it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  7.20it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.94it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:01,  7.69it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  8.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  8.36it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  7.53it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  6.67it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.81it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.53it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:00,  6.03it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.36it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.50it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.266728162765503, 'eval_runtime': 8.2756, 'eval_samples_per_second': 12.084, 'eval_steps_per_second': 6.042, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [07:54<04:06,  6.00s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.54it/s]\u001b[A\n",
      "{'loss': 1.4948, 'grad_norm': 0.6982643008232117, 'learning_rate': 2.3784635822138424e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.4333, 'grad_norm': 0.5703365206718445, 'learning_rate': 1.8041652058350767e-05, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [08:58<03:11,  6.18s/it][INFO|trainer.py:3788] 2024-07-11 11:16:07,191 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:16:07,191 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:16:07,192 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.99it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.11it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.94it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.98it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.29it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.93it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.85it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.94it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.41it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.49it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.27it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.02it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  6.96it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.81it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.26it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.66it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.80it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.57it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.58it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.76it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.22it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.40it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.46it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.74it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.69it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.14it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2665677070617676, 'eval_runtime': 6.3362, 'eval_samples_per_second': 15.782, 'eval_steps_per_second': 7.891, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [09:05<03:11,  6.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.87it/s]\u001b[A\n",
      "{'loss': 1.159, 'grad_norm': 0.5371204614639282, 'learning_rate': 1.2937598223330005e-05, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.3845, 'grad_norm': 0.604777455329895, 'learning_rate': 8.574517537807897e-06, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [10:01<01:56,  5.57s/it][INFO|trainer.py:3788] 2024-07-11 11:17:09,428 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:17:09,428 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:17:09,428 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.84it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.80it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.18it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.74it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.08it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.97it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.49it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.69it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.24it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.56it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.32it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.18it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.08it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.90it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.35it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.67it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.90it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.34it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.51it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.60it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.75it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2670025825500488, 'eval_runtime': 6.4273, 'eval_samples_per_second': 15.559, 'eval_steps_per_second': 7.779, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [10:07<01:56,  5.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.80it/s]\u001b[A\n",
      "{'loss': 1.2856, 'grad_norm': 0.5558770895004272, 'learning_rate': 5.0396392552081564e-06, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.422, 'grad_norm': 0.5736969709396362, 'learning_rate': 2.403634723543674e-06, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [11:07<01:10,  6.44s/it][INFO|trainer.py:3788] 2024-07-11 11:18:15,810 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:18:15,810 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:18:15,810 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.04it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.02it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.67it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.13it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.67it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.32it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.83it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.52it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.98it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.04it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.79it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.76it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.54it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.57it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.44it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.47it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.83it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.39it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.61it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.65it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.06it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.40it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.22it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.02it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.80it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2667909860610962, 'eval_runtime': 6.213, 'eval_samples_per_second': 16.095, 'eval_steps_per_second': 8.048, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [11:13<01:10,  6.44s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.79it/s]\u001b[A\n",
      "{'loss': 1.3076, 'grad_norm': 0.5788726210594177, 'learning_rate': 7.192044826145771e-07, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.2417, 'grad_norm': 0.6642646193504333, 'learning_rate': 2.0024644083921352e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [12:13<00:06,  6.44s/it][INFO|trainer.py:3788] 2024-07-11 11:19:21,591 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:19:21,591 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:19:21,591 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.09it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.57it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.70it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.37it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.49it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.27it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.90it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.76it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.84it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.50it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.64it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.24it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.32it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.48it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.07it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.81it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.64it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.41it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.56it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.66it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.13it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:01,  2.39it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  2.25it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:07<00:00,  2.80it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2666865587234497, 'eval_runtime': 8.0428, 'eval_samples_per_second': 12.433, 'eval_steps_per_second': 6.217, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [12:21<00:06,  6.44s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:07<00:00,  2.89it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [12:27<00:00,  8.79s/it][INFO|trainer.py:3478] 2024-07-11 11:19:35,858 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/checkpoint-111\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f323840a4a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 0fd84210-fa40-430d-bf21-36c9e13ab178)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 11:19:46,288 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 11:19:46,298 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 11:19:47,361 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 759.0457, 'train_samples_per_second': 3.557, 'train_steps_per_second': 0.146, 'train_loss': 1.393181021149094, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [12:39<00:00,  6.84s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 11:19:47,373 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f323831ffa0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 649c5b57-f68c-457d-a9f5-47fcd44850cb)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 11:19:57,838 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 11:19:57,848 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8142145GF\n",
      "  train_loss               =     1.3932\n",
      "  train_runtime            = 0:12:39.04\n",
      "  train_samples_per_second =      3.557\n",
      "  train_steps_per_second   =      0.146\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 11:19:58,581 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:19:58,581 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:19:58,581 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.86it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.2667\n",
      "  eval_runtime            = 0:00:04.31\n",
      "  eval_samples_per_second =     23.152\n",
      "  eval_steps_per_second   =     11.576\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-11 11:20:02,950 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163af63b-92f7-4cdc-bbfb-a10985cf7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:20:20,680] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 11:20:25 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:20:25,140 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 11:20:25,471 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 11:20:25 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 11:20:25 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "07/11/2024 11:20:31 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 11:20:32,120 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 11:20:32,121 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 11:20:32 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 11:20:32,165 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 11:20:33,089 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:20:33,094 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 11:21:52,686 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 11:21:52,687 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 11:21:52,698 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:21:52,699 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 11:21:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 11:22:44 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 11:22:44 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4\n",
      "07/11/2024 11:22:44 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:3788] 2024-07-11 11:22:44,263 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:22:44,263 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:22:44,263 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 11:22:44,855 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:36<00:00,  1.26s/it]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.635 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [01:37<00:00,  1.02it/s]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =    32.2351\n",
      "  predict_rouge-1            =    40.0692\n",
      "  predict_rouge-2            =     20.046\n",
      "  predict_rouge-l            =    35.6594\n",
      "  predict_runtime            = 0:01:38.56\n",
      "  predict_samples_per_second =      2.029\n",
      "  predict_steps_per_second   =      1.015\n",
      "07/11/2024 11:24:22 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-35_lr=1e-4/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-35_lr=1e-4 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_lr=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "batch size = 3, max_example = 500, lr=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e72b73d-6bb7-4671-8ebc-5efb909cfafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:24:41,046] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 11:24:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:24:45,446 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 11:24:45,720 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 11:24:45 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 11:24:45 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/11/2024 11:24:51 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 11:24:52,595 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 11:24:52,597 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 11:24:52,643 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 11:24:53,543 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:24:53,547 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 11:26:48,944 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 11:26:48,944 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 11:26:48,967 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:26:48,967 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 11:26:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 11:26:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 11:26:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 11:26:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 11:26:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,k_proj,down_proj,up_proj,gate_proj,v_proj\n",
      "07/11/2024 11:27:08 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-11 11:27:08,297 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 11:27:08,777 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 11:27:08,777 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 11:27:08,777 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-11 11:27:08,777 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 11:27:08,777 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 11:27:08,777 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 11:27:08,777 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-11 11:27:08,781 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.6015, 'grad_norm': 0.7149219512939453, 'learning_rate': 0.0004975009271054409, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.4568, 'grad_norm': 0.5460855960845947, 'learning_rate': 0.0004900536713444224, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [01:36<11:36,  6.90s/it][INFO|trainer.py:3788] 2024-07-11 11:28:44,981 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:28:44,981 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:28:44,981 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 14.13it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.41it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.38it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.86it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:02<00:12,  2.95it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:12,  2.92it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:11,  3.11it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:10,  3.22it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:03<00:09,  3.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:03<00:07,  4.31it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:03<00:06,  4.73it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:06,  4.68it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:04<00:04,  6.09it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:04<00:04,  5.81it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:04<00:04,  6.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:03,  6.58it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:02,  8.49it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:05<00:02,  8.64it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:05<00:02,  8.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:05<00:02,  9.30it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:02,  8.71it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:05<00:02,  8.89it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:01,  8.99it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:01,  8.87it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:06<00:01,  9.29it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01, 10.12it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  9.09it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  8.20it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:00,  8.69it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:00,  8.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:00,  8.30it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:07<00:00,  8.48it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  8.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  7.45it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  6.35it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2834669351577759, 'eval_runtime': 8.0068, 'eval_samples_per_second': 12.489, 'eval_steps_per_second': 6.245, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [01:44<11:36,  6.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:07<00:00,  6.77it/s]\u001b[A\n",
      "{'loss': 1.421, 'grad_norm': 0.634940505027771, 'learning_rate': 0.00047780712259703394, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.4325, 'grad_norm': 0.4698368310928345, 'learning_rate': 0.00046100612100748764, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [02:46<08:54,  5.87s/it][INFO|trainer.py:3788] 2024-07-11 11:29:55,344 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:29:55,344 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:29:55,344 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.72it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.40it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.91it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.86it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.34it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.14it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.96it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.39it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.70it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:05,  5.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  6.41it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  5.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.62it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.68it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  7.33it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:04,  5.55it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:04,  5.45it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:03,  6.64it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.08it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  6.96it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  6.48it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.96it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  7.24it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  7.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:01,  6.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  6.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:05<00:01,  6.74it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.05it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:01,  5.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  6.22it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  5.47it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.270946741104126, 'eval_runtime': 7.5345, 'eval_samples_per_second': 13.272, 'eval_steps_per_second': 6.636, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [02:54<08:54,  5.87s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:07<00:00,  5.78it/s]\u001b[A\n",
      "{'loss': 1.434, 'grad_norm': 0.38827013969421387, 'learning_rate': 0.00043998656199717434, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2143, 'grad_norm': 0.4654407501220703, 'learning_rate': 0.00041516868084752034, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [03:53<07:58,  5.90s/it][INFO|trainer.py:3788] 2024-07-11 11:31:02,714 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:31:02,714 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:31:02,714 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.87it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.23it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.63it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.52it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.90it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.48it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.24it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.86it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.23it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.76it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.29it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:04,  6.17it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:04,  6.00it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  7.41it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:03,  6.37it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  5.80it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  6.68it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:03,  6.01it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  6.72it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  6.11it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:02,  6.29it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:02,  6.48it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.50it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:01,  5.86it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  5.55it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.18it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:00,  6.40it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  6.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  6.86it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2702041864395142, 'eval_runtime': 6.9769, 'eval_samples_per_second': 14.333, 'eval_steps_per_second': 7.167, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [04:00<07:58,  5.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.17it/s]\u001b[A\n",
      "{'loss': 1.5086, 'grad_norm': 0.4756075143814087, 'learning_rate': 0.0003870486511111775, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3106, 'grad_norm': 0.38932737708091736, 'learning_rate': 0.000356188664821012, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [05:02<07:29,  6.33s/it][INFO|trainer.py:3788] 2024-07-11 11:32:10,856 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:32:10,856 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:32:10,856 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:04,  9.66it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:06,  7.34it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:04,  9.59it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:05,  8.48it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  8.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.42it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.13it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.68it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.86it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:05,  6.97it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:02<00:04,  8.16it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.11it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  7.87it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.50it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  7.40it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.48it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.34it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.70it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  7.86it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.56it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  8.23it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.21it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:02,  7.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  7.00it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  6.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:01,  5.75it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  5.10it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:05<00:01,  5.74it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  5.33it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  5.47it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:01,  5.46it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  6.65it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  6.05it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2676769495010376, 'eval_runtime': 7.0855, 'eval_samples_per_second': 14.113, 'eval_steps_per_second': 7.057, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [05:09<07:29,  6.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.75it/s]\u001b[A\n",
      "{'loss': 1.3118, 'grad_norm': 0.4965905547142029, 'learning_rate': 0.0003232056928191376, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.2331, 'grad_norm': 0.5039529204368591, 'learning_rate': 0.00028875914991604944, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [06:10<06:09,  6.05s/it][INFO|trainer.py:3788] 2024-07-11 11:33:19,080 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:33:19,081 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:33:19,081 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.64it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  9.13it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.21it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  8.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.67it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.56it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  9.37it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.10it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.01it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.94it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.70it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.12it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.70it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.88it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.63it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.66it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.62it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.97it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  9.00it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.50it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.76it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.39it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.53it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.59it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.71it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.38it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  6.92it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.11it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  5.81it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:01,  5.46it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.05it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  5.25it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  4.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2807247638702393, 'eval_runtime': 6.9394, 'eval_samples_per_second': 14.411, 'eval_steps_per_second': 7.205, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [06:17<06:09,  6.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  4.77it/s]\u001b[A\n",
      "{'loss': 1.17, 'grad_norm': 0.6674433350563049, 'learning_rate': 0.00025353771148519056, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.1452, 'grad_norm': 0.6766863465309143, 'learning_rate': 0.0002182455450632803, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [07:17<05:06,  6.01s/it][INFO|trainer.py:3788] 2024-07-11 11:34:26,507 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:34:26,507 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:34:26,507 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.63it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.78it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.34it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.08it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.46it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.17it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.98it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.03it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.42it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.71it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.52it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.57it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.23it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.31it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.82it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.68it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.41it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.49it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.59it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.25it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.40it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.49it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.70it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.67it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.58it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.80it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.13it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.30it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.11it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.09it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.12it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3039851188659668, 'eval_runtime': 6.0169, 'eval_samples_per_second': 16.62, 'eval_steps_per_second': 8.31, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [07:23<05:06,  6.01s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.69it/s]\u001b[A\n",
      "{'loss': 1.1301, 'grad_norm': 0.6839889287948608, 'learning_rate': 0.00018358823222228097, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.079, 'grad_norm': 0.6698341369628906, 'learning_rate': 0.00015025866217114592, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [08:11<03:30,  5.13s/it][INFO|trainer.py:3788] 2024-07-11 11:35:20,217 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:35:20,217 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:35:20,217 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.97it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.78it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.33it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.62it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.61it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.93it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.19it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.38it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.32it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  9.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.43it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  8.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:02,  9.10it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.82it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:05,  2.80it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:04,  3.11it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:03,  3.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:03,  3.79it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:02,  3.92it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:02,  4.00it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  4.90it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  5.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:01,  5.41it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  6.60it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  6.19it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  6.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.314318060874939, 'eval_runtime': 7.5589, 'eval_samples_per_second': 13.229, 'eval_steps_per_second': 6.615, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [08:18<03:30,  5.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:07<00:00,  7.21it/s]\u001b[A\n",
      "{'loss': 1.2424, 'grad_norm': 0.865300714969635, 'learning_rate': 0.00011892317911069211, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.0629, 'grad_norm': 0.688367486000061, 'learning_rate': 9.020826029175384e-05, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [09:13<02:41,  5.20s/it][INFO|trainer.py:3788] 2024-07-11 11:36:22,358 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:36:22,358 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:36:22,359 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.97it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.94it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.17it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.43it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.89it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.70it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.01it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.48it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 10.91it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.35it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.15it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.50it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.83it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.57it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  9.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.58it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.40it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 10.41it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.15it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  7.90it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  7.13it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3389315605163574, 'eval_runtime': 5.2968, 'eval_samples_per_second': 18.879, 'eval_steps_per_second': 9.44, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [09:18<02:41,  5.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.49it/s]\u001b[A\n",
      "{'loss': 0.8137, 'grad_norm': 0.7149925827980042, 'learning_rate': 6.468799111665003e-05, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.0191, 'grad_norm': 1.0583235025405884, 'learning_rate': 4.287258768903948e-05, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [10:08<01:49,  5.24s/it][INFO|trainer.py:3788] 2024-07-11 11:37:16,897 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:37:16,898 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:37:16,898 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.40it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.30it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.11it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.69it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.93it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.82it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.03it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.16it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.63it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.98it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.12it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.85it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.41it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.63it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.01it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.96it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.55it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.28it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.55it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.50it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.77it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.71it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.88it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.61it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.50it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.81it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.28it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3883798122406006, 'eval_runtime': 5.9482, 'eval_samples_per_second': 16.812, 'eval_steps_per_second': 8.406, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [10:14<01:49,  5.24s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  8.35it/s]\u001b[A\n",
      "{'loss': 0.9032, 'grad_norm': 0.815992534160614, 'learning_rate': 2.519819627604078e-05, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 0.9946, 'grad_norm': 0.9519784450531006, 'learning_rate': 1.201817361771837e-05, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [11:10<01:08,  6.21s/it][INFO|trainer.py:3788] 2024-07-11 11:38:19,394 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:38:19,395 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:38:19,395 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.95it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.83it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.11it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  8.96it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  8.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:05,  7.44it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  6.98it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.47it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.02it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.49it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.71it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.10it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  7.19it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  6.80it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.91it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.85it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.26it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.29it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.24it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.80it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:01,  8.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01,  8.47it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.74it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  9.57it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  9.00it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:00,  9.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  9.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  9.15it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  9.47it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  8.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  8.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4038498401641846, 'eval_runtime': 6.0495, 'eval_samples_per_second': 16.53, 'eval_steps_per_second': 8.265, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [11:16<01:08,  6.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  8.99it/s]\u001b[A\n",
      "{'loss': 0.9156, 'grad_norm': 1.025201439857483, 'learning_rate': 3.5960224130728858e-06, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 0.881, 'grad_norm': 1.0855685472488403, 'learning_rate': 1.0012322041960675e-07, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [12:13<00:05,  5.82s/it][INFO|trainer.py:3788] 2024-07-11 11:39:22,023 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:39:22,024 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:39:22,024 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 14.03it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.14it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.43it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.57it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.82it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  9.02it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.90it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.72it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.91it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.74it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.31it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.71it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.98it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  7.93it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  7.31it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.72it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.60it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.62it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.50it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.78it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.81it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.21it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.80it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  5.98it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4052067995071411, 'eval_runtime': 6.4648, 'eval_samples_per_second': 15.468, 'eval_steps_per_second': 7.734, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [12:19<00:05,  5.82s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.32it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [12:24<00:00,  7.48s/it][INFO|trainer.py:3478] 2024-07-11 11:39:33,365 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/checkpoint-111\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f03fc6524a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 0ee4a2f1-4eee-4288-9fcf-a2076922b7a5)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 11:39:43,892 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 11:39:43,902 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 11:39:45,123 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 756.3414, 'train_samples_per_second': 3.57, 'train_steps_per_second': 0.147, 'train_loss': 1.1941010565371126, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [12:36<00:00,  6.81s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 11:39:45,132 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f03fc567ee0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: dad4df72-6f47-4141-8196-c68b03e2db46)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 11:39:55,575 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 11:39:55,585 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8142145GF\n",
      "  train_loss               =     1.1941\n",
      "  train_runtime            = 0:12:36.34\n",
      "  train_samples_per_second =       3.57\n",
      "  train_steps_per_second   =      0.147\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 11:39:56,293 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:39:56,293 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:39:56,293 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.95it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.4051\n",
      "  eval_runtime            = 0:00:05.16\n",
      "  eval_samples_per_second =     19.379\n",
      "  eval_steps_per_second   =       9.69\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-11 11:40:01,481 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8e1ccb-eab2-4618-a501-20068aab09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 11:40:23,069] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 11:40:27 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,628 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,628 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,628 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,628 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,629 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 11:40:27,629 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 11:40:28,101 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 11:40:28 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 11:40:28 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "07/11/2024 11:40:33 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 11:40:44,676 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 11:40:44,677 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 11:40:44 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 11:40:44,721 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 11:40:45,518 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:40:45,522 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 11:44:43,783 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 11:44:43,783 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 11:44:43,790 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 11:44:43,791 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 11:44:44 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 11:46:20 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 11:46:20 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4\n",
      "07/11/2024 11:46:20 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:3788] 2024-07-11 11:46:20,234 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 11:46:20,234 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 11:46:20,234 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 11:46:20,852 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:45<00:00,  1.83s/it]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.818 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [01:46<00:00,  1.07s/it]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =    34.0195\n",
      "  predict_rouge-1            =     40.804\n",
      "  predict_rouge-2            =    20.4076\n",
      "  predict_rouge-l            =    35.9407\n",
      "  predict_runtime            = 0:01:47.64\n",
      "  predict_samples_per_second =      1.858\n",
      "  predict_steps_per_second   =      0.929\n",
      "07/11/2024 11:48:07 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-36_lr=5e-4/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-36_lr=5e-4 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_lr=5e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
