{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.41.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/5c/244db59e074e80248fdfa60495eeee257e4d97c3df3487df68be30cd60c8/transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.16.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/2d/963b266bb8f88492d5ab4232d74292af8beb5b6fdae97902df9e284d4c32/datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.30.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/74/564f621699b049b0358f7ad83d7437f8219a5d6efb69bbfcca328b60152f/accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl>=0.8.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/8c/7d1007557b343d5cf18349802e94d3a14397121e9105b4661f8cd753f9bf/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/54/eb7fcfc0e1ec6a8404cadd11ac957b3ee4fd0774225cafe3ffe6287861cb/pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting tyro>=0.5.11 (from trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/ea/c8967a64769ec465a2d49bf81e1e135999741704a36993b6b51465ce8503/tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/71/6ce4136149cb42b98599d49c39b3a39dd6858b5f9307490998c40e26a51e/huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/a3/16e9fe32187e9c8bc7f9b7bcd9728529faa725231a0c96f2f98714ff2fc5/fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=b1e7f48edc33d2dc2b06559778f28994078a9cfc3866cef58fbac03b3a7801b8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zm4yy4nh/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: tqdm, shtab, requests, pyarrow, fsspec, docstring-parser, tiktoken, huggingface-hub, tyro, tokenizers, transformers, datasets, accelerate, trl, peft, llamafactory\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.15\n",
      "    Uninstalling docstring-parser-0.15:\n",
      "      Successfully uninstalled docstring-parser-0.15\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.7\n",
      "    Uninstalling datasets-2.14.7:\n",
      "      Successfully uninstalled datasets-2.14.7\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.24.1\n",
      "    Uninstalling accelerate-0.24.1:\n",
      "      Successfully uninstalled accelerate-0.24.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchx 0.6.0 requires fsspec==2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 docstring-parser-0.16 fsspec-2024.5.0 huggingface-hub-0.23.4 llamafactory-0.8.2.dev0 peft-0.11.1 pyarrow-16.1.0 requests-2.32.3 shtab-1.7.1 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.3 trl-0.9.6 tyro-0.8.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/1b/12521efcbc6058e2673583bb096c2b5046a9df39bd73eca392c1efed24e5/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cuda 23.10.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting optimum\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/e4/f832e42a1eb9d5ac4fa6379295e05aebeae507d171babc1786bfa0210299/optimum-1.21.2-py3-none-any.whl (424 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.3)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.21.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found existing installation: apex 0.9.10.dev0\n",
      "Uninstalling apex-0.9.10.dev0:\n",
      "  Successfully uninstalled apex-0.9.10.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd83f-942a-4b63-9e4e-e08578379499",
   "metadata": {},
   "source": [
    "batch size = 3, max_example = 500, lr = 1e-4, epoch=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 12:23:23,556] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 12:23:42 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:23:42,167 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 12:23:42,554 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 12:23:42 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 12:23:42 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "Generating train split: 10178 examples [00:00, 56915.56 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1279.5\n",
      "07/11/2024 12:23:49 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Generating train split: 900 examples [00:00, 12544.98 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1359.7\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:03<00:00, 282.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 12:23:54,726 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 12:23:54,738 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 12:23:55,321 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 12:23:55,952 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:23:55,957 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 12:24:43,711 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 12:24:43,712 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 12:24:43,725 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:24:43,726 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 12:24:44 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 12:24:44 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 12:24:44 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 12:24:44 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 12:24:44 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,up_proj,gate_proj,o_proj,q_proj,k_proj,v_proj\n",
      "07/11/2024 12:24:46 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-11 12:24:46,141 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 12:24:47,019 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 12:24:47,019 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 12:24:47,019 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-11 12:24:47,019 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 12:24:47,019 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 12:24:47,019 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 12:24:47,020 >>   Total optimization steps = 74\n",
      "[INFO|trainer.py:2137] 2024-07-11 12:24:47,028 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.7651, 'grad_norm': 0.8111330270767212, 'learning_rate': 9.887776194738432e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.5946, 'grad_norm': 0.58365398645401, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [01:08<05:45,  5.40s/it][INFO|trainer.py:3788] 2024-07-11 12:25:55,323 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:25:55,323 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:25:55,324 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.11it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.92it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.66it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.08it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.66it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.49it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.46it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.42it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.10it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 10.48it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 10.41it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 10.56it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 11.22it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:01, 11.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.36it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 10.19it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.27it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  8.28it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.43it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.70it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  6.90it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.04it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  6.76it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.28it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.80it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3626350164413452, 'eval_runtime': 6.027, 'eval_samples_per_second': 16.592, 'eval_steps_per_second': 8.296, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      " 14%|█████▊                                     | 10/74 [01:14<05:45,  5.40s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  6.61it/s]\u001b[A\n",
      "{'loss': 1.4668, 'grad_norm': 0.6838335990905762, 'learning_rate': 9.019985651834703e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.4563, 'grad_norm': 0.5687180161476135, 'learning_rate': 8.303373616950408e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [02:09<05:00,  5.57s/it][INFO|trainer.py:3788] 2024-07-11 12:26:56,867 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:26:56,867 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:26:56,867 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.30it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.19it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.00it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.44it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.93it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.50it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.15it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.22it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.65it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.84it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.43it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.47it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.23it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.96it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.35it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.75it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.58it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.45it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.80it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.53it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.15it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.81it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.99it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.27it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2853671312332153, 'eval_runtime': 6.3556, 'eval_samples_per_second': 15.734, 'eval_steps_per_second': 7.867, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 27%|███████████▌                               | 20/74 [02:16<05:00,  5.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.89it/s]\u001b[A\n",
      "{'loss': 1.4437, 'grad_norm': 0.4489496052265167, 'learning_rate': 7.438474719068173e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2196, 'grad_norm': 0.5133236050605774, 'learning_rate': 6.464113856382752e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [03:12<04:07,  5.62s/it][INFO|trainer.py:3788] 2024-07-11 12:27:59,180 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:27:59,181 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:27:59,181 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.04it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.82it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.49it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.92it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:04,  9.00it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  9.61it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.09it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  9.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.07it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.22it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.78it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.19it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.01it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.16it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:03,  7.52it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  7.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.68it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.29it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.82it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.57it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.55it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.76it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.82it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.82it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.92it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:05<00:00,  8.02it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.07it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.93it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2759456634521484, 'eval_runtime': 6.1751, 'eval_samples_per_second': 16.194, 'eval_steps_per_second': 8.097, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 41%|█████████████████▍                         | 30/74 [03:18<04:07,  5.62s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.35it/s]\u001b[A\n",
      "{'loss': 1.5165, 'grad_norm': 0.5633290410041809, 'learning_rate': 5.4240296223775465e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3798, 'grad_norm': 0.48710763454437256, 'learning_rate': 4.364910901265606e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [04:18<03:18,  5.85s/it][INFO|trainer.py:3788] 2024-07-11 12:29:06,010 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:29:06,011 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:29:06,011 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.05it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.82it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.65it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.55it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.61it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.31it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.35it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.63it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.97it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.46it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.35it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.37it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.15it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.99it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  6.88it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.75it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.98it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.47it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.55it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.65it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.92it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.31it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.24it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2719694375991821, 'eval_runtime': 6.443, 'eval_samples_per_second': 15.521, 'eval_steps_per_second': 7.76, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 54%|███████████████████████▏                   | 40/74 [04:25<03:18,  5.85s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.89it/s]\u001b[A\n",
      "{'loss': 1.4461, 'grad_norm': 0.6118695139884949, 'learning_rate': 3.334301026289712e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.4044, 'grad_norm': 0.527836799621582, 'learning_rate': 2.3784635822138424e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [05:26<02:24,  6.03s/it][INFO|trainer.py:3788] 2024-07-11 12:30:13,112 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:30:13,113 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:30:13,113 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.36it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.27it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.06it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.87it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.38it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.75it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.38it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.30it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.64it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.01it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.59it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.72it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.22it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.12it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.58it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.46it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.80it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.60it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.24it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.42it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.31it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.63it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.55it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.27it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.80it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.87it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2695385217666626, 'eval_runtime': 6.3917, 'eval_samples_per_second': 15.645, 'eval_steps_per_second': 7.823, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 68%|█████████████████████████████              | 50/74 [05:32<02:24,  6.03s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.92it/s]\u001b[A\n",
      "{'loss': 1.3426, 'grad_norm': 0.5573875308036804, 'learning_rate': 1.5403056551122697e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3264, 'grad_norm': 0.5230586528778076, 'learning_rate': 8.574517537807897e-06, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [06:29<01:20,  5.75s/it][INFO|trainer.py:3788] 2024-07-11 12:31:16,591 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:31:16,591 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:31:16,591 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.88it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.34it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.53it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.79it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.26it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.69it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.96it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.34it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.54it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.30it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.28it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.83it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.11it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.75it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  7.98it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.41it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.81it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.54it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.94it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.38it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.50it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.17it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.83it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.62it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2682921886444092, 'eval_runtime': 6.4208, 'eval_samples_per_second': 15.574, 'eval_steps_per_second': 7.787, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 81%|██████████████████████████████████▊        | 60/74 [06:35<01:20,  5.75s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.72it/s]\u001b[A\n",
      "{'loss': 1.322, 'grad_norm': 0.5719035863876343, 'learning_rate': 3.605548635174533e-06, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.255, 'grad_norm': 0.4788074195384979, 'learning_rate': 7.192044826145771e-07, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [07:30<00:22,  5.63s/it][INFO|trainer.py:3788] 2024-07-11 12:32:17,742 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:32:17,742 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:32:17,742 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.51it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.50it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.65it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.13it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.78it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.43it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.96it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.17it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.47it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.71it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.43it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.44it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.27it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.17it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.98it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.39it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.49it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.77it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.61it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.68it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.56it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.41it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.80it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.87it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2677174806594849, 'eval_runtime': 6.3445, 'eval_samples_per_second': 15.762, 'eval_steps_per_second': 7.881, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [07:37<00:22,  5.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.91it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 74/74 [08:03<00:00,  6.92s/it][INFO|trainer.py:3478] 2024-07-11 12:32:50,343 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/checkpoint-74\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fedb185fd00>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f8d38725-29eb-4e54-9567-2836acb4f300)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 12:33:01,555 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/checkpoint-74/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 12:33:01,567 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/checkpoint-74/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 12:33:04,147 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 497.12, 'train_samples_per_second': 3.621, 'train_steps_per_second': 0.149, 'train_loss': 1.4307718856914624, 'epoch': 1.97, 'num_input_tokens_seen': 739920}\n",
      "100%|███████████████████████████████████████████| 74/74 [08:17<00:00,  6.72s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 12:33:04,161 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fedb17818d0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 0d06fbcd-0bf4-48a1-909b-a89ba02f3ceb)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 12:33:15,269 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 12:33:15,282 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9733\n",
      "  num_input_tokens_seen    =     739920\n",
      "  total_flos               =  5455939GF\n",
      "  train_loss               =     1.4308\n",
      "  train_runtime            = 0:08:17.12\n",
      "  train_samples_per_second =      3.621\n",
      "  train_steps_per_second   =      0.149\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 12:33:16,671 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:33:16,671 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:33:16,671 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.41it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9733\n",
      "  eval_loss               =     1.2677\n",
      "  eval_runtime            = 0:00:04.49\n",
      "  eval_samples_per_second =     22.269\n",
      "  eval_steps_per_second   =     11.135\n",
      "  num_input_tokens_seen   =     739920\n",
      "[INFO|modelcard.py:449] 2024-07-11 12:33:21,208 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d5fa37-2d80-4197-ae5f-8e9e58eed058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 12:34:31,725] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 12:34:51 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:34:51,534 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 12:34:51,978 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 12:34:51 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 12:34:51 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "Generating train split: 1273 examples [00:00, 14158.97 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 257.11\n",
      "07/11/2024 12:34:58 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "Generating train split: 100 examples [00:00, 2962.69 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 292.08\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 200/200 [00:02<00:00, 67.19 \n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 12:35:03,840 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 12:35:03,851 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 12:35:03 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 12:35:04,442 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 12:35:05,479 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:35:05,483 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 12:35:31,193 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 12:35:31,194 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 12:35:31,203 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:35:31,204 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 12:35:31 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 12:35:38 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 12:35:38 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2\n",
      "07/11/2024 12:35:38 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "[INFO|trainer.py:3788] 2024-07-11 12:35:39,246 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:35:39,246 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:35:39,246 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 12:35:46,247 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:51<00:00,  1.57s/it]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.746 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [01:52<00:00,  1.13s/it]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =    32.2894\n",
      "  predict_rouge-1            =    39.2029\n",
      "  predict_rouge-2            =    19.1569\n",
      "  predict_rouge-l            =    34.6529\n",
      "  predict_runtime            = 0:02:00.52\n",
      "  predict_samples_per_second =      1.659\n",
      "  predict_steps_per_second   =       0.83\n",
      "07/11/2024 12:37:39 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-34_epoch=2/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-34_epoch=2 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-25_epoch=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "batch size = 3, max_exmaple = 500, lr=1e-4, epoch=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 12:38:38,322] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 12:39:00 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 12:39:00,067 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 12:39:00,423 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 12:39:00 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 12:39:00 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/11/2024 12:39:06 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 12:39:07,489 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 12:39:07,496 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 12:39:08,045 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 12:39:09,075 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:39:09,080 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 12:39:43,491 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 12:39:43,491 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 12:39:43,504 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 12:39:43,505 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 12:39:43 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 12:39:43 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 12:39:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 12:39:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 12:39:43 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,down_proj,q_proj,up_proj,k_proj,v_proj,o_proj\n",
      "07/11/2024 12:39:45 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-11 12:39:45,616 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 12:39:46,528 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 12:39:46,528 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 12:39:46,528 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2131] 2024-07-11 12:39:46,528 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 12:39:46,528 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 12:39:46,528 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 12:39:46,528 >>   Total optimization steps = 185\n",
      "[INFO|trainer.py:2137] 2024-07-11 12:39:46,533 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.7651, 'grad_norm': 0.8123871684074402, 'learning_rate': 9.981987442712633e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.5938, 'grad_norm': 0.5800332427024841, 'learning_rate': 9.928079551738543e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  5%|██▎                                       | 10/185 [01:04<15:54,  5.45s/it][INFO|trainer.py:3788] 2024-07-11 12:40:51,100 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:40:51,101 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:40:51,101 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.95it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.50it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.36it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.19it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.86it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.51it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.51it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.63it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.50it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.49it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.65it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.73it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02,  9.90it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.93it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.59it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.68it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.57it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.68it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  9.89it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.46it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  7.70it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.01it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:01,  6.86it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  6.46it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.91it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.14it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.360728144645691, 'eval_runtime': 6.2211, 'eval_samples_per_second': 16.074, 'eval_steps_per_second': 8.037, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  5%|██▎                                       | 10/185 [01:10<15:54,  5.45s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  6.13it/s]\u001b[A\n",
      "{'loss': 1.4648, 'grad_norm': 0.6802999377250671, 'learning_rate': 9.838664734667495e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.4546, 'grad_norm': 0.5784505605697632, 'learning_rate': 9.714387227305422e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 11%|████▌                                     | 20/185 [02:08<15:40,  5.70s/it][INFO|trainer.py:3788] 2024-07-11 12:41:54,644 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:41:54,644 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:41:54,644 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.06it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.76it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.46it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.48it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.41it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  9.04it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.44it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.06it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.79it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.68it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.85it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.57it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.62it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.46it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.25it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.02it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.71it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  7.72it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.67it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.63it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.67it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.00it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.38it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.18it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.83it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.66it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2840111255645752, 'eval_runtime': 6.511, 'eval_samples_per_second': 15.359, 'eval_steps_per_second': 7.679, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 11%|████▌                                     | 20/185 [02:14<15:40,  5.70s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.66it/s]\u001b[A\n",
      "{'loss': 1.4428, 'grad_norm': 0.4524359703063965, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2184, 'grad_norm': 0.5147111415863037, 'learning_rate': 9.365070565805941e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 16%|██████▊                                   | 30/185 [03:11<15:02,  5.83s/it][INFO|trainer.py:3788] 2024-07-11 12:42:58,405 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:42:58,405 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:42:58,405 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.62it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.37it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.85it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.97it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.03it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.73it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.55it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.23it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.80it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.90it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.74it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.49it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.99it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.46it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.86it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.13it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.32it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:04<00:02,  7.39it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.94it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.36it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.60it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.74it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.35it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.92it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.71it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2746762037277222, 'eval_runtime': 6.3757, 'eval_samples_per_second': 15.684, 'eval_steps_per_second': 7.842, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 16%|██████▊                                   | 30/185 [03:18<15:02,  5.83s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.69it/s]\u001b[A\n",
      "{'loss': 1.5142, 'grad_norm': 0.5605530142784119, 'learning_rate': 9.142548246219212e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3752, 'grad_norm': 0.5191389918327332, 'learning_rate': 8.890178771592199e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 22%|█████████                                 | 40/185 [04:21<15:23,  6.37s/it][INFO|trainer.py:3788] 2024-07-11 12:44:08,238 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:44:08,239 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:44:08,239 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.82it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  8.94it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.00it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.54it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.41it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.21it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.05it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.93it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.54it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.52it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.52it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.92it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.21it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.49it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.35it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.37it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.16it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.67it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.70it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.51it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.60it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.36it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.99it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.32it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.32it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.08it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.14it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2712141275405884, 'eval_runtime': 6.4559, 'eval_samples_per_second': 15.49, 'eval_steps_per_second': 7.745, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 22%|█████████                                 | 40/185 [04:28<15:23,  6.37s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.89it/s]\u001b[A\n",
      "{'loss': 1.436, 'grad_norm': 0.5792906880378723, 'learning_rate': 8.609780469772623e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.3935, 'grad_norm': 0.5114952921867371, 'learning_rate': 8.303373616950408e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 27%|███████████▎                              | 50/185 [05:28<13:17,  5.91s/it][INFO|trainer.py:3788] 2024-07-11 12:45:14,667 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:45:14,668 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:45:14,668 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.83it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.03it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.71it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.24it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.78it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.47it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.91it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.35it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.67it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.75it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.44it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.35it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.30it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.14it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.49it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.63it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.93it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.38it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.55it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.86it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.32it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.04it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.45it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.27it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2671067714691162, 'eval_runtime': 6.3622, 'eval_samples_per_second': 15.718, 'eval_steps_per_second': 7.859, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 27%|███████████▎                              | 50/185 [05:34<13:17,  5.91s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.95it/s]\u001b[A\n",
      "{'loss': 1.3313, 'grad_norm': 0.5739120841026306, 'learning_rate': 7.973165881521434e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3112, 'grad_norm': 0.5437410473823547, 'learning_rate': 7.621536417786159e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 32%|█████████████▌                            | 60/185 [06:31<12:06,  5.81s/it][INFO|trainer.py:3788] 2024-07-11 12:46:17,902 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:46:17,902 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:46:17,902 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.64it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.81it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.60it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.72it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.22it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.76it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.55it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.42it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.73it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.07it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.65it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.56it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.36it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.16it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.14it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.94it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.46it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.69it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.59it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.83it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.57it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.73it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.46it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.08it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.56it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  7.59it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.33it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.61it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.02it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2660818099975586, 'eval_runtime': 6.4756, 'eval_samples_per_second': 15.443, 'eval_steps_per_second': 7.721, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 32%|█████████████▌                            | 60/185 [06:37<12:06,  5.81s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.79it/s]\u001b[A\n",
      "{'loss': 1.3019, 'grad_norm': 0.6006428003311157, 'learning_rate': 7.251018724088367e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.2393, 'grad_norm': 0.5094550848007202, 'learning_rate': 6.864282388901544e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 38%|███████████████▉                          | 70/185 [07:33<10:57,  5.72s/it][INFO|trainer.py:3788] 2024-07-11 12:47:19,675 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:47:19,675 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:47:19,676 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.75it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.19it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.58it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.36it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.33it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.69it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.88it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.57it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.52it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.39it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.47it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.21it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.23it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.10it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.61it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.94it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.79it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.56it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.77it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.09it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.71it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2680879831314087, 'eval_runtime': 6.4076, 'eval_samples_per_second': 15.606, 'eval_steps_per_second': 7.803, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 38%|███████████████▉                          | 70/185 [07:39<10:57,  5.72s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.76it/s]\u001b[A\n",
      "{'loss': 1.4854, 'grad_norm': 0.7158110737800598, 'learning_rate': 6.464113856382752e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.4045, 'grad_norm': 0.5923131108283997, 'learning_rate': 6.0533963499786314e-05, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 43%|██████████████████▏                       | 80/185 [08:40<10:20,  5.91s/it][INFO|trainer.py:3788] 2024-07-11 12:48:27,495 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:48:27,495 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:48:27,495 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.54it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.34it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.89it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.67it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.20it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.79it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.09it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.44it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.56it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.32it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.08it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.11it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.82it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.42it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.87it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.93it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.71it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.42it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.99it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.26it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.20it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:01,  5.44it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.63it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.01it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.66it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2717220783233643, 'eval_runtime': 6.5368, 'eval_samples_per_second': 15.298, 'eval_steps_per_second': 7.649, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 43%|██████████████████▏                       | 80/185 [08:47<10:20,  5.91s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.51it/s]\u001b[A\n",
      "{'loss': 1.1274, 'grad_norm': 0.5789905190467834, 'learning_rate': 5.6350890987343944e-05, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.3523, 'grad_norm': 0.7213501334190369, 'learning_rate': 5.212206015980742e-05, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 49%|████████████████████▍                     | 90/185 [09:44<08:55,  5.64s/it][INFO|trainer.py:3788] 2024-07-11 12:49:30,601 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:49:30,602 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:49:30,602 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.41it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.86it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.06it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.53it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.33it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.99it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.01it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.57it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.02it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.40it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.49it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.24it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.41it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.05it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.84it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.40it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.33it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.90it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.87it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.90it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.14it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.65it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.28it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.64it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2727714776992798, 'eval_runtime': 6.4963, 'eval_samples_per_second': 15.393, 'eval_steps_per_second': 7.697, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 49%|████████████████████▍                     | 90/185 [09:50<08:55,  5.64s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.73it/s]\u001b[A\n",
      "{'loss': 1.2471, 'grad_norm': 0.6429906487464905, 'learning_rate': 4.78779398401926e-05, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.3792, 'grad_norm': 0.7086110711097717, 'learning_rate': 4.364910901265606e-05, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 54%|██████████████████████▏                  | 100/185 [10:50<08:58,  6.33s/it][INFO|trainer.py:3788] 2024-07-11 12:50:36,717 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:50:36,717 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:50:36,717 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.29it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.69it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.64it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.16it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.62it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.72it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.53it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.95it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.98it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.77it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.66it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.27it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.60it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.90it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.06it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.10it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.83it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.93it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.68it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.93it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.39it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.45it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.00it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.70it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2734733819961548, 'eval_runtime': 6.1809, 'eval_samples_per_second': 16.179, 'eval_steps_per_second': 8.089, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 54%|██████████████████████▏                  | 100/185 [10:56<08:58,  6.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.72it/s]\u001b[A\n",
      "{'loss': 1.2657, 'grad_norm': 0.8150094747543335, 'learning_rate': 3.94660365002137e-05, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.2047, 'grad_norm': 0.8805524706840515, 'learning_rate': 3.5358861436172485e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 59%|████████████████████████▍                | 110/185 [11:52<07:30,  6.00s/it][INFO|trainer.py:3788] 2024-07-11 12:51:39,517 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:51:39,517 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:51:39,517 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.29it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.46it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.74it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:04,  8.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.97it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.42it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.22it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.84it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.95it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.40it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.42it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.01it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.00it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.81it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.82it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.40it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.72it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.95it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:01,  8.88it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01,  8.14it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.46it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.72it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.92it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.34it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.73it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2749145030975342, 'eval_runtime': 6.5307, 'eval_samples_per_second': 15.312, 'eval_steps_per_second': 7.656, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 59%|████████████████████████▍                | 110/185 [11:59<07:30,  6.00s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.72it/s]\u001b[A\n",
      "{'loss': 1.2595, 'grad_norm': 0.8219723701477051, 'learning_rate': 3.135717611098458e-05, 'epoch': 3.07, 'num_input_tokens_seen': 1144800}\n",
      "{'loss': 1.1469, 'grad_norm': 0.8077529072761536, 'learning_rate': 2.748981275911633e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 65%|██████████████████████████▌              | 120/185 [12:56<05:57,  5.51s/it][INFO|trainer.py:3788] 2024-07-11 12:52:43,566 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:52:43,566 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:52:43,566 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.18it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.22it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.13it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  8.95it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.18it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.03it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.95it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.51it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.56it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.42it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.50it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.20it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.36it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  6.99it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.72it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.22it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  7.11it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.43it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  8.37it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.22it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.63it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.77it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.28it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.10it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.85it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.43it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.66it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2780817747116089, 'eval_runtime': 6.5945, 'eval_samples_per_second': 15.164, 'eval_steps_per_second': 7.582, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 65%|██████████████████████████▌              | 120/185 [13:03<05:57,  5.51s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.63it/s]\u001b[A\n",
      "{'loss': 1.3121, 'grad_norm': 0.7830844521522522, 'learning_rate': 2.3784635822138424e-05, 'epoch': 3.33, 'num_input_tokens_seen': 1240056}\n",
      "{'loss': 1.1413, 'grad_norm': 0.6894633769989014, 'learning_rate': 2.026834118478567e-05, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 70%|████████████████████████████▊            | 130/185 [14:02<05:28,  5.97s/it][INFO|trainer.py:3788] 2024-07-11 12:53:48,666 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:53:48,666 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:53:48,666 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.70it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.01it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.33it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.04it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.96it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.98it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.47it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.93it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.78it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.72it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.76it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.47it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:04,  6.39it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  7.58it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.31it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.30it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.20it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.12it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01,  8.12it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.64it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.55it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.40it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.49it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.14it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:05<00:00,  7.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.74it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.52it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.77it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2818201780319214, 'eval_runtime': 6.4454, 'eval_samples_per_second': 15.515, 'eval_steps_per_second': 7.757, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 70%|████████████████████████████▊            | 130/185 [14:08<05:28,  5.97s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.41it/s]\u001b[A\n",
      "{'loss': 1.3501, 'grad_norm': 0.8915462493896484, 'learning_rate': 1.6966263830495936e-05, 'epoch': 3.6, 'num_input_tokens_seen': 1343448}\n",
      "{'loss': 1.0826, 'grad_norm': 0.7574428915977478, 'learning_rate': 1.3902195302273779e-05, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 76%|███████████████████████████████          | 140/185 [15:07<04:20,  5.79s/it][INFO|trainer.py:3788] 2024-07-11 12:54:53,952 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:54:53,952 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:54:53,952 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.89it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.90it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.16it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.38it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.80it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.37it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.98it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.40it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.02it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.00it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.49it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.48it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.22it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.22it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.86it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.42it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.80it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.74it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.81it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.37it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.03it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.39it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.78it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.78it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.93it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.29it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2837378978729248, 'eval_runtime': 6.2989, 'eval_samples_per_second': 15.876, 'eval_steps_per_second': 7.938, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 76%|███████████████████████████████          | 140/185 [15:13<04:20,  5.79s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.72it/s]\u001b[A\n",
      "{'loss': 1.2327, 'grad_norm': 0.9143020510673523, 'learning_rate': 1.1098212284078036e-05, 'epoch': 3.87, 'num_input_tokens_seen': 1441848}\n",
      "{'loss': 1.3732, 'grad_norm': 0.9388176798820496, 'learning_rate': 8.574517537807897e-06, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 81%|█████████████████████████████████▏       | 150/185 [16:15<03:44,  6.42s/it][INFO|trainer.py:3788] 2024-07-11 12:56:01,754 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:56:01,754 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:56:01,754 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.89it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.73it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.85it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.89it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.06it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.17it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  9.02it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.69it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.08it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.87it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.92it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.75it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.77it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.44it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  6.93it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.60it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.53it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.63it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.61it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.67it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.93it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.71it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.12it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.14it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:06<00:00,  6.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2855935096740723, 'eval_runtime': 6.273, 'eval_samples_per_second': 15.941, 'eval_steps_per_second': 7.971, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 81%|█████████████████████████████████▏       | 150/185 [16:21<03:44,  6.42s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.91it/s]\u001b[A\n",
      "{'loss': 1.0772, 'grad_norm': 0.792390763759613, 'learning_rate': 6.349294341940593e-06, 'epoch': 4.13, 'num_input_tokens_seen': 1543032}\n",
      "{'loss': 1.2267, 'grad_norm': 0.8750095963478088, 'learning_rate': 4.43857548059321e-06, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 86%|███████████████████████████████████▍     | 160/185 [17:19<02:42,  6.50s/it][INFO|trainer.py:3788] 2024-07-11 12:57:05,752 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:57:05,752 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:57:05,752 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.04it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.72it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.39it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.90it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.89it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.38it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.85it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.97it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.52it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.57it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.24it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.16it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.33it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.52it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.55it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.54it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.94it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.40it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.92it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.37it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.47it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.11it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.84it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.286484956741333, 'eval_runtime': 6.2767, 'eval_samples_per_second': 15.932, 'eval_steps_per_second': 7.966, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 86%|███████████████████████████████████▍     | 160/185 [17:25<02:42,  6.50s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.79it/s]\u001b[A\n",
      "{'loss': 1.2208, 'grad_norm': 0.8704597353935242, 'learning_rate': 2.85612772694579e-06, 'epoch': 4.4, 'num_input_tokens_seen': 1644360}\n",
      "{'loss': 1.1249, 'grad_norm': 0.8583284616470337, 'learning_rate': 1.6133526533250565e-06, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 92%|█████████████████████████████████████▋   | 170/185 [18:23<01:31,  6.10s/it][INFO|trainer.py:3788] 2024-07-11 12:58:10,019 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:58:10,020 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:58:10,020 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.76it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.62it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.28it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.67it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03, 10.34it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 10.11it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.17it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.18it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.43it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.37it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.12it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.05it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.06it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.38it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.51it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.44it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.69it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.21it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.58it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.64it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.07it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.52it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.14it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.85it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.86it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.287381887435913, 'eval_runtime': 6.207, 'eval_samples_per_second': 16.111, 'eval_steps_per_second': 8.055, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 92%|█████████████████████████████████████▋   | 170/185 [18:29<01:31,  6.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.84it/s]\u001b[A\n",
      "{'loss': 1.1729, 'grad_norm': 0.8122776746749878, 'learning_rate': 7.192044826145771e-07, 'epoch': 4.67, 'num_input_tokens_seen': 1744008}\n",
      "{'loss': 1.1858, 'grad_norm': 0.9919771552085876, 'learning_rate': 1.8012557287367392e-07, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 97%|███████████████████████████████████████▉ | 180/185 [19:28<00:29,  5.91s/it][INFO|trainer.py:3788] 2024-07-11 12:59:14,701 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 12:59:14,701 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 12:59:14,702 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.14it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.36it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.01it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.98it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.15it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.39it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.75it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.29it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  8.54it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.91it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.00it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.64it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.36it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.37it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.63it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.55it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.11it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.56it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  6.72it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.56it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.86it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.94it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  6.59it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.19it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2874531745910645, 'eval_runtime': 6.466, 'eval_samples_per_second': 15.465, 'eval_steps_per_second': 7.733, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 97%|███████████████████████████████████████▉ | 180/185 [19:34<00:29,  5.91s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.67it/s]\u001b[A\n",
      "{'loss': 1.1941, 'grad_norm': 0.82035231590271, 'learning_rate': 0.0, 'epoch': 4.93, 'num_input_tokens_seen': 1840776}\n",
      "100%|█████████████████████████████████████████| 185/185 [20:01<00:00,  5.67s/it][INFO|trainer.py:3478] 2024-07-11 12:59:47,641 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/checkpoint-185\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f222c1bb7c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: dfd05588-7db8-453c-8d44-1b0b9f0e1010)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 12:59:58,662 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/checkpoint-185/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 12:59:58,676 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/checkpoint-185/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 13:00:01,578 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1215.0457, 'train_samples_per_second': 3.704, 'train_steps_per_second': 0.152, 'train_loss': 1.3083686699738373, 'epoch': 4.93, 'num_input_tokens_seen': 1840776}\n",
      "100%|█████████████████████████████████████████| 185/185 [20:15<00:00,  6.57s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 13:00:01,595 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f222c0ca110>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 5d37f6ce-2f8f-408e-8f29-3af621cbe3c8)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 13:00:12,512 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 13:00:12,526 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     4.9333\n",
      "  num_input_tokens_seen    =    1840776\n",
      "  total_flos               = 13573309GF\n",
      "  train_loss               =     1.3084\n",
      "  train_runtime            = 0:20:15.04\n",
      "  train_samples_per_second =      3.704\n",
      "  train_steps_per_second   =      0.152\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 13:00:13,738 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:00:13,738 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:00:13,738 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.63it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     4.9333\n",
      "  eval_loss               =     1.2876\n",
      "  eval_runtime            = 0:00:04.82\n",
      "  eval_samples_per_second =     20.741\n",
      "  eval_steps_per_second   =      10.37\n",
      "  num_input_tokens_seen   =    1840776\n",
      "[INFO|modelcard.py:449] 2024-07-11 13:00:18,612 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163af63b-92f7-4cdc-bbfb-a10985cf7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 13:01:29,646] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 13:01:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:01:50,854 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 13:01:51,171 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 13:01:51 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 13:01:51 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "07/11/2024 13:01:56 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 13:01:58,321 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 13:01:58,331 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 13:01:58 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 13:01:58,876 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 13:01:59,906 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:01:59,910 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 13:02:20,122 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 13:02:20,122 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 13:02:20,134 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:02:20,134 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 13:02:20 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 13:02:31 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 13:02:31 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5\n",
      "07/11/2024 13:02:31 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "[INFO|trainer.py:3788] 2024-07-11 13:02:31,779 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:02:31,780 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:02:31,780 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 13:02:40,402 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:59<00:00,  1.71s/it]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.820 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [02:00<00:00,  1.21s/it]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =    36.1359\n",
      "  predict_rouge-1            =    42.0937\n",
      "  predict_rouge-2            =     22.228\n",
      "  predict_rouge-l            =    37.4151\n",
      "  predict_runtime            = 0:02:09.84\n",
      "  predict_samples_per_second =       1.54\n",
      "  predict_steps_per_second   =       0.77\n",
      "07/11/2024 13:04:41 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-35_epoch=5/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-35_epoch=5 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-26_epoch=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "batch size = 3, max_example = 500, lr=1e-4, epoch=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e72b73d-6bb7-4671-8ebc-5efb909cfafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 13:05:50,601] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 13:06:08 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,732 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,733 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,733 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,733 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,733 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:06:08,733 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 13:06:09,045 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 13:06:09 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 13:06:09 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/11/2024 13:06:14 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 13:06:15,981 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 13:06:15,990 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 13:06:16,262 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 13:06:17,381 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:06:17,386 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 13:06:42,038 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 13:06:42,039 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 13:06:42,046 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:06:42,046 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 13:06:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/11/2024 13:06:42 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 13:06:42 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/11/2024 13:06:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/11/2024 13:06:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,gate_proj,k_proj,v_proj,up_proj,down_proj,o_proj\n",
      "07/11/2024 13:06:44 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "[INFO|trainer.py:642] 2024-07-11 13:06:44,215 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-11 13:06:45,152 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-11 13:06:45,152 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-11 13:06:45,152 >>   Num Epochs = 7\n",
      "[INFO|trainer.py:2131] 2024-07-11 13:06:45,152 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-11 13:06:45,152 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-11 13:06:45,152 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-11 13:06:45,152 >>   Total optimization steps = 259\n",
      "[INFO|trainer.py:2137] 2024-07-11 13:06:45,156 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.765, 'grad_norm': 0.8090507388114929, 'learning_rate': 9.990807214841287e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.5937, 'grad_norm': 0.579946756362915, 'learning_rate': 9.963262662284736e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  4%|█▌                                        | 10/259 [01:07<22:44,  5.48s/it][INFO|trainer.py:3788] 2024-07-11 13:07:52,455 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:07:52,455 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:07:52,455 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.26it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.74it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.49it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.60it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.70it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.02it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.63it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.48it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.92it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.80it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02,  9.69it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02,  9.57it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.48it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  8.78it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.09it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.73it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  8.37it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.46it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.30it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  8.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.66it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.09it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.29it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.13it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.69it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.17it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.23it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.68it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.360557198524475, 'eval_runtime': 6.3009, 'eval_samples_per_second': 15.871, 'eval_steps_per_second': 7.935, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  4%|█▌                                        | 10/259 [01:13<22:44,  5.48s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.27it/s]\u001b[A\n",
      "{'loss': 1.4646, 'grad_norm': 0.6794503927230835, 'learning_rate': 9.917467626791925e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.4544, 'grad_norm': 0.5791390538215637, 'learning_rate': 9.853590501931904e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      "  8%|███▏                                      | 20/259 [02:10<22:52,  5.74s/it][INFO|trainer.py:3788] 2024-07-11 13:08:55,277 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:08:55,277 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:08:55,277 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.38it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.61it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.70it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.95it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.11it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.79it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.58it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:05,  6.82it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:05,  6.88it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.23it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.66it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.52it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.07it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.05it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.36it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.27it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:01,  8.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  9.40it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.22it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.51it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.15it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.55it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  5.93it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2840346097946167, 'eval_runtime': 6.6204, 'eval_samples_per_second': 15.105, 'eval_steps_per_second': 7.552, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      "  8%|███▏                                      | 20/259 [02:16<22:52,  5.74s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  5.49it/s]\u001b[A\n",
      "{'loss': 1.4426, 'grad_norm': 0.45313209295272827, 'learning_rate': 9.771866171178831e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.2184, 'grad_norm': 0.5153014063835144, 'learning_rate': 9.672595144218646e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 12%|████▊                                     | 30/259 [03:12<21:03,  5.52s/it][INFO|trainer.py:3788] 2024-07-11 13:09:57,915 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:09:57,915 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:09:57,915 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.77it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.85it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.97it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.81it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.33it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.83it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.88it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  8.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.83it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.03it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:02<00:03,  8.40it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.70it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.40it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.56it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.22it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.65it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.80it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.45it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.97it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.34it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.42it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.63it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:05<00:00,  7.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.15it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.40it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.99it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.274725317955017, 'eval_runtime': 6.3168, 'eval_samples_per_second': 15.831, 'eval_steps_per_second': 7.915, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 12%|████▊                                     | 30/259 [03:19<21:03,  5.52s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.92it/s]\u001b[A\n",
      "{'loss': 1.5141, 'grad_norm': 0.5589990019798279, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3747, 'grad_norm': 0.5170177817344666, 'learning_rate': 9.422936304177439e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 15%|██████▍                                   | 40/259 [04:21<22:32,  6.18s/it][INFO|trainer.py:3788] 2024-07-11 13:11:06,931 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:11:06,932 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:11:06,932 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.59it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.44it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.13it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.80it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.96it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.00it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.36it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.53it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.68it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.10it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.96it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.05it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.67it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.63it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.27it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.51it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.93it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.60it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.02it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.66it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.87it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.12it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.36it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.52it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.37it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.05it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.13it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:01,  5.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.90it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:06<00:00,  6.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.271120548248291, 'eval_runtime': 6.5425, 'eval_samples_per_second': 15.285, 'eval_steps_per_second': 7.642, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 15%|██████▍                                   | 40/259 [04:28<22:32,  6.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.49it/s]\u001b[A\n",
      "{'loss': 1.435, 'grad_norm': 0.574812650680542, 'learning_rate': 9.273466515128209e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.3927, 'grad_norm': 0.511595606803894, 'learning_rate': 9.108282702256365e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 19%|████████                                  | 50/259 [05:29<21:13,  6.10s/it][INFO|trainer.py:3788] 2024-07-11 13:12:14,272 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:12:14,272 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:12:14,272 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 14.38it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.23it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.46it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.64it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:04,  8.70it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.74it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:05,  6.65it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  7.08it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:05,  7.04it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.06it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:04,  7.60it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.99it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.12it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.38it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.34it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.03it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.41it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.47it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:02,  8.47it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.52it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:04<00:01,  8.04it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.49it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.15it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.53it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.90it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.96it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.52it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:05<00:00,  7.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  5.70it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2672924995422363, 'eval_runtime': 6.5379, 'eval_samples_per_second': 15.295, 'eval_steps_per_second': 7.648, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 19%|████████                                  | 50/259 [05:35<21:13,  6.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.22it/s]\u001b[A\n",
      "{'loss': 1.3302, 'grad_norm': 0.5738421678543091, 'learning_rate': 8.927992265283282e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.3097, 'grad_norm': 0.5481173396110535, 'learning_rate': 8.733258152710262e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 23%|█████████▋                                | 60/259 [06:32<19:13,  5.80s/it][INFO|trainer.py:3788] 2024-07-11 13:13:18,157 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:13:18,158 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:13:18,158 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.65it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.76it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.10it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04, 10.00it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.72it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.50it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.97it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.19it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.89it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.95it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.73it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.70it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.22it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.33it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.51it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.88it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.72it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.26it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.65it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.18it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.51it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.87it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.266448974609375, 'eval_runtime': 6.3207, 'eval_samples_per_second': 15.821, 'eval_steps_per_second': 7.911, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 23%|█████████▋                                | 60/259 [06:39<19:13,  5.80s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.81it/s]\u001b[A\n",
      "{'loss': 1.2997, 'grad_norm': 0.6053950190544128, 'learning_rate': 8.524796424081292e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.2379, 'grad_norm': 0.5289751887321472, 'learning_rate': 8.303373616950408e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 27%|███████████▎                              | 70/259 [07:33<17:37,  5.59s/it][INFO|trainer.py:3788] 2024-07-11 13:14:18,938 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:14:18,938 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:14:18,938 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.47it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.97it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.72it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.01it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.94it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.63it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.49it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.98it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.08it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.52it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.72it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.37it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.94it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.25it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.76it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.66it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.61it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.93it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.54it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.28it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.39it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.58it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.06it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.46it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.10it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.97it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2691900730133057, 'eval_runtime': 6.4138, 'eval_samples_per_second': 15.591, 'eval_steps_per_second': 7.796, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 27%|███████████▎                              | 70/259 [07:40<17:37,  5.59s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.86it/s]\u001b[A\n",
      "{'loss': 1.4829, 'grad_norm': 0.7341960668563843, 'learning_rate': 8.069803928235689e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.3941, 'grad_norm': 0.6028954982757568, 'learning_rate': 7.824946220324312e-05, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 31%|████████████▉                             | 80/259 [08:40<17:28,  5.86s/it][INFO|trainer.py:3788] 2024-07-11 13:15:26,101 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:15:26,102 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:15:26,102 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.49it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.22it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.04it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.88it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.04it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.80it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.20it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.80it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  6.37it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  6.16it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:05,  4.98it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:05,  4.68it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:05,  4.73it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  6.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:04,  5.65it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:04,  5.46it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.22it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  7.17it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  6.64it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:01,  7.27it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  7.68it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  7.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:05<00:01,  7.10it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  6.78it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:00,  7.22it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:06<00:00,  6.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:06<00:00,  7.57it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  6.69it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2737878561019897, 'eval_runtime': 7.2808, 'eval_samples_per_second': 13.735, 'eval_steps_per_second': 6.867, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 31%|████████████▉                             | 80/259 [08:48<17:28,  5.86s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:07<00:00,  6.79it/s]\u001b[A\n",
      "{'loss': 1.1157, 'grad_norm': 0.5896998047828674, 'learning_rate': 7.56970086293759e-05, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.3395, 'grad_norm': 0.7532356381416321, 'learning_rate': 7.305006422368811e-05, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 35%|██████████████▌                           | 90/259 [09:44<15:51,  5.63s/it][INFO|trainer.py:3788] 2024-07-11 13:16:29,947 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:16:29,948 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:16:29,948 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.08it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.62it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.03it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.21it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  8.88it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:04,  8.99it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.23it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.52it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.66it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.28it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.59it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.39it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:02<00:03,  7.83it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  7.45it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  6.34it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:04,  6.47it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.74it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:03,  7.96it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.88it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.88it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:01,  8.85it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.97it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.42it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.69it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.85it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.19it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.54it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.93it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2763586044311523, 'eval_runtime': 6.4755, 'eval_samples_per_second': 15.443, 'eval_steps_per_second': 7.721, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 35%|██████████████▌                           | 90/259 [09:51<15:51,  5.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.38it/s]\u001b[A\n",
      "{'loss': 1.2342, 'grad_norm': 0.6973042488098145, 'learning_rate': 7.031836210267915e-05, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.3609, 'grad_norm': 0.7689399719238281, 'learning_rate': 6.751194704663543e-05, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 39%|███████████████▊                         | 100/259 [10:51<17:25,  6.57s/it][INFO|trainer.py:3788] 2024-07-11 13:17:36,703 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:17:36,703 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:17:36,703 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.51it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.20it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.57it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.32it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.76it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.39it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.80it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.36it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.41it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.03it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.12it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.62it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.46it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.09it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.78it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.51it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.61it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.38it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.98it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.34it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.29it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.98it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.44it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.05it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.72it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.74it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2796459197998047, 'eval_runtime': 6.2238, 'eval_samples_per_second': 16.067, 'eval_steps_per_second': 8.034, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 39%|███████████████▊                         | 100/259 [10:57<17:25,  6.57s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.80it/s]\u001b[A\n",
      "{'loss': 1.2505, 'grad_norm': 0.9351783394813538, 'learning_rate': 6.464113856382752e-05, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.19, 'grad_norm': 0.9892970323562622, 'learning_rate': 6.171649294450113e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 42%|█████████████████▍                       | 110/259 [11:54<14:50,  5.97s/it][INFO|trainer.py:3788] 2024-07-11 13:18:39,602 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:18:39,603 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:18:39,603 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.26it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.49it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:04,  9.14it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.36it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:04,  9.33it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:06,  6.99it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.09it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.97it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.42it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.29it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.08it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  9.13it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02,  9.98it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.22it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02,  9.95it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.44it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:03<00:01, 10.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:01, 10.73it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 10.68it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.91it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 10.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01, 10.10it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00,  9.95it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  8.51it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  8.81it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.62it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.80it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2859545946121216, 'eval_runtime': 5.659, 'eval_samples_per_second': 17.671, 'eval_steps_per_second': 8.836, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 42%|█████████████████▍                       | 110/259 [12:00<14:50,  5.97s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.19it/s]\u001b[A\n",
      "{'loss': 1.2346, 'grad_norm': 0.9282048940658569, 'learning_rate': 5.874876444419377e-05, 'epoch': 3.07, 'num_input_tokens_seen': 1144800}\n",
      "{'loss': 1.1029, 'grad_norm': 0.9042193293571472, 'learning_rate': 5.574886573911056e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 46%|██████████████████▉                      | 120/259 [12:58<12:51,  5.55s/it][INFO|trainer.py:3788] 2024-07-11 13:19:43,207 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:19:43,207 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:19:43,207 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.24it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.27it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.46it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  8.29it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.61it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.48it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  7.96it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.84it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.98it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.23it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.60it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  8.00it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.41it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.15it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.29it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.84it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.44it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.04it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.59it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.86it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:01,  8.52it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.58it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.91it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  7.88it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  7.07it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.68it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.20it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2929917573928833, 'eval_runtime': 6.5273, 'eval_samples_per_second': 15.32, 'eval_steps_per_second': 7.66, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 46%|██████████████████▉                      | 120/259 [13:04<12:51,  5.55s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.07it/s]\u001b[A\n",
      "{'loss': 1.2574, 'grad_norm': 0.8961623907089233, 'learning_rate': 5.272782779896898e-05, 'epoch': 3.33, 'num_input_tokens_seen': 1240056}\n",
      "{'loss': 1.0897, 'grad_norm': 0.8513991832733154, 'learning_rate': 4.969675932486503e-05, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 50%|████████████████████▌                    | 130/259 [14:02<12:26,  5.79s/it][INFO|trainer.py:3788] 2024-07-11 13:20:47,544 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:20:47,544 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:20:47,544 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.84it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.15it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.78it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.77it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.16it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.31it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.68it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.94it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.72it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.13it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.09it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  6.92it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.23it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.97it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.30it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.83it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.72it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.79it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  7.87it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.43it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.93it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:05<00:01,  7.49it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.34it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.71it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.79it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.01it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.90it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.33it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3065799474716187, 'eval_runtime': 6.466, 'eval_samples_per_second': 15.466, 'eval_steps_per_second': 7.733, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 50%|████████████████████▌                    | 130/259 [14:08<12:26,  5.79s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.86it/s]\u001b[A\n",
      "{'loss': 1.2781, 'grad_norm': 1.1670589447021484, 'learning_rate': 4.666680590131225e-05, 'epoch': 3.6, 'num_input_tokens_seen': 1343448}\n",
      "{'loss': 1.0271, 'grad_norm': 1.0086426734924316, 'learning_rate': 4.364910901265606e-05, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 54%|██████████████████████▏                  | 140/259 [15:06<11:18,  5.70s/it][INFO|trainer.py:3788] 2024-07-11 13:21:51,913 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:21:51,913 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:21:51,913 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.50it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.20it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.32it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.92it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.16it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.12it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.65it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.95it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.43it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.44it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  7.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.21it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.10it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.23it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.60it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.03it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.35it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.68it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.77it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.17it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.53it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.99it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.34it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.84it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.37it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.55it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.89it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.79it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3203943967819214, 'eval_runtime': 6.5622, 'eval_samples_per_second': 15.239, 'eval_steps_per_second': 7.619, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 54%|██████████████████████▏                  | 140/259 [15:13<11:18,  5.70s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.71it/s]\u001b[A\n",
      "{'loss': 1.1665, 'grad_norm': 1.2809512615203857, 'learning_rate': 4.0654765074565124e-05, 'epoch': 3.87, 'num_input_tokens_seen': 1441848}\n",
      "{'loss': 1.301, 'grad_norm': 1.2866379022598267, 'learning_rate': 3.769478463124507e-05, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 58%|███████████████████████▋                 | 150/259 [16:14<11:37,  6.40s/it][INFO|trainer.py:3788] 2024-07-11 13:22:59,749 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:22:59,749 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:22:59,749 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.04it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.25it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.31it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.30it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.38it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.54it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.19it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.71it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.81it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.33it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.64it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.71it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.45it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.02it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.82it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.98it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.02it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.42it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.56it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.67it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.67it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.52it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.44it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.89it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3249564170837402, 'eval_runtime': 6.2727, 'eval_samples_per_second': 15.942, 'eval_steps_per_second': 7.971, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 58%|███████████████████████▋                 | 150/259 [16:20<11:37,  6.40s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.56it/s]\u001b[A\n",
      "{'loss': 0.9667, 'grad_norm': 1.0898643732070923, 'learning_rate': 3.4780051868411675e-05, 'epoch': 4.13, 'num_input_tokens_seen': 1543032}\n",
      "{'loss': 1.094, 'grad_norm': 1.2372268438339233, 'learning_rate': 3.1921284590898456e-05, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 62%|█████████████████████████▎               | 160/259 [17:19<10:43,  6.50s/it][INFO|trainer.py:3788] 2024-07-11 13:24:04,843 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:24:04,844 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:24:04,844 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.41it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.84it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.55it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.36it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.71it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.56it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.18it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  8.89it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.11it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.94it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.88it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.73it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.25it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.08it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.85it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.95it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.80it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.99it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.28it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.89it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.95it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.31it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.48it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.53it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.84it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.33it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.50it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3385075330734253, 'eval_runtime': 6.2311, 'eval_samples_per_second': 16.049, 'eval_steps_per_second': 8.024, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 62%|█████████████████████████▎               | 160/259 [17:25<10:43,  6.50s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.61it/s]\u001b[A\n",
      "{'loss': 1.0847, 'grad_norm': 1.3618159294128418, 'learning_rate': 2.912899481206582e-05, 'epoch': 4.4, 'num_input_tokens_seen': 1644360}\n",
      "{'loss': 0.9873, 'grad_norm': 1.3438111543655396, 'learning_rate': 2.6413450099928783e-05, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 66%|██████████████████████████▉              | 170/259 [18:24<08:57,  6.04s/it][INFO|trainer.py:3788] 2024-07-11 13:25:09,783 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:25:09,783 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:25:09,783 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.27it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.29it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.86it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.49it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.76it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.96it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.35it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.46it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.48it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.71it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.61it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.22it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  6.86it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.53it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.06it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  7.21it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.55it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.83it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.16it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.28it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.50it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.08it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.76it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.88it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.368817925453186, 'eval_runtime': 6.2867, 'eval_samples_per_second': 15.907, 'eval_steps_per_second': 7.953, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 66%|██████████████████████████▉              | 170/259 [18:30<08:57,  6.04s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.96it/s]\u001b[A\n",
      "{'loss': 1.0311, 'grad_norm': 1.3717215061187744, 'learning_rate': 2.3784635822138424e-05, 'epoch': 4.67, 'num_input_tokens_seen': 1744008}\n",
      "{'loss': 1.0398, 'grad_norm': 1.6174300909042358, 'learning_rate': 2.1252218428645846e-05, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 69%|████████████████████████████▍            | 180/259 [19:29<07:47,  5.92s/it][INFO|trainer.py:3788] 2024-07-11 13:26:14,734 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:26:14,734 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:26:14,734 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.86it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.77it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.98it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.81it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.42it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.86it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.06it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.45it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.63it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.20it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.67it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.69it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.05it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.66it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.87it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.17it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.15it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.71it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.85it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.28it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.51it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.26it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.99it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.58it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.70it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  6.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3790349960327148, 'eval_runtime': 6.1457, 'eval_samples_per_second': 16.271, 'eval_steps_per_second': 8.136, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 69%|████████████████████████████▍            | 180/259 [19:35<07:47,  5.92s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.86it/s]\u001b[A\n",
      "{'loss': 1.0546, 'grad_norm': 1.346174716949463, 'learning_rate': 1.8825509907063327e-05, 'epoch': 4.93, 'num_input_tokens_seen': 1840776}\n",
      "{'loss': 1.1382, 'grad_norm': 1.5471510887145996, 'learning_rate': 1.6513433541423528e-05, 'epoch': 5.07, 'num_input_tokens_seen': 1892112}\n",
      " 73%|██████████████████████████████           | 190/259 [20:32<06:44,  5.86s/it][INFO|trainer.py:3788] 2024-07-11 13:27:17,736 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:27:17,736 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:27:17,736 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.64it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.47it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.87it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.86it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.35it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  7.63it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:05,  7.32it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.20it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:05,  6.86it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.64it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  6.99it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  7.71it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  6.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:04,  6.73it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.83it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.73it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.03it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.51it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.48it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:02,  8.27it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.33it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.25it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.56it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  7.18it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.31it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:06<00:00,  6.91it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3762786388397217, 'eval_runtime': 6.676, 'eval_samples_per_second': 14.979, 'eval_steps_per_second': 7.489, 'epoch': 5.07, 'num_input_tokens_seen': 1892112}\n",
      " 73%|██████████████████████████████           | 190/259 [20:39<06:44,  5.86s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.90it/s]\u001b[A\n",
      "{'loss': 0.9862, 'grad_norm': 1.4600437879562378, 'learning_rate': 1.4324491100246385e-05, 'epoch': 5.2, 'num_input_tokens_seen': 1940640}\n",
      "{'loss': 1.0018, 'grad_norm': 1.7413994073867798, 'learning_rate': 1.2266731574566536e-05, 'epoch': 5.33, 'num_input_tokens_seen': 1990440}\n",
      " 77%|███████████████████████████████▋         | 200/259 [21:36<05:40,  5.77s/it][INFO|trainer.py:3788] 2024-07-11 13:28:21,532 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:28:21,532 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:28:21,532 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.42it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.55it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.27it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.41it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  9.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.72it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.03it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.82it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.64it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.92it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.46it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.49it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.15it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.61it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.43it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:03<00:02,  7.59it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.32it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:02,  8.66it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.10it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.47it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.64it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.60it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.95it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  7.43it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  6.68it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:01,  6.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.54it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.38it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.46it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:06<00:00,  6.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.385881781578064, 'eval_runtime': 6.4834, 'eval_samples_per_second': 15.424, 'eval_steps_per_second': 7.712, 'epoch': 5.33, 'num_input_tokens_seen': 1990440}\n",
      " 77%|███████████████████████████████▋         | 200/259 [21:42<05:40,  5.77s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.39it/s]\u001b[A\n",
      "{'loss': 0.9341, 'grad_norm': 1.5426005125045776, 'learning_rate': 1.0347721580875126e-05, 'epoch': 5.47, 'num_input_tokens_seen': 2042376}\n",
      "{'loss': 0.9702, 'grad_norm': 1.371677279472351, 'learning_rate': 8.574517537807897e-06, 'epoch': 5.6, 'num_input_tokens_seen': 2093040}\n",
      " 81%|█████████████████████████████████▏       | 210/259 [22:41<04:45,  5.83s/it][INFO|trainer.py:3788] 2024-07-11 13:29:26,931 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:29:26,931 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:29:26,931 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.59it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.93it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.23it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.07it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.90it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.62it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:04,  7.90it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:05,  6.08it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:04,  7.18it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:04,  6.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:04,  6.63it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:03,  6.80it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:03<00:03,  7.34it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.66it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.44it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.33it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:04<00:01,  9.11it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.63it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.18it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  8.04it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:05<00:01,  7.44it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:00,  8.03it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  8.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.88it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.95it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  7.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.396744728088379, 'eval_runtime': 6.35, 'eval_samples_per_second': 15.748, 'eval_steps_per_second': 7.874, 'epoch': 5.6, 'num_input_tokens_seen': 2093040}\n",
      " 81%|█████████████████████████████████▏       | 210/259 [22:48<04:45,  5.83s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  7.92it/s]\u001b[A\n",
      "{'loss': 1.0216, 'grad_norm': 1.7215954065322876, 'learning_rate': 6.953639718889076e-06, 'epoch': 5.73, 'num_input_tokens_seen': 2138688}\n",
      "{'loss': 1.0051, 'grad_norm': 1.5808241367340088, 'learning_rate': 5.491048276741784e-06, 'epoch': 5.87, 'num_input_tokens_seen': 2190936}\n",
      " 85%|██████████████████████████████████▊      | 220/259 [23:45<03:54,  6.01s/it][INFO|trainer.py:3788] 2024-07-11 13:30:30,583 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:30:30,583 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:30:30,583 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.22it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.12it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.67it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.81it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.93it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.56it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.55it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.30it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.84it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  7.85it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.55it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.65it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.28it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.31it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.34it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:03,  6.52it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  7.99it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.36it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.25it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.66it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.31it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.56it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.83it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.29it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  7.87it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.58it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.87it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.60it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.68it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  5.92it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4015213251113892, 'eval_runtime': 6.3947, 'eval_samples_per_second': 15.638, 'eval_steps_per_second': 7.819, 'epoch': 5.87, 'num_input_tokens_seen': 2190936}\n",
      " 85%|██████████████████████████████████▊      | 220/259 [23:51<03:54,  6.01s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.67it/s]\u001b[A\n",
      "{'loss': 0.9495, 'grad_norm': 1.5596814155578613, 'learning_rate': 4.192121326927073e-06, 'epoch': 6.0, 'num_input_tokens_seen': 2237952}\n",
      "{'loss': 0.9664, 'grad_norm': 1.4354134798049927, 'learning_rate': 3.061635171999566e-06, 'epoch': 6.13, 'num_input_tokens_seen': 2288592}\n",
      " 89%|████████████████████████████████████▍    | 230/259 [24:48<02:50,  5.87s/it][INFO|trainer.py:3788] 2024-07-11 13:31:33,927 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:31:33,928 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:31:33,928 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.45it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.59it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.68it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.75it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:04,  9.77it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.68it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.45it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.45it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.72it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  8.86it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.36it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.60it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:02<00:03,  7.95it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.42it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.07it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.91it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.33it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.54it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.73it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.54it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.07it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.34it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.58it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.04it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.35it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.76it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.80it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.404261827468872, 'eval_runtime': 6.4134, 'eval_samples_per_second': 15.592, 'eval_steps_per_second': 7.796, 'epoch': 6.13, 'num_input_tokens_seen': 2288592}\n",
      " 89%|████████████████████████████████████▍    | 230/259 [24:55<02:50,  5.87s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.76it/s]\u001b[A\n",
      "{'loss': 1.0264, 'grad_norm': 1.4441384077072144, 'learning_rate': 2.1037467384981026e-06, 'epoch': 6.27, 'num_input_tokens_seen': 2341272}\n",
      "{'loss': 0.9416, 'grad_norm': 1.2896746397018433, 'learning_rate': 1.3219782914527634e-06, 'epoch': 6.4, 'num_input_tokens_seen': 2391672}\n",
      " 93%|█████████████████████████████████████▉   | 240/259 [25:55<01:49,  5.78s/it][INFO|trainer.py:3788] 2024-07-11 13:32:40,224 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:32:40,225 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:32:40,225 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.63it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 10.27it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.14it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  8.21it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.46it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.40it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.07it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.62it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  8.03it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.62it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.61it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.31it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.13it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.24it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  8.76it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.60it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.80it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.63it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:04<00:02,  7.80it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  8.25it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01,  8.45it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.74it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.78it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.04it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:05<00:01,  7.39it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.89it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.55it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:06<00:00,  6.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4066672325134277, 'eval_runtime': 6.4877, 'eval_samples_per_second': 15.414, 'eval_steps_per_second': 7.707, 'epoch': 6.4, 'num_input_tokens_seen': 2391672}\n",
      " 93%|█████████████████████████████████████▉   | 240/259 [26:01<01:49,  5.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.58it/s]\u001b[A\n",
      "{'loss': 0.9218, 'grad_norm': 1.3836816549301147, 'learning_rate': 7.192044826145771e-07, 'epoch': 6.53, 'num_input_tokens_seen': 2438184}\n",
      "{'loss': 0.9548, 'grad_norm': 1.4456722736358643, 'learning_rate': 2.976417800331144e-07, 'epoch': 6.67, 'num_input_tokens_seen': 2485344}\n",
      " 97%|███████████████████████████████████████▌ | 250/259 [26:56<00:50,  5.59s/it][INFO|trainer.py:3788] 2024-07-11 13:33:41,748 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:33:41,749 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:33:41,749 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.08it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.05it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.19it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.48it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:03,  9.52it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.36it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.34it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.38it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.86it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.87it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 10.29it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.65it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.25it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.62it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.45it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  7.88it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.29it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  7.52it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  7.95it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.35it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  6.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:05<00:00,  7.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  6.94it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  7.46it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.37it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4075121879577637, 'eval_runtime': 6.271, 'eval_samples_per_second': 15.946, 'eval_steps_per_second': 7.973, 'epoch': 6.67, 'num_input_tokens_seen': 2485344}\n",
      " 97%|███████████████████████████████████████▌ | 250/259 [27:02<00:50,  5.59s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:06<00:00,  6.53it/s]\u001b[A\n",
      "{'loss': 1.01, 'grad_norm': 1.5259889364242554, 'learning_rate': 5.8840317850683555e-08, 'epoch': 6.8, 'num_input_tokens_seen': 2538288}\n",
      "100%|█████████████████████████████████████████| 259/259 [27:56<00:00,  5.83s/it][INFO|trainer.py:3478] 2024-07-11 13:34:41,518 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/checkpoint-259\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f82f575a5c0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f2959631-1b93-4dd7-8709-fba61f445c1e)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 13:34:52,671 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/checkpoint-259/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 13:34:52,685 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/checkpoint-259/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-11 13:34:55,448 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1690.2923, 'train_samples_per_second': 3.727, 'train_steps_per_second': 0.153, 'train_loss': 1.1864960515821301, 'epoch': 6.91, 'num_input_tokens_seen': 2576304}\n",
      "100%|█████████████████████████████████████████| 259/259 [28:10<00:00,  6.53s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-11 13:34:55,462 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f82f565ffd0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f808446a-cc8c-48cb-8d27-ad3f69af44c8)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-11 13:35:06,540 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-11 13:35:06,572 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     6.9067\n",
      "  num_input_tokens_seen    =    2576304\n",
      "  total_flos               = 18996863GF\n",
      "  train_loss               =     1.1865\n",
      "  train_runtime            = 0:28:10.29\n",
      "  train_samples_per_second =      3.727\n",
      "  train_steps_per_second   =      0.153\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-11 13:35:07,827 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:35:07,827 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:35:07,827 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.37it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     6.9067\n",
      "  eval_loss               =     1.4076\n",
      "  eval_runtime            = 0:00:04.93\n",
      "  eval_samples_per_second =     20.271\n",
      "  eval_steps_per_second   =     10.135\n",
      "  num_input_tokens_seen   =    2576304\n",
      "[INFO|modelcard.py:449] 2024-07-11 13:35:12,803 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 7.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8e1ccb-eab2-4618-a501-20068aab09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-11 13:36:17,240] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/11/2024 13:36:37 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,845 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,845 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,845 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,845 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,845 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-11 13:36:37,846 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-11 13:36:38,191 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/11/2024 13:36:38 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/11/2024 13:36:38 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\n",
      "07/11/2024 13:36:44 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "[INFO|configuration_utils.py:731] 2024-07-11 13:36:45,115 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-11 13:36:45,122 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/11/2024 13:36:45 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-11 13:36:45,639 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-11 13:36:46,779 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:36:46,784 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-11 13:37:05,354 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-11 13:37:05,354 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-11 13:37:05,367 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-11 13:37:05,367 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/11/2024 13:37:05 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/11/2024 13:37:13 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
      "07/11/2024 13:37:13 - INFO - llamafactory.model.adapter - Loaded adapter(s): saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7\n",
      "07/11/2024 13:37:13 - INFO - llamafactory.model.loader - all params: 1543714304\n",
      "[INFO|trainer.py:3788] 2024-07-11 13:37:14,463 >> \n",
      "***** Running Prediction *****\n",
      "[INFO|trainer.py:3790] 2024-07-11 13:37:14,463 >>   Num examples = 200\n",
      "[INFO|trainer.py:3793] 2024-07-11 13:37:14,463 >>   Batch size = 2\n",
      "[WARNING|logging.py:328] 2024-07-11 13:37:23,122 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "100%|█████████████████████████████████████████| 100/100 [01:56<00:00,  1.84s/it]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.828 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|█████████████████████████████████████████| 100/100 [01:57<00:00,  1.18s/it]\n",
      "***** predict metrics *****\n",
      "  predict_bleu-4             =     34.338\n",
      "  predict_rouge-1            =    40.7582\n",
      "  predict_rouge-2            =    19.6334\n",
      "  predict_rouge-l            =    35.4526\n",
      "  predict_runtime            = 0:02:07.22\n",
      "  predict_samples_per_second =      1.572\n",
      "  predict_steps_per_second   =      0.786\n",
      "07/11/2024 13:39:21 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-36_epoch=7/generated_predictions.jsonl\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/lora/eval_2024-07-09-00-26-36_epoch=7 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves/Qwen2-1.5B-Instruct/lora/train_2024-07-08-20-41-27_epoch=7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
