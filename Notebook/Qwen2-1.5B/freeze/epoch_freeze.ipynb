{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.41.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/dc/23c26b7b0bce5aaccf2b767db3e9c4f5ae4331bd47688c1f2ef091b23696/transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting datasets>=2.16.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/2d/963b266bb8f88492d5ab4232d74292af8beb5b6fdae97902df9e284d4c32/datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.30.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/74/564f621699b049b0358f7ad83d7437f8219a5d6efb69bbfcca328b60152f/accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting trl>=0.8.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/8c/7d1007557b343d5cf18349802e94d3a14397121e9105b4661f8cd753f9bf/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/54/eb7fcfc0e1ec6a8404cadd11ac957b3ee4fd0774225cafe3ffe6287861cb/pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tyro>=0.5.11 (from trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/ea/c8967a64769ec465a2d49bf81e1e135999741704a36993b6b51465ce8503/tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/71/6ce4136149cb42b98599d49c39b3a39dd6858b5f9307490998c40e26a51e/huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/a3/16e9fe32187e9c8bc7f9b7bcd9728529faa725231a0c96f2f98714ff2fc5/fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=844e4d312eff1a2f8cc262fd3414b371c5b7abd1bce4ddfeb5154220e7b63af3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-zt5w0kyk/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: tqdm, shtab, requests, pyarrow, fsspec, docstring-parser, tiktoken, huggingface-hub, tyro, tokenizers, transformers, datasets, accelerate, trl, peft, llamafactory\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.15\n",
      "    Uninstalling docstring-parser-0.15:\n",
      "      Successfully uninstalled docstring-parser-0.15\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.7\n",
      "    Uninstalling datasets-2.14.7:\n",
      "      Successfully uninstalled datasets-2.14.7\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.24.1\n",
      "    Uninstalling accelerate-0.24.1:\n",
      "      Successfully uninstalled accelerate-0.24.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchx 0.6.0 requires fsspec==2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 docstring-parser-0.16 fsspec-2024.5.0 huggingface-hub-0.23.4 llamafactory-0.8.2.dev0 peft-0.11.1 pyarrow-16.1.0 requests-2.32.3 shtab-1.7.1 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.4 trl-0.9.6 tyro-0.8.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/1b/12521efcbc6058e2673583bb096c2b5046a9df39bd73eca392c1efed24e5/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cuda 23.10.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting optimum\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/e4/f832e42a1eb9d5ac4fa6379295e05aebeae507d171babc1786bfa0210299/optimum-1.21.2-py3-none-any.whl (424 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.21.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found existing installation: apex 0.9.10.dev0\n",
      "Uninstalling apex-0.9.10.dev0:\n",
      "  Successfully uninstalled apex-0.9.10.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd83f-942a-4b63-9e4e-e08578379499",
   "metadata": {},
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-13 11:11:53,280] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/13/2024 11:12:16 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,346 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,347 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,347 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,347 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,347 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:12:16,347 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-13 11:12:16,809 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/13/2024 11:12:16 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/13/2024 11:12:16 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "Generating train split: 10178 examples [00:00, 38576.52 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1108.2\n",
      "07/13/2024 11:12:33 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Generating train split: 900 examples [00:00, 11320.35 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1167.3\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:04<00:00, 228.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-13 11:12:40,463 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-13 11:12:40,473 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-13 11:12:40,986 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-13 11:12:41,685 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:12:41,691 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-13 11:13:37,838 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-13 11:13:37,839 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-13 11:13:37,859 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:13:37,860 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/13/2024 11:13:38 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-13 11:13:38,436 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-13 11:13:39,369 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-13 11:13:39,369 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-13 11:13:39,369 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-13 11:13:39,369 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-13 11:13:39,369 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-13 11:13:39,369 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-13 11:13:39,369 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-13 11:13:39,371 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                   | 0/111 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.8412, 'grad_norm': 3.711768627166748, 'learning_rate': 9.950018542108818e-06, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.764, 'grad_norm': 3.0691888332366943, 'learning_rate': 9.801073426888447e-06, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:30<03:37,  2.15s/it][INFO|trainer.py:3788] 2024-07-13 11:14:09,825 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:14:09,826 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:14:09,826 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:04, 10.42it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  8.54it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.83it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.89it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.05it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:02, 12.45it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.86it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.40it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.27it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 10.34it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.53it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.86it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02,  9.97it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  8.78it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.39it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.61it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.63it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  9.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  8.29it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.28it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  7.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.47it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 10.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5485281944274902, 'eval_runtime': 5.3159, 'eval_samples_per_second': 18.812, 'eval_steps_per_second': 9.406, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:35<03:37,  2.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.04it/s]\u001b[A\n",
      "{'loss': 1.6568, 'grad_norm': 3.186408758163452, 'learning_rate': 9.55614245194068e-06, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.6512, 'grad_norm': 2.692101001739502, 'learning_rate': 9.220122420149753e-06, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [00:55<03:01,  1.99s/it][INFO|trainer.py:3788] 2024-07-13 11:14:35,251 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:14:35,251 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:14:35,251 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:03, 15.00it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 16.38it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 12.65it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 12.59it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.93it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.81it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 11.26it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.11it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.14it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.43it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.44it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02,  9.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.35it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.85it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.20it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 12.12it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 13.02it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 13.49it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 11.19it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 10.73it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  8.60it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  7.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4736430644989014, 'eval_runtime': 5.003, 'eval_samples_per_second': 19.988, 'eval_steps_per_second': 9.994, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [01:00<03:01,  1.99s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  7.83it/s]\u001b[A\n",
      "{'loss': 1.6184, 'grad_norm': 2.3567955493927, 'learning_rate': 8.799731239943488e-06, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.3816, 'grad_norm': 2.9048640727996826, 'learning_rate': 8.303373616950408e-06, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [01:20<02:38,  1.96s/it][INFO|trainer.py:3788] 2024-07-13 11:14:59,630 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:14:59,630 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:14:59,630 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.87it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.65it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.83it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.16it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 11.70it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.48it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.75it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 11.74it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02,  9.83it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.18it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.99it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.39it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.46it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 12.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 13.10it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 13.21it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 13.37it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:00, 14.28it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 14.07it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 14.73it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 14.70it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 14.68it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 13.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4540302753448486, 'eval_runtime': 4.295, 'eval_samples_per_second': 23.283, 'eval_steps_per_second': 11.641, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [01:24<02:38,  1.96s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 14.08it/s]\u001b[A\n",
      "{'loss': 1.6971, 'grad_norm': 2.550114631652832, 'learning_rate': 7.74097302222355e-06, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.5367, 'grad_norm': 2.4421770572662354, 'learning_rate': 7.12377329642024e-06, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [01:43<02:14,  1.90s/it][INFO|trainer.py:3788] 2024-07-13 11:15:22,619 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:15:22,619 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:15:22,619 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.80it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 16.14it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 14.52it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.09it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 12.81it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 11.57it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.45it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.00it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.44it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.08it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  8.97it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.92it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 10.71it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01,  9.92it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 10.46it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.48it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.66it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  8.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00,  9.33it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.48it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 11.34it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  8.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4416298866271973, 'eval_runtime': 4.914, 'eval_samples_per_second': 20.35, 'eval_steps_per_second': 10.175, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [01:48<02:14,  1.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  8.86it/s]\u001b[A\n",
      "{'loss': 1.5846, 'grad_norm': 2.408215045928955, 'learning_rate': 6.464113856382752e-06, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.5224, 'grad_norm': 2.534334659576416, 'learning_rate': 5.77518299832099e-06, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [02:10<02:17,  2.25s/it][INFO|trainer.py:3788] 2024-07-13 11:15:50,129 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:15:50,130 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:15:50,130 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 18.04it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.29it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.78it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.78it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.67it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.64it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.55it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.15it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.24it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.54it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.29it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.82it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.93it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 15.39it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.87it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.88it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.94it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 15.78it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 15.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 14.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 11.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4336962699890137, 'eval_runtime': 3.8083, 'eval_samples_per_second': 26.259, 'eval_steps_per_second': 13.129, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [02:14<02:17,  2.25s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00,  9.99it/s]\u001b[A\n",
      "{'loss': 1.463, 'grad_norm': 2.695378541946411, 'learning_rate': 5.070754229703811e-06, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.4232, 'grad_norm': 2.322571277618408, 'learning_rate': 4.364910901265607e-06, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [02:35<01:50,  2.17s/it][INFO|trainer.py:3788] 2024-07-13 11:16:14,884 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:16:14,885 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:16:14,885 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.80it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.48it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.19it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.16it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.73it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.19it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.38it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 12.19it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 12.84it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:03,  8.43it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02,  9.69it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.34it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 10.70it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 10.71it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 11.62it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  8.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00,  9.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 10.88it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 12.12it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 12.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4274619817733765, 'eval_runtime': 4.411, 'eval_samples_per_second': 22.671, 'eval_steps_per_second': 11.335, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [02:39<01:50,  2.17s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 12.91it/s]\u001b[A\n",
      "{'loss': 1.4255, 'grad_norm': 2.647733211517334, 'learning_rate': 3.6717646444456196e-06, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.3666, 'grad_norm': 2.3642971515655518, 'learning_rate': 3.0051732434229185e-06, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [02:59<01:18,  1.91s/it][INFO|trainer.py:3788] 2024-07-13 11:16:38,953 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:16:38,953 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:16:38,953 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 16.07it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.95it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.46it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:03, 11.77it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 11.20it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.48it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.80it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.70it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.23it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.45it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.37it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02,  9.38it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  8.33it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.76it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.74it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 11.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 12.80it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 13.53it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 13.88it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 12.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4258068799972534, 'eval_runtime': 4.6043, 'eval_samples_per_second': 21.719, 'eval_steps_per_second': 10.859, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [03:04<01:18,  1.91s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.78it/s]\u001b[A\n",
      "{'loss': 1.6287, 'grad_norm': 2.654996871948242, 'learning_rate': 2.3784635822138424e-06, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.5663, 'grad_norm': 2.5377731323242188, 'learning_rate': 1.8041652058350768e-06, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [03:26<01:06,  2.14s/it][INFO|trainer.py:3788] 2024-07-13 11:17:05,815 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:17:05,815 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:17:05,815 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.93it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.84it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.53it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.05it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 12.61it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.02it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 10.84it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:02, 11.43it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 11.20it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.34it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 10.55it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 10.12it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01,  9.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.66it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  8.70it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.92it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 11.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 12.58it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00, 13.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00, 12.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4235156774520874, 'eval_runtime': 4.2612, 'eval_samples_per_second': 23.467, 'eval_steps_per_second': 11.734, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [03:30<01:06,  2.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 12.54it/s]\u001b[A\n",
      "{'loss': 1.2632, 'grad_norm': 2.441002607345581, 'learning_rate': 1.2937598223330006e-06, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.5083, 'grad_norm': 2.768449068069458, 'learning_rate': 8.574517537807897e-07, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [03:51<00:44,  2.10s/it][INFO|trainer.py:3788] 2024-07-13 11:17:31,015 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:17:31,015 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:17:31,015 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.29it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04,  9.45it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.10it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  8.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:05,  7.26it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.46it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.81it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03, 10.48it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 11.73it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 12.31it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 12.50it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 11.94it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.93it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 14.61it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.91it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.26it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:00, 15.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 15.52it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 15.41it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 15.03it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 15.75it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 11.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4223991632461548, 'eval_runtime': 4.4249, 'eval_samples_per_second': 22.599, 'eval_steps_per_second': 11.3, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [03:56<00:44,  2.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.86it/s]\u001b[A\n",
      "{'loss': 1.3987, 'grad_norm': 2.5468733310699463, 'learning_rate': 5.039639255208156e-07, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.5395, 'grad_norm': 2.4756548404693604, 'learning_rate': 2.403634723543674e-07, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [04:17<00:24,  2.22s/it][INFO|trainer.py:3788] 2024-07-13 11:17:56,804 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:17:56,804 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:17:56,804 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.88it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:05,  8.65it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:03, 11.93it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:04, 10.73it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 11.85it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.56it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.92it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.73it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.35it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 11.94it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.59it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 11.17it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.80it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  9.43it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.34it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01,  9.35it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 11.84it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 13.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 13.14it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 12.55it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  9.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4219344854354858, 'eval_runtime': 4.7485, 'eval_samples_per_second': 21.059, 'eval_steps_per_second': 10.53, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [04:22<00:24,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.61it/s]\u001b[A\n",
      "{'loss': 1.4271, 'grad_norm': 2.711745500564575, 'learning_rate': 7.192044826145772e-08, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.3662, 'grad_norm': 2.971336841583252, 'learning_rate': 2.002464408392135e-09, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [04:41<00:02,  2.08s/it][INFO|trainer.py:3788] 2024-07-13 11:18:21,392 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:18:21,393 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:18:21,393 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 15.80it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.51it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 13.16it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 12.90it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.94it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 11.12it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.79it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.19it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.65it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 11.29it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.22it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.46it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.41it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02,  9.50it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  8.33it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.39it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.18it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.28it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:01,  7.53it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  6.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:01,  6.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  6.27it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  6.94it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  6.27it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  5.93it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4217420816421509, 'eval_runtime': 5.6557, 'eval_samples_per_second': 17.681, 'eval_steps_per_second': 8.841, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [04:47<00:02,  2.08s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  6.75it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [04:49<00:00,  3.84s/it][INFO|trainer.py:3478] 2024-07-13 11:18:29,330 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:18:29,359 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:18:29,377 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:18:52,096 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:18:52,121 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:18:52,135 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-13 11:18:57,544 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 318.1741, 'train_samples_per_second': 8.486, 'train_steps_per_second': 0.349, 'train_loss': 1.5303315020896293, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [05:18<00:00,  2.87s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-13 11:18:57,558 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:18:57,588 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:18:57,603 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:19:16,363 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:19:16,386 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:19:16,402 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8085178GF\n",
      "  train_loss               =     1.5303\n",
      "  train_runtime            = 0:05:18.17\n",
      "  train_samples_per_second =      8.486\n",
      "  train_steps_per_second   =      0.349\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-13 11:19:18,004 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:19:18,004 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:19:18,004 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 13.25it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.4217\n",
      "  eval_runtime            = 0:00:03.96\n",
      "  eval_samples_per_second =     25.249\n",
      "  eval_steps_per_second   =     12.625\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-13 11:19:22,002 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-13 11:20:33,732] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/13/2024 11:20:54 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:20:54,201 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-13 11:20:54,762 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/13/2024 11:20:54 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/13/2024 11:20:54 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/13/2024 11:21:00 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-13 11:21:01,785 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-13 11:21:01,795 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-13 11:21:02,352 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-13 11:21:03,356 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:21:03,361 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-13 11:21:51,935 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-13 11:21:51,935 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-13 11:21:51,946 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:21:51,947 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/13/2024 11:21:52 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-13 11:21:52,332 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-13 11:21:53,266 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-13 11:21:53,267 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-13 11:21:53,267 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-13 11:21:53,267 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-13 11:21:53,267 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-13 11:21:53,267 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-13 11:21:53,267 >>   Total optimization steps = 111\n",
      "[INFO|trainer.py:2137] 2024-07-13 11:21:53,268 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                   | 0/111 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.6806, 'grad_norm': 3.874378204345703, 'learning_rate': 9.950018542108818e-05, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.6542, 'grad_norm': 3.644102096557617, 'learning_rate': 9.801073426888447e-05, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:33<03:43,  2.21s/it][INFO|trainer.py:3788] 2024-07-13 11:22:26,299 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:22:26,299 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:22:26,299 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.50it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.06it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.18it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 12.61it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 12.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.95it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.42it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.75it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.61it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.07it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.89it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.65it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.19it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.18it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 15.41it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 12.71it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 11.16it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 10.47it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00,  8.28it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00,  7.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4932889938354492, 'eval_runtime': 4.345, 'eval_samples_per_second': 23.015, 'eval_steps_per_second': 11.507, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  9%|███▊                                      | 10/111 [00:37<03:43,  2.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  8.86it/s]\u001b[A\n",
      "{'loss': 1.6059, 'grad_norm': 2.8542559146881104, 'learning_rate': 9.55614245194068e-05, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.6238, 'grad_norm': 2.4666032791137695, 'learning_rate': 9.220122420149753e-05, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [00:55<02:39,  1.75s/it][INFO|trainer.py:3788] 2024-07-13 11:22:48,523 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:22:48,523 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:22:48,523 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:03, 14.43it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 14.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.76it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 11.96it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.20it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.62it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  8.98it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03, 10.51it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.71it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:02, 12.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 12.49it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 12.23it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.43it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  9.12it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.27it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01,  9.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.15it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.22it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  8.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.62it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  8.67it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  8.41it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.28it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  8.54it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  7.56it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4422801733016968, 'eval_runtime': 5.3932, 'eval_samples_per_second': 18.542, 'eval_steps_per_second': 9.271, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 18%|███████▌                                  | 20/111 [01:00<02:39,  1.75s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.66it/s]\u001b[A\n",
      "{'loss': 1.5939, 'grad_norm': 1.727403998374939, 'learning_rate': 8.799731239943487e-05, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.3663, 'grad_norm': 2.0717852115631104, 'learning_rate': 8.303373616950408e-05, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [01:19<02:26,  1.80s/it][INFO|trainer.py:3788] 2024-07-13 11:23:12,310 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:23:12,310 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:23:12,310 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:03, 15.00it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 16.12it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 14.00it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 14.07it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.77it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.15it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.57it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  8.52it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.43it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  7.93it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02,  9.71it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.60it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 12.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.04it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 13.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.97it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.71it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.04it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:00, 14.92it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 15.19it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 15.30it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 15.20it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 13.98it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 10.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.431177020072937, 'eval_runtime': 4.4926, 'eval_samples_per_second': 22.259, 'eval_steps_per_second': 11.129, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 27%|███████████▎                              | 30/111 [01:23<02:26,  1.80s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.41it/s]\u001b[A\n",
      "{'loss': 1.6831, 'grad_norm': 2.0144295692443848, 'learning_rate': 7.740973022223549e-05, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.3663, 'grad_norm': 1.5981477499008179, 'learning_rate': 7.12377329642024e-05, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [01:45<02:33,  2.16s/it][INFO|trainer.py:3788] 2024-07-13 11:23:38,964 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:23:38,964 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:23:38,964 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 15.98it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.23it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 15.43it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 15.62it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.78it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.88it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 13.79it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.01it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.61it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.10it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 10.75it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:02, 11.34it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 11.11it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.11it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 10.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01,  9.59it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01,  9.58it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.29it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.17it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.87it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.10it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  7.53it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00,  8.91it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00, 10.03it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  7.62it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  6.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4335052967071533, 'eval_runtime': 5.0525, 'eval_samples_per_second': 19.792, 'eval_steps_per_second': 9.896, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 36%|███████████████▏                          | 40/111 [01:50<02:33,  2.16s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  6.44it/s]\u001b[A\n",
      "{'loss': 1.2086, 'grad_norm': 2.018167018890381, 'learning_rate': 6.464113856382752e-05, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.0946, 'grad_norm': 1.8740240335464478, 'learning_rate': 5.7751829983209896e-05, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [02:12<02:10,  2.14s/it][INFO|trainer.py:3788] 2024-07-13 11:24:05,514 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:24:05,515 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:24:05,515 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:04, 10.56it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  8.44it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.84it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.89it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.29it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:02, 12.46it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.43it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 13.06it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.62it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.20it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.20it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  9.90it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02, 10.04it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  9.35it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 10.13it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  9.56it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  8.96it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  8.27it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.21it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.93it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00,  9.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.446669340133667, 'eval_runtime': 5.0518, 'eval_samples_per_second': 19.795, 'eval_steps_per_second': 9.898, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 45%|██████████████████▉                       | 50/111 [02:17<02:10,  2.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 11.44it/s]\u001b[A\n",
      "{'loss': 1.0774, 'grad_norm': 2.3952794075012207, 'learning_rate': 5.0707542297038114e-05, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.0107, 'grad_norm': 2.1666853427886963, 'learning_rate': 4.364910901265606e-05, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [02:36<01:37,  1.91s/it][INFO|trainer.py:3788] 2024-07-13 11:24:29,945 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:24:29,945 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:24:29,945 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.62it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.92it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 13.83it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 13.19it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.85it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.97it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.80it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 11.85it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.10it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.30it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02,  8.75it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.70it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  8.07it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 10.09it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 11.69it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.26it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 10.97it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 11.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:01,  8.70it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.75it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  8.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  8.19it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  8.78it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  7.78it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  7.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4627904891967773, 'eval_runtime': 5.1869, 'eval_samples_per_second': 19.279, 'eval_steps_per_second': 9.64, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 54%|██████████████████████▋                   | 60/111 [02:41<01:37,  1.91s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.90it/s]\u001b[A\n",
      "{'loss': 0.9976, 'grad_norm': 2.617795944213867, 'learning_rate': 3.6717646444456193e-05, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 0.9786, 'grad_norm': 2.2391724586486816, 'learning_rate': 3.0051732434229184e-05, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [02:59<01:13,  1.80s/it][INFO|trainer.py:3788] 2024-07-13 11:24:53,200 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:24:53,200 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:24:53,200 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.47it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.71it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.52it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.19it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 14.15it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 14.24it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.47it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 14.75it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 15.28it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.25it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 15.32it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.46it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 13.75it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.24it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 12.46it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 11.79it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:01, 11.12it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:01,  9.90it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00,  9.86it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00,  8.60it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:03<00:00,  8.21it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:03<00:00,  8.35it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00,  8.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  6.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4712519645690918, 'eval_runtime': 4.5553, 'eval_samples_per_second': 21.952, 'eval_steps_per_second': 10.976, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 63%|██████████████████████████▍               | 70/111 [03:04<01:13,  1.80s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  7.21it/s]\u001b[A\n",
      "{'loss': 1.1216, 'grad_norm': 2.6013777256011963, 'learning_rate': 2.3784635822138424e-05, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 0.9039, 'grad_norm': 2.3789899349212646, 'learning_rate': 1.8041652058350767e-05, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [03:29<01:14,  2.40s/it][INFO|trainer.py:3788] 2024-07-13 11:25:22,392 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:25:22,393 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:25:22,393 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 17.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 16.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.96it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.63it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:03, 11.84it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 11.42it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.57it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.77it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:03,  9.40it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.97it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.76it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.54it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02, 10.58it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 12.12it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 13.05it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 11.28it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.41it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:00, 12.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.33it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00, 10.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00, 10.00it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  8.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4753594398498535, 'eval_runtime': 4.8411, 'eval_samples_per_second': 20.657, 'eval_steps_per_second': 10.328, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 72%|██████████████████████████████▎           | 80/111 [03:33<01:14,  2.40s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  8.99it/s]\u001b[A\n",
      "{'loss': 0.6683, 'grad_norm': 1.9400086402893066, 'learning_rate': 1.2937598223330005e-05, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 0.8361, 'grad_norm': 2.3707640171051025, 'learning_rate': 8.574517537807897e-06, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [03:52<00:37,  1.78s/it][INFO|trainer.py:3788] 2024-07-13 11:25:45,465 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:25:45,466 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:25:45,466 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:03, 13.53it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:03, 14.41it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:04, 10.01it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:04,  9.97it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:03, 10.14it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.08it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.07it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.15it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.43it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.82it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.28it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.35it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.74it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.86it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.48it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.39it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.72it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.44it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.81it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.90it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.56it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 12.65it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:00, 13.15it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00, 12.39it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00, 11.52it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 11.88it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4853711128234863, 'eval_runtime': 5.3111, 'eval_samples_per_second': 18.829, 'eval_steps_per_second': 9.414, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 81%|██████████████████████████████████        | 90/111 [03:57<00:37,  1.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.95it/s]\u001b[A\n",
      "{'loss': 0.7288, 'grad_norm': 2.214077949523926, 'learning_rate': 5.0396392552081564e-06, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 0.8072, 'grad_norm': 2.264308452606201, 'learning_rate': 2.403634723543674e-06, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [04:19<00:26,  2.45s/it][INFO|trainer.py:3788] 2024-07-13 11:26:12,589 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:26:12,589 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:26:12,589 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.03it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  8.73it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:05,  8.41it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:05,  7.69it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:05,  7.10it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:04,  9.73it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:05,  7.60it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  7.53it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  7.94it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.78it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.58it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.26it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.15it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.84it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:03<00:02,  9.61it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:03<00:02,  8.53it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  7.89it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.71it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:04<00:01, 11.32it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:04<00:01, 12.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:00, 13.51it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00, 14.12it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00, 12.07it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 12.05it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  9.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4894825220108032, 'eval_runtime': 5.5981, 'eval_samples_per_second': 17.863, 'eval_steps_per_second': 8.932, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 90%|████████████████████████████████████▉    | 100/111 [04:24<00:26,  2.45s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.07it/s]\u001b[A\n",
      "{'loss': 0.7386, 'grad_norm': 2.257403612136841, 'learning_rate': 7.192044826145771e-07, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 0.7134, 'grad_norm': 2.4599835872650146, 'learning_rate': 2.0024644083921352e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [04:46<00:02,  2.46s/it][INFO|trainer.py:3788] 2024-07-13 11:26:40,078 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:26:40,078 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:26:40,078 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:04, 10.68it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:05,  9.10it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.45it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.15it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.69it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.35it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.17it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.86it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.26it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.53it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.24it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.47it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:01, 10.30it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 10.26it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  9.35it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.61it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  8.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.33it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  8.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00,  9.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 10.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4896905422210693, 'eval_runtime': 5.2542, 'eval_samples_per_second': 19.032, 'eval_steps_per_second': 9.516, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 99%|████████████████████████████████████████▋| 110/111 [04:52<00:02,  2.46s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00, 11.65it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 111/111 [04:54<00:00,  3.90s/it][INFO|trainer.py:3478] 2024-07-13 11:26:47,327 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:26:47,355 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:26:47,374 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:27:10,135 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:27:10,161 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:27:10,176 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/checkpoint-111/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-13 11:27:15,996 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 322.7288, 'train_samples_per_second': 8.366, 'train_steps_per_second': 0.344, 'train_loss': 1.1551600640958495, 'epoch': 2.96, 'num_input_tokens_seen': 1104216}\n",
      "100%|█████████████████████████████████████████| 111/111 [05:22<00:00,  2.91s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-13 11:27:16,009 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:27:16,031 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:27:16,045 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:27:36,346 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:27:36,463 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:27:36,478 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.96\n",
      "  num_input_tokens_seen    =    1104216\n",
      "  total_flos               =  8085178GF\n",
      "  train_loss               =     1.1552\n",
      "  train_runtime            = 0:05:22.72\n",
      "  train_samples_per_second =      8.366\n",
      "  train_steps_per_second   =      0.344\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-13 11:27:38,044 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:27:38,045 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:27:38,045 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.39it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.96\n",
      "  eval_loss               =     1.4897\n",
      "  eval_runtime            = 0:00:03.60\n",
      "  eval_samples_per_second =     27.756\n",
      "  eval_steps_per_second   =     13.878\n",
      "  num_input_tokens_seen   =    1104216\n",
      "[INFO|modelcard.py:449] 2024-07-13 11:27:41,684 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-4 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "lr=1e-5, epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93eb3e05-b604-4699-8971-a9f2d9fc07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-13 11:50:31,516] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/13/2024 11:50:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,890 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,891 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,891 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,891 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,891 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-13 11:50:53,891 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-13 11:50:54,286 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/13/2024 11:50:54 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/13/2024 11:50:54 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/13/2024 11:51:00 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-13 11:51:01,132 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-13 11:51:01,134 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-13 11:51:01,571 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-13 11:51:02,638 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:51:02,644 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-13 11:51:30,878 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-13 11:51:30,879 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-13 11:51:30,891 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-13 11:51:30,892 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.adapter - Set trainable layers: .26.,.27.\n",
      "07/13/2024 11:51:31 - INFO - llamafactory.model.loader - trainable params: 93595648 || all params: 1543714304 || trainable%: 6.0630\n",
      "[INFO|trainer.py:642] 2024-07-13 11:51:31,421 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-13 11:51:32,577 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-13 11:51:32,577 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-13 11:51:32,577 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2131] 2024-07-13 11:51:32,577 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:2134] 2024-07-13 11:51:32,577 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2135] 2024-07-13 11:51:32,577 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-13 11:51:32,577 >>   Total optimization steps = 185\n",
      "[INFO|trainer.py:2137] 2024-07-13 11:51:32,578 >>   Number of trainable parameters = 93,595,648\n",
      "  0%|                                                   | 0/185 [00:00<?, ?it/s]/root/miniconda3/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "{'loss': 1.8412, 'grad_norm': 3.711113452911377, 'learning_rate': 9.981987442712634e-06, 'epoch': 0.13, 'num_input_tokens_seen': 48792}\n",
      "{'loss': 1.7638, 'grad_norm': 3.067967653274536, 'learning_rate': 9.928079551738542e-06, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  5%|██▎                                       | 10/185 [00:27<05:06,  1.75s/it][INFO|trainer.py:3788] 2024-07-13 11:52:00,160 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:52:00,161 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:52:00,161 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 15.19it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 14.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.65it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.21it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:03, 12.12it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 11.83it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 13.37it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 12.44it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.11it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 11.93it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.05it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 13.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.31it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 12.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 13.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:01, 12.57it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 12.24it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 12.39it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 11.72it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 12.14it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00,  9.00it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:04<00:00,  8.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.547890305519104, 'eval_runtime': 4.6448, 'eval_samples_per_second': 21.529, 'eval_steps_per_second': 10.765, 'epoch': 0.27, 'num_input_tokens_seen': 98208}\n",
      "  5%|██▎                                       | 10/185 [00:32<05:06,  1.75s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  7.47it/s]\u001b[A\n",
      "{'loss': 1.6561, 'grad_norm': 3.180854082107544, 'learning_rate': 9.838664734667496e-06, 'epoch': 0.4, 'num_input_tokens_seen': 148704}\n",
      "{'loss': 1.6503, 'grad_norm': 2.6930992603302, 'learning_rate': 9.714387227305422e-06, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 11%|████▌                                     | 20/185 [00:50<05:11,  1.89s/it][INFO|trainer.py:3788] 2024-07-13 11:52:23,081 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:52:23,082 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:52:23,082 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 13.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.37it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.43it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:03, 10.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  8.28it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.30it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.90it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.21it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  8.80it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:03,  8.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.27it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.64it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.80it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 11.47it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 12.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:01, 12.23it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 13.43it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 13.98it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:00, 13.78it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 13.80it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 14.25it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 13.34it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 13.95it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00, 12.37it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.472578763961792, 'eval_runtime': 4.6421, 'eval_samples_per_second': 21.542, 'eval_steps_per_second': 10.771, 'epoch': 0.53, 'num_input_tokens_seen': 196296}\n",
      " 11%|████▌                                     | 20/185 [00:55<05:11,  1.89s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 12.28it/s]\u001b[A\n",
      "{'loss': 1.6172, 'grad_norm': 2.3536198139190674, 'learning_rate': 9.55614245194068e-06, 'epoch': 0.67, 'num_input_tokens_seen': 247056}\n",
      "{'loss': 1.38, 'grad_norm': 2.8871679306030273, 'learning_rate': 9.365070565805941e-06, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 16%|██████▊                                   | 30/185 [01:14<05:26,  2.10s/it][INFO|trainer.py:3788] 2024-07-13 11:52:46,714 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:52:46,714 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:52:46,714 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.40it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 12.89it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.53it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 10.84it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.32it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.41it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.72it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.62it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.33it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  8.46it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.38it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.36it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  9.96it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:02<00:01, 11.22it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01, 12.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01, 10.97it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01, 11.41it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:00, 12.08it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:03<00:00, 10.55it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00, 10.45it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.47it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  9.54it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  8.24it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  7.33it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.452325463294983, 'eval_runtime': 5.205, 'eval_samples_per_second': 19.212, 'eval_steps_per_second': 9.606, 'epoch': 0.8, 'num_input_tokens_seen': 291864}\n",
      " 16%|██████▊                                   | 30/185 [01:19<05:26,  2.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.88it/s]\u001b[A\n",
      "{'loss': 1.6948, 'grad_norm': 2.526693344116211, 'learning_rate': 9.142548246219212e-06, 'epoch': 0.93, 'num_input_tokens_seen': 348504}\n",
      "{'loss': 1.5326, 'grad_norm': 2.446970224380493, 'learning_rate': 8.890178771592198e-06, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 22%|█████████                                 | 40/185 [01:39<04:58,  2.06s/it][INFO|trainer.py:3788] 2024-07-13 11:53:12,401 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:53:12,401 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:53:12,401 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.99it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.08it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 13.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.08it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.60it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 11.31it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 11.17it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  9.06it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:03,  8.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.42it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.14it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  8.96it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  7.72it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:03,  7.15it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  8.23it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  7.43it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:02,  8.28it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  8.10it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  8.77it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01,  7.96it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.16it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.70it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.68it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:05<00:00,  7.47it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  8.40it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  7.13it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4392672777175903, 'eval_runtime': 5.8474, 'eval_samples_per_second': 17.102, 'eval_steps_per_second': 8.551, 'epoch': 1.07, 'num_input_tokens_seen': 399552}\n",
      " 22%|█████████                                 | 40/185 [01:45<04:58,  2.06s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  8.11it/s]\u001b[A\n",
      "{'loss': 1.5753, 'grad_norm': 2.404165029525757, 'learning_rate': 8.609780469772623e-06, 'epoch': 1.2, 'num_input_tokens_seen': 452616}\n",
      "{'loss': 1.5158, 'grad_norm': 2.5387072563171387, 'learning_rate': 8.303373616950408e-06, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 27%|███████████▎                              | 50/185 [02:05<04:23,  1.95s/it][INFO|trainer.py:3788] 2024-07-13 11:53:38,287 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:53:38,287 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:53:38,287 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 15.90it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 16.22it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 14.14it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.04it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 12.95it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.87it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.55it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.10it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 12.98it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 14.02it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 14.03it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.37it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.04it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.06it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 13.40it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 12.28it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.88it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 10.73it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 11.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00,  8.89it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00,  7.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4297780990600586, 'eval_runtime': 4.3059, 'eval_samples_per_second': 23.224, 'eval_steps_per_second': 11.612, 'epoch': 1.33, 'num_input_tokens_seen': 503832}\n",
      " 27%|███████████▎                              | 50/185 [02:09<04:23,  1.95s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  7.97it/s]\u001b[A\n",
      "{'loss': 1.4534, 'grad_norm': 2.7069754600524902, 'learning_rate': 7.973165881521435e-06, 'epoch': 1.47, 'num_input_tokens_seen': 555072}\n",
      "{'loss': 1.4126, 'grad_norm': 2.3428616523742676, 'learning_rate': 7.621536417786159e-06, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 32%|█████████████▌                            | 60/185 [02:29<03:43,  1.78s/it][INFO|trainer.py:3788] 2024-07-13 11:54:01,924 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:54:01,924 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:54:01,925 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.98it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.85it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 13.16it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.23it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 13.43it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 11.33it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 11.02it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.40it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.69it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 10.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.49it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.31it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.09it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02, 10.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01,  9.76it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01, 10.42it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.73it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 10.35it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  9.81it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00,  9.76it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:03<00:00,  9.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.26it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00, 10.31it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  8.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4222668409347534, 'eval_runtime': 4.9214, 'eval_samples_per_second': 20.32, 'eval_steps_per_second': 10.16, 'epoch': 1.6, 'num_input_tokens_seen': 601560}\n",
      " 32%|█████████████▌                            | 60/185 [02:34<03:43,  1.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  8.87it/s]\u001b[A\n",
      "{'loss': 1.4131, 'grad_norm': 2.651017427444458, 'learning_rate': 7.251018724088367e-06, 'epoch': 1.73, 'num_input_tokens_seen': 647904}\n",
      "{'loss': 1.356, 'grad_norm': 2.424999475479126, 'learning_rate': 6.864282388901544e-06, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 38%|███████████████▉                          | 70/185 [02:49<02:56,  1.54s/it][INFO|trainer.py:3788] 2024-07-13 11:54:22,126 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:54:22,126 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:54:22,126 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.18it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.16it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 13.87it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 11.95it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 13.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:03, 11.29it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03, 11.22it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 11.36it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 12.17it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 11.76it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 11.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:02, 11.47it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.45it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 12.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.65it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 12.95it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 13.11it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:01, 12.19it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.30it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 10.99it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00,  9.83it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 10.31it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  8.60it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:04<00:00,  7.80it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4191699028015137, 'eval_runtime': 4.6249, 'eval_samples_per_second': 21.622, 'eval_steps_per_second': 10.811, 'epoch': 1.87, 'num_input_tokens_seen': 694752}\n",
      " 38%|███████████████▉                          | 70/185 [02:54<02:56,  1.54s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  8.35it/s]\u001b[A\n",
      "{'loss': 1.6128, 'grad_norm': 2.7211804389953613, 'learning_rate': 6.464113856382752e-06, 'epoch': 2.0, 'num_input_tokens_seen': 750504}\n",
      "{'loss': 1.5329, 'grad_norm': 2.6829910278320312, 'learning_rate': 6.053396349978632e-06, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 43%|██████████████████▏                       | 80/185 [03:12<02:53,  1.65s/it][INFO|trainer.py:3788] 2024-07-13 11:54:44,948 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:54:44,949 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:54:44,949 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 16.47it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.38it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.17it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.48it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 13.71it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 13.56it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 13.88it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 14.49it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:02, 13.34it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:02, 12.45it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:02, 12.32it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 13.57it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.50it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.45it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.91it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 13.50it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 13.13it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 12.63it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 12.35it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 11.57it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 11.99it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 10.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4147011041641235, 'eval_runtime': 4.0861, 'eval_samples_per_second': 24.473, 'eval_steps_per_second': 12.237, 'epoch': 2.13, 'num_input_tokens_seen': 799752}\n",
      " 43%|██████████████████▏                       | 80/185 [03:16<02:53,  1.65s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 10.44it/s]\u001b[A\n",
      "{'loss': 1.2268, 'grad_norm': 2.430363893508911, 'learning_rate': 5.635089098734394e-06, 'epoch': 2.27, 'num_input_tokens_seen': 849240}\n",
      "{'loss': 1.4721, 'grad_norm': 2.840683937072754, 'learning_rate': 5.212206015980742e-06, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 49%|████████████████████▍                     | 90/185 [03:34<02:45,  1.75s/it][INFO|trainer.py:3788] 2024-07-13 11:55:06,787 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:55:06,787 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:55:06,787 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 19.20it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:02, 15.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 14.34it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 13.21it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 15.46it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 13.38it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.99it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 14.44it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.48it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 14.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 12.85it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.66it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 13.78it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.12it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.66it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 13.59it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 13.23it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 12.73it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 12.94it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 12.37it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 13.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 11.81it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.41237473487854, 'eval_runtime': 3.896, 'eval_samples_per_second': 25.668, 'eval_steps_per_second': 12.834, 'epoch': 2.4, 'num_input_tokens_seen': 896400}\n",
      " 49%|████████████████████▍                     | 90/185 [03:38<02:45,  1.75s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 12.09it/s]\u001b[A\n",
      "{'loss': 1.3546, 'grad_norm': 2.4966132640838623, 'learning_rate': 4.78779398401926e-06, 'epoch': 2.53, 'num_input_tokens_seen': 944808}\n",
      "{'loss': 1.4941, 'grad_norm': 2.508057117462158, 'learning_rate': 4.364910901265607e-06, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 54%|██████████████████████▏                  | 100/185 [03:56<02:47,  1.97s/it][INFO|trainer.py:3788] 2024-07-13 11:55:29,446 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:55:29,446 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:55:29,446 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.49it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.90it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 14.75it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.88it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.89it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 12.78it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 12.51it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 13.66it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 12.52it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.50it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 12.26it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.30it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.33it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 13.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.07it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 12.93it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:01, 12.52it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.47it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.57it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 10.74it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 11.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00,  9.89it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.411116361618042, 'eval_runtime': 4.2178, 'eval_samples_per_second': 23.709, 'eval_steps_per_second': 11.855, 'epoch': 2.67, 'num_input_tokens_seen': 999816}\n",
      " 54%|██████████████████████▏                  | 100/185 [04:01<02:47,  1.97s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.07it/s]\u001b[A\n",
      "{'loss': 1.3826, 'grad_norm': 2.589057445526123, 'learning_rate': 3.94660365002137e-06, 'epoch': 2.8, 'num_input_tokens_seen': 1047072}\n",
      "{'loss': 1.3259, 'grad_norm': 2.9934239387512207, 'learning_rate': 3.5358861436172487e-06, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 59%|████████████████████████▍                | 110/185 [04:20<02:56,  2.35s/it][INFO|trainer.py:3788] 2024-07-13 11:55:53,587 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:55:53,587 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:55:53,587 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.37it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.57it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.21it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.90it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 11.57it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:04,  9.10it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  8.07it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03,  9.83it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.87it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  9.50it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  8.34it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  8.36it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02, 10.21it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:02<00:02,  8.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.72it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01,  9.63it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  8.98it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  9.33it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01, 10.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  8.84it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  8.74it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  8.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  8.30it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  8.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  7.94it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  7.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.408645749092102, 'eval_runtime': 5.5701, 'eval_samples_per_second': 17.953, 'eval_steps_per_second': 8.977, 'epoch': 2.93, 'num_input_tokens_seen': 1095072}\n",
      " 59%|████████████████████████▍                | 110/185 [04:26<02:56,  2.35s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  8.37it/s]\u001b[A\n",
      "{'loss': 1.3738, 'grad_norm': 2.3804378509521484, 'learning_rate': 3.1357176110984578e-06, 'epoch': 3.07, 'num_input_tokens_seen': 1144800}\n",
      "{'loss': 1.2682, 'grad_norm': 2.286926746368408, 'learning_rate': 2.748981275911633e-06, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 65%|██████████████████████████▌              | 120/185 [04:43<01:45,  1.63s/it][INFO|trainer.py:3788] 2024-07-13 11:56:16,270 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:56:16,270 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:56:16,270 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 18.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.70it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 13.90it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.46it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.16it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 11.85it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 11.39it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.46it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 11.06it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 11.33it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02,  9.69it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.54it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.69it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 11.15it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 10.53it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 11.47it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 11.80it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 11.47it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.07it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.02it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 10.61it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 11.23it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.408174991607666, 'eval_runtime': 4.6425, 'eval_samples_per_second': 21.54, 'eval_steps_per_second': 10.77, 'epoch': 3.2, 'num_input_tokens_seen': 1191912}\n",
      " 65%|██████████████████████████▌              | 120/185 [04:48<01:45,  1.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00,  9.86it/s]\u001b[A\n",
      "{'loss': 1.4376, 'grad_norm': 2.509091377258301, 'learning_rate': 2.3784635822138424e-06, 'epoch': 3.33, 'num_input_tokens_seen': 1240056}\n",
      "{'loss': 1.2599, 'grad_norm': 2.211961030960083, 'learning_rate': 2.0268341184785674e-06, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 70%|████████████████████████████▊            | 130/185 [05:05<01:35,  1.73s/it][INFO|trainer.py:3788] 2024-07-13 11:56:38,578 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:56:38,579 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:56:38,579 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.01it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 13.50it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 12.67it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04, 10.19it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 11.90it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.03it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.01it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:03,  9.12it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:03,  9.38it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03,  8.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  8.68it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.68it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.35it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.36it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.34it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02,  8.40it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.01it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:03<00:02,  8.67it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  9.31it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 10.96it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 10.49it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:04<00:01, 10.18it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:04<00:00, 10.28it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 10.50it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.22it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  8.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4086151123046875, 'eval_runtime': 5.4012, 'eval_samples_per_second': 18.514, 'eval_steps_per_second': 9.257, 'epoch': 3.47, 'num_input_tokens_seen': 1290936}\n",
      " 70%|████████████████████████████▊            | 130/185 [05:11<01:35,  1.73s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  9.23it/s]\u001b[A\n",
      "{'loss': 1.5139, 'grad_norm': 2.5806286334991455, 'learning_rate': 1.6966263830495939e-06, 'epoch': 3.6, 'num_input_tokens_seen': 1343448}\n",
      "{'loss': 1.2261, 'grad_norm': 2.313185453414917, 'learning_rate': 1.390219530227378e-06, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 76%|███████████████████████████████          | 140/185 [05:30<01:23,  1.86s/it][INFO|trainer.py:3788] 2024-07-13 11:57:03,150 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:57:03,150 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:57:03,150 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 19.07it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:02, 15.35it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 14.08it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.95it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.02it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:03, 12.01it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 11.83it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.20it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 11.45it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 12.02it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 10.27it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 10.77it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:02, 10.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:02,  9.98it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:02,  8.70it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:03<00:01,  8.73it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:02,  7.95it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:03<00:01,  7.82it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01,  7.96it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:03<00:01,  8.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:01,  7.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  7.01it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:01,  7.54it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  7.58it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  7.46it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:04<00:00,  8.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  7.48it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  7.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4083733558654785, 'eval_runtime': 5.4389, 'eval_samples_per_second': 18.386, 'eval_steps_per_second': 9.193, 'epoch': 3.73, 'num_input_tokens_seen': 1389936}\n",
      " 76%|███████████████████████████████          | 140/185 [05:35<01:23,  1.86s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  7.73it/s]\u001b[A\n",
      "{'loss': 1.3871, 'grad_norm': 2.4585649967193604, 'learning_rate': 1.1098212284078037e-06, 'epoch': 3.87, 'num_input_tokens_seen': 1441848}\n",
      "{'loss': 1.4994, 'grad_norm': 2.6535656452178955, 'learning_rate': 8.574517537807897e-07, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 81%|█████████████████████████████████▏       | 150/185 [05:58<01:20,  2.31s/it][INFO|trainer.py:3788] 2024-07-13 11:57:30,898 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:57:30,898 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:57:30,898 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 15.77it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.38it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04,  9.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.77it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:03, 10.71it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03,  9.46it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:03,  9.34it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  9.26it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.54it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:03, 10.04it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 10.91it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 10.46it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 11.63it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 12.15it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 12.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.64it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.13it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:03<00:01, 13.20it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:01, 12.78it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 11.69it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 11.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 10.46it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:04<00:00, 11.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:04<00:00,  9.82it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.408231496810913, 'eval_runtime': 4.7543, 'eval_samples_per_second': 21.034, 'eval_steps_per_second': 10.517, 'epoch': 4.0, 'num_input_tokens_seen': 1496040}\n",
      " 81%|█████████████████████████████████▏       | 150/185 [06:03<01:20,  2.31s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 10.01it/s]\u001b[A\n",
      "{'loss': 1.2115, 'grad_norm': 2.2432422637939453, 'learning_rate': 6.349294341940593e-07, 'epoch': 4.13, 'num_input_tokens_seen': 1543032}\n",
      "{'loss': 1.3723, 'grad_norm': 2.4565298557281494, 'learning_rate': 4.43857548059321e-07, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 86%|███████████████████████████████████▍     | 160/185 [06:21<00:57,  2.32s/it][INFO|trainer.py:3788] 2024-07-13 11:57:54,448 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:57:54,448 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:57:54,448 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 17.25it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 14.84it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 14.25it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:03, 12.42it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 14.14it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.45it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03, 10.61it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 11.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.69it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 11.72it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:02, 11.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:02, 12.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 12.75it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 13.11it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 12.72it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.11it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 13.14it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 13.21it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 13.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 13.43it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 13.19it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 13.60it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 12.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4079340696334839, 'eval_runtime': 4.1124, 'eval_samples_per_second': 24.317, 'eval_steps_per_second': 12.158, 'epoch': 4.27, 'num_input_tokens_seen': 1595352}\n",
      " 86%|███████████████████████████████████▍     | 160/185 [06:25<00:57,  2.32s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 12.74it/s]\u001b[A\n",
      "{'loss': 1.3836, 'grad_norm': 2.4842636585235596, 'learning_rate': 2.85612772694579e-07, 'epoch': 4.4, 'num_input_tokens_seen': 1644360}\n",
      "{'loss': 1.2811, 'grad_norm': 2.6061394214630127, 'learning_rate': 1.6133526533250566e-07, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 92%|█████████████████████████████████████▋   | 170/185 [06:44<00:31,  2.11s/it][INFO|trainer.py:3788] 2024-07-13 11:58:17,410 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:58:17,410 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:58:17,410 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:03, 12.15it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:03, 11.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:04, 10.14it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  8.83it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.88it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.76it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.75it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:03,  8.87it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:03, 10.58it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 10.93it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:02, 12.08it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:02, 11.74it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:02<00:01, 12.96it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.61it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 14.39it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.04it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.63it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:03<00:00, 14.41it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:03<00:00, 14.50it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 14.71it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 14.36it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 13.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4078588485717773, 'eval_runtime': 4.2589, 'eval_samples_per_second': 23.48, 'eval_steps_per_second': 11.74, 'epoch': 4.53, 'num_input_tokens_seen': 1694616}\n",
      " 92%|█████████████████████████████████████▋   | 170/185 [06:49<00:31,  2.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:04<00:00, 13.26it/s]\u001b[A\n",
      "{'loss': 1.3207, 'grad_norm': 2.295027732849121, 'learning_rate': 7.192044826145772e-08, 'epoch': 4.67, 'num_input_tokens_seen': 1744008}\n",
      "{'loss': 1.326, 'grad_norm': 2.3612990379333496, 'learning_rate': 1.8012557287367394e-08, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 97%|███████████████████████████████████████▉ | 180/185 [07:06<00:09,  1.81s/it][INFO|trainer.py:3788] 2024-07-13 11:58:39,234 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:58:39,234 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:58:39,234 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:02, 16.51it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:04, 11.13it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:03, 11.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:04,  9.15it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:03, 10.82it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:04,  8.87it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:01<00:04,  8.26it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:04,  7.78it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:04,  7.77it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:03,  8.07it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:02<00:04,  7.18it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:02<00:03,  7.96it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:02<00:03,  7.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:02<00:03,  7.20it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:02<00:02,  9.71it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:03<00:02,  8.45it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:03<00:02,  9.53it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:03<00:01,  9.69it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:03<00:01,  9.20it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:04<00:01,  9.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:04<00:01,  9.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:04<00:01,  9.46it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:04<00:00,  9.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:04<00:00,  9.18it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:04<00:00,  9.03it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:05<00:00,  9.77it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:05<00:00,  9.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:05<00:00,  8.48it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4077847003936768, 'eval_runtime': 5.7381, 'eval_samples_per_second': 17.427, 'eval_steps_per_second': 8.714, 'epoch': 4.8, 'num_input_tokens_seen': 1794456}\n",
      " 97%|███████████████████████████████████████▉ | 180/185 [07:12<00:09,  1.81s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:05<00:00,  8.53it/s]\u001b[A\n",
      "{'loss': 1.3429, 'grad_norm': 2.3867080211639404, 'learning_rate': 0.0, 'epoch': 4.93, 'num_input_tokens_seen': 1840776}\n",
      "100%|█████████████████████████████████████████| 185/185 [07:20<00:00,  1.99s/it][INFO|trainer.py:3478] 2024-07-13 11:58:53,105 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:58:53,146 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:58:53,163 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:59:15,114 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:59:15,135 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:59:15,150 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/checkpoint-185/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-13 11:59:21,245 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 468.667, 'train_samples_per_second': 9.602, 'train_steps_per_second': 0.395, 'train_loss': 1.4450835124866384, 'epoch': 4.93, 'num_input_tokens_seen': 1840776}\n",
      "100%|█████████████████████████████████████████| 185/185 [07:48<00:00,  2.53s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-13 11:59:21,264 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5\n",
      "[INFO|configuration_utils.py:472] 2024-07-13 11:59:21,293 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-13 11:59:21,309 >> Configuration saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-13 11:59:40,393 >> Model weights saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-13 11:59:40,424 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-13 11:59:40,437 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     4.9333\n",
      "  num_input_tokens_seen    =    1840776\n",
      "  total_flos               = 13478343GF\n",
      "  train_loss               =     1.4451\n",
      "  train_runtime            = 0:07:48.66\n",
      "  train_samples_per_second =      9.602\n",
      "  train_steps_per_second   =      0.395\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-13 11:59:42,058 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-13 11:59:42,059 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-13 11:59:42,059 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 12.92it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     4.9333\n",
      "  eval_loss               =     1.4078\n",
      "  eval_runtime            = 0:00:03.99\n",
      "  eval_samples_per_second =     25.042\n",
      "  eval_steps_per_second   =     12.521\n",
      "  num_input_tokens_seen   =    1840776\n",
      "[INFO|modelcard.py:449] 2024-07-13 11:59:46,081 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type freeze \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/freeze/train_lr=1e-5_epoch=5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee798a-95d0-4a47-ac86-da1a8a0109b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
