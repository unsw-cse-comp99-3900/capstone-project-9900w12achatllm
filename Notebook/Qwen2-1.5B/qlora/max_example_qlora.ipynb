{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /root/miniconda3/lib/python3.10/site-packages (4.42.4)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /root/miniconda3/lib/python3.10/site-packages (0.32.1)\n",
      "Requirement already satisfied: peft>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: trl>=0.8.6 in /root/miniconda3/lib/python3.10/site-packages (0.9.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/miniconda3/lib/python3.10/site-packages (from trl>=0.8.6) (0.8.5)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (1.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=55ed3bed2b5bce575db2fe0a0233ef5b58a7c0bc1dc19b1ec2a2f35a10125764\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-n_50kp2r/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.8.2.dev0\n",
      "    Uninstalling llamafactory-0.8.2.dev0:\n",
      "      Successfully uninstalled llamafactory-0.8.2.dev0\n",
      "Successfully installed llamafactory-0.8.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: optimum in /root/miniconda3/lib/python3.10/site-packages (1.21.2)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping apex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd83f-942a-4b63-9e4e-e08578379499",
   "metadata": {},
   "source": [
    "max_example = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 04:38:14,918] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 04:38:19 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 04:38:19 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:38:19,031 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 04:38:19,282 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 04:38:19 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 04:38:19 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/12/2024 04:38:25 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 04:38:25,956 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 04:38:25,958 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 04:38:25 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 04:38:26,010 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 04:38:26,616 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 04:38:26,620 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 04:40:44,459 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 04:40:44,460 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 04:40:44,468 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 04:40:44,468 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 04:40:44 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 04:40:44 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 04:40:44 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 04:40:44 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 04:40:44 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,v_proj,o_proj,k_proj,q_proj,gate_proj,down_proj\n",
      "07/12/2024 04:40:57 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 04:40:57,877 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 04:40:58,391 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 04:40:58,391 >>   Num examples = 450\n",
      "[INFO|trainer.py:2130] 2024-07-12 04:40:58,391 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 04:40:58,391 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 04:40:58,391 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 04:40:58,391 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 04:40:58,391 >>   Total optimization steps = 42\n",
      "[INFO|trainer.py:2137] 2024-07-12 04:40:58,396 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 2.014, 'grad_norm': 1.0898600816726685, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.35, 'num_input_tokens_seen': 73728}\n",
      "{'loss': 1.745, 'grad_norm': 0.687960684299469, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 150240}\n",
      " 24%|██████████▏                                | 10/42 [02:11<05:23, 10.10s/it][INFO|trainer.py:3788] 2024-07-12 04:43:09,970 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:43:09,970 >>   Num examples = 50\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:43:09,970 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:03,  7.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:04,  5.21it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:04,  4.50it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:01<00:04,  4.22it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:01<00:04,  4.03it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:04,  3.94it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:01<00:04,  3.89it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:02<00:04,  3.69it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:02<00:04,  3.66it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:02<00:04,  3.48it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:03<00:03,  3.57it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:03<00:03,  3.62it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:03<00:03,  3.64it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:03<00:02,  3.72it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:04<00:02,  3.60it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:04<00:02,  3.49it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:04<00:02,  3.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:05<00:01,  3.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:05<00:01,  3.23it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:05<00:01,  3.00it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:06<00:01,  3.00it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:06<00:00,  2.97it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:06<00:00,  3.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5146658420562744, 'eval_runtime': 7.2652, 'eval_samples_per_second': 6.882, 'eval_steps_per_second': 3.441, 'epoch': 0.71, 'num_input_tokens_seen': 150240}\n",
      " 24%|██████████▏                                | 10/42 [02:18<05:23, 10.10s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  3.35it/s]\u001b[A\n",
      "{'loss': 1.6013, 'grad_norm': 0.6430463194847107, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.06, 'num_input_tokens_seen': 218976}\n",
      "{'loss': 1.4243, 'grad_norm': 0.5421609878540039, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 291520}\n",
      " 48%|████████████████████▍                      | 20/42 [03:46<03:19,  9.05s/it][INFO|trainer.py:3788] 2024-07-12 04:44:45,046 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:44:45,047 >>   Num examples = 50\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:44:45,047 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:03,  6.33it/s]\u001b[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:05,  4.07it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:05,  3.80it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:01<00:05,  3.76it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.63it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:04,  3.60it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:02<00:04,  3.56it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:02<00:04,  3.52it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:02<00:04,  3.51it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:02<00:03,  3.55it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:03<00:03,  3.56it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:03<00:03,  3.65it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:03<00:02,  3.74it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:04<00:02,  3.74it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:04<00:02,  3.87it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:04<00:02,  3.94it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:04<00:01,  3.98it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:04<00:01,  4.16it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:05<00:01,  4.18it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:05<00:00,  4.04it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:05<00:00,  4.01it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:05<00:00,  3.95it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:06<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4070162773132324, 'eval_runtime': 6.8211, 'eval_samples_per_second': 7.33, 'eval_steps_per_second': 3.665, 'epoch': 1.42, 'num_input_tokens_seen': 291520}\n",
      " 48%|████████████████████▍                      | 20/42 [03:53<03:19,  9.05s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  3.92it/s]\u001b[A\n",
      "{'loss': 1.585, 'grad_norm': 0.5727041959762573, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.77, 'num_input_tokens_seen': 364960}\n",
      "{'loss': 1.4363, 'grad_norm': 0.4809879660606384, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.12, 'num_input_tokens_seen': 433424}\n",
      " 71%|██████████████████████████████▋            | 30/42 [05:10<01:33,  7.78s/it][INFO|trainer.py:3788] 2024-07-12 04:46:09,391 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:46:09,391 >>   Num examples = 50\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:46:09,391 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:03,  6.87it/s]\u001b[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:04,  4.89it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:05,  4.16it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:01<00:05,  3.87it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.64it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:05,  3.57it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:02<00:04,  3.48it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:02<00:04,  3.39it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:02<00:04,  3.34it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:02<00:04,  3.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:03<00:03,  3.39it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:03<00:03,  3.29it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:03<00:03,  3.24it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:04<00:03,  3.12it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:04<00:02,  3.02it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:04<00:02,  3.04it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:05<00:02,  3.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:05<00:01,  3.13it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:05<00:01,  3.19it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:06<00:01,  3.06it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:06<00:00,  3.12it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:06<00:00,  3.17it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:07<00:00,  3.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3788177967071533, 'eval_runtime': 7.9153, 'eval_samples_per_second': 6.317, 'eval_steps_per_second': 3.158, 'epoch': 2.12, 'num_input_tokens_seen': 433424}\n",
      " 71%|██████████████████████████████▋            | 30/42 [05:18<01:33,  7.78s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:07<00:00,  2.80it/s]\u001b[A\n",
      "{'loss': 1.4841, 'grad_norm': 0.49840810894966125, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.48, 'num_input_tokens_seen': 505456}\n",
      "{'loss': 1.5903, 'grad_norm': 0.5037906765937805, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.83, 'num_input_tokens_seen': 583152}\n",
      " 95%|████████████████████████████████████████▉  | 40/42 [06:44<00:17,  8.84s/it][INFO|trainer.py:3788] 2024-07-12 04:47:42,891 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:47:42,891 >>   Num examples = 50\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:47:42,891 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 2/25 [00:00<00:03,  6.92it/s]\u001b[A\n",
      " 12%|█████▎                                      | 3/25 [00:00<00:04,  4.93it/s]\u001b[A\n",
      " 16%|███████                                     | 4/25 [00:00<00:05,  4.19it/s]\u001b[A\n",
      " 20%|████████▊                                   | 5/25 [00:01<00:05,  3.90it/s]\u001b[A\n",
      " 24%|██████████▌                                 | 6/25 [00:01<00:05,  3.70it/s]\u001b[A\n",
      " 28%|████████████▎                               | 7/25 [00:01<00:04,  3.64it/s]\u001b[A\n",
      " 32%|██████████████                              | 8/25 [00:02<00:04,  3.60it/s]\u001b[A\n",
      " 36%|███████████████▊                            | 9/25 [00:02<00:04,  3.55it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 10/25 [00:02<00:04,  3.50it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 11/25 [00:02<00:03,  3.62it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 12/25 [00:03<00:03,  3.69it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 13/25 [00:03<00:03,  3.74it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 14/25 [00:03<00:02,  3.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 15/25 [00:03<00:02,  3.94it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 16/25 [00:04<00:02,  4.05it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:04<00:01,  4.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:04<00:01,  4.04it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 19/25 [00:04<00:01,  4.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:05<00:01,  4.19it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 21/25 [00:05<00:00,  4.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:05<00:00,  3.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:05<00:00,  3.92it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:06<00:00,  3.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3736652135849, 'eval_runtime': 6.6646, 'eval_samples_per_second': 7.502, 'eval_steps_per_second': 3.751, 'epoch': 2.83, 'num_input_tokens_seen': 583152}\n",
      " 95%|████████████████████████████████████████▉  | 40/42 [06:51<00:17,  8.84s/it]\n",
      "100%|███████████████████████████████████████████| 25/25 [00:06<00:00,  3.89it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 42/42 [07:06<00:00,  9.75s/it][INFO|trainer.py:3478] 2024-07-12 04:48:05,113 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/checkpoint-42\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f37286252a0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: d1b1be0c-ca1a-43e2-84f9-e002901106b0)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 04:48:15,460 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/checkpoint-42/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 04:48:15,468 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/checkpoint-42/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 04:48:16,537 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 438.1415, 'train_samples_per_second': 3.081, 'train_steps_per_second': 0.096, 'train_loss': 1.606027432850429, 'epoch': 2.97, 'num_input_tokens_seen': 611984}\n",
      "100%|███████████████████████████████████████████| 42/42 [07:18<00:00, 10.43s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 04:48:16,579 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f37284c2d10>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 776db5c7-78fb-4c2e-8720-3cce862acfad)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 04:48:26,878 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 04:48:26,886 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9735\n",
      "  num_input_tokens_seen    =     611984\n",
      "  total_flos               =  4512579GF\n",
      "  train_loss               =      1.606\n",
      "  train_runtime            = 0:07:18.14\n",
      "  train_samples_per_second =      3.081\n",
      "  train_steps_per_second   =      0.096\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 04:48:27,643 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:48:27,643 >>   Num examples = 50\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:48:27,643 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 25/25 [00:07<00:00,  3.22it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9735\n",
      "  eval_loss               =     1.3738\n",
      "  eval_runtime            = 0:00:08.08\n",
      "  eval_samples_per_second =      6.182\n",
      "  eval_steps_per_second   =      3.091\n",
      "  num_input_tokens_seen   =     611984\n",
      "[INFO|modelcard.py:449] 2024-07-12 04:48:35,760 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 250 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_max_example=250 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "max_exmaple = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 04:48:59,012] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 04:49:03 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 04:49:03 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 04:49:03,688 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 04:49:04,013 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 04:49:04 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 04:49:04 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/12/2024 04:49:09 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 04:49:10,587 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 04:49:10,588 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 04:49:10 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 04:49:10,634 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 04:49:11,195 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 04:49:11,199 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 04:51:23,324 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 04:51:23,324 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 04:51:23,331 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 04:51:23,331 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 04:51:23 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 04:51:23 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 04:51:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 04:51:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 04:51:23 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,gate_proj,q_proj,o_proj,k_proj,up_proj\n",
      "07/12/2024 04:51:34 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 04:51:34,100 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 04:51:34,502 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 04:51:34,502 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-12 04:51:34,502 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 04:51:34,502 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 04:51:34,502 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 04:51:34,502 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 04:51:34,502 >>   Total optimization steps = 84\n",
      "[INFO|trainer.py:2137] 2024-07-12 04:51:34,508 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.9547, 'grad_norm': 1.0991132259368896, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 72832}\n",
      "{'loss': 1.7941, 'grad_norm': 0.6734211444854736, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:04<12:19,  9.99s/it][INFO|trainer.py:3788] 2024-07-12 04:53:39,247 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:53:39,247 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:53:39,247 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:06,  7.36it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:08,  5.27it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:09,  4.69it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:10,  4.34it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:10,  4.13it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:10,  4.02it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:10,  3.94it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:10,  3.98it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.99it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:09,  3.93it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:02<00:09,  3.82it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:09,  3.95it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:08,  4.10it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:08,  4.11it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:03<00:08,  4.08it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:07,  4.22it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:07,  4.21it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:07,  4.17it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:04<00:07,  4.27it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:04<00:06,  4.25it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:06,  4.21it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:06,  4.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:05<00:06,  4.20it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:05<00:05,  4.27it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:05,  4.32it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:06<00:05,  4.31it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:06<00:05,  4.25it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:06<00:04,  4.32it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:04,  4.31it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:07<00:04,  4.17it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:07<00:04,  4.30it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:07<00:03,  4.30it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:03,  4.27it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:08<00:03,  4.36it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:08<00:03,  4.34it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:08<00:03,  4.30it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:09<00:02,  4.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:09<00:03,  3.59it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:09<00:03,  3.33it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:09<00:02,  3.51it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:10<00:02,  3.68it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:10<00:01,  3.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:10<00:01,  3.70it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:10<00:01,  3.87it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:11<00:01,  3.95it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:11<00:00,  3.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:11<00:00,  4.01it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:11<00:00,  4.13it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4726632833480835, 'eval_runtime': 12.4676, 'eval_samples_per_second': 8.021, 'eval_steps_per_second': 4.01, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:17<12:19,  9.99s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  4.10it/s]\u001b[A\n",
      "{'loss': 1.515, 'grad_norm': 0.6772484183311462, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 215776}\n",
      "{'loss': 1.4885, 'grad_norm': 0.7022586464881897, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [03:42<09:14,  8.66s/it][INFO|trainer.py:3788] 2024-07-12 04:55:17,382 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:55:17,383 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:55:17,383 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.25it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  3.96it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:12,  3.66it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.32it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:13,  3.19it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.21it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.07it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.00it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.85it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  2.95it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.19it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.29it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.48it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:08,  3.72it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.76it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.80it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  3.98it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:06,  4.02it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  4.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  4.11it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:05,  4.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.23it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.13it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  4.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.13it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.08it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.22it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  4.20it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.16it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.07it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.05it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.04it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.10it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.09it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.11it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  3.98it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3390065431594849, 'eval_runtime': 13.4931, 'eval_samples_per_second': 7.411, 'eval_steps_per_second': 3.706, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [03:56<09:14,  8.66s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.97it/s]\u001b[A\n",
      "{'loss': 1.4275, 'grad_norm': 0.494032084941864, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 357632}\n",
      "{'loss': 1.5319, 'grad_norm': 0.4413515329360962, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [05:19<08:13,  9.13s/it][INFO|trainer.py:3788] 2024-07-12 04:56:54,178 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:56:54,178 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:56:54,178 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.08it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.57it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.12it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.13it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.21it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:13,  3.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.34it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.42it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.42it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.01it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:12,  2.91it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.01it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  3.05it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  2.90it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.85it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:11,  2.81it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  2.85it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:09,  3.03it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  2.92it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:09,  2.84it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:09,  3.00it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:07,  3.27it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:07,  3.48it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:06,  3.69it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:06,  3.80it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  3.83it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.92it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:05,  3.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  3.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.08it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.06it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:04,  3.91it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:04,  3.71it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  3.67it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  3.58it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:03,  3.54it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:03,  3.48it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  3.54it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  3.48it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  3.49it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  3.53it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  3.54it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.53it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  3.53it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  3.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.53it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.44it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2962123155593872, 'eval_runtime': 15.1887, 'eval_samples_per_second': 6.584, 'eval_steps_per_second': 3.292, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [05:34<08:13,  9.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.39it/s]\u001b[A\n",
      "{'loss': 1.4865, 'grad_norm': 0.6378650069236755, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 507104}\n",
      "{'loss': 1.4442, 'grad_norm': 0.4713888466358185, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [06:58<06:22,  8.69s/it][INFO|trainer.py:3788] 2024-07-12 04:58:33,186 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 04:58:33,186 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 04:58:33,186 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.73it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.38it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.22it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.22it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.91it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.80it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:14,  2.77it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.82it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.88it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:13,  2.80it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.70it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.59it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.62it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:12,  2.60it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.56it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:11,  2.59it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.62it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.60it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.60it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.58it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:10,  2.59it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.61it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:09,  2.63it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:08,  2.64it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:07,  2.58it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:06,  2.67it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:06,  2.72it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  2.70it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.72it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:05,  2.67it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:05,  2.59it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:04,  2.58it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:04,  2.61it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.72it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.72it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  2.73it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.77it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:16<00:02,  2.71it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  2.74it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.84it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:17<00:01,  2.70it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  2.69it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  2.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.283237338066101, 'eval_runtime': 18.8618, 'eval_samples_per_second': 5.302, 'eval_steps_per_second': 2.651, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:17<06:22,  8.69s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  2.60it/s]\u001b[A\n",
      "{'loss': 1.4273, 'grad_norm': 0.4922820031642914, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 651104}\n",
      "{'loss': 1.4433, 'grad_norm': 0.45068123936653137, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [08:39<04:50,  8.54s/it][INFO|trainer.py:3788] 2024-07-12 05:00:13,834 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:00:13,834 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:00:13,834 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:06,  6.88it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.63it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:12,  3.71it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.45it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.28it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.25it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.28it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.31it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:12,  3.33it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.18it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.25it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:10,  3.28it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.35it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:10,  3.38it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:09,  3.39it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.29it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.29it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:09,  3.31it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:08,  3.25it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:08,  3.27it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:08,  3.29it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:07,  3.37it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:07,  3.45it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:07,  3.39it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.35it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:06,  3.35it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:06,  3.43it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.41it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:05,  3.35it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:05,  3.30it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:05,  3.17it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:05,  3.04it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:05,  3.00it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:04,  2.91it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:04,  2.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:04,  2.74it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:03,  2.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:03,  2.80it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:03,  2.78it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  2.96it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:13<00:02,  3.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  3.44it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.61it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  3.71it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:14<00:00,  3.73it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.277538776397705, 'eval_runtime': 15.3298, 'eval_samples_per_second': 6.523, 'eval_steps_per_second': 3.262, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [08:54<04:50,  8.54s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  3.83it/s]\u001b[A\n",
      "{'loss': 1.4582, 'grad_norm': 0.48731908202171326, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 793792}\n",
      "{'loss': 1.459, 'grad_norm': 0.4513431489467621, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [10:20<03:33,  8.90s/it][INFO|trainer.py:3788] 2024-07-12 05:01:54,815 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:01:54,815 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:01:54,815 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.21it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.36it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:12,  3.70it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.19it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.93it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.84it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:15,  2.67it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.61it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.61it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.76it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:11,  3.05it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:10,  3.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.56it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.71it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:08,  3.85it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  4.02it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  4.00it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:06,  4.03it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  4.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  4.09it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.10it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  3.72it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.58it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:05,  3.47it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:05,  3.55it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  3.53it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:04,  3.49it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:04,  3.50it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:04,  3.48it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  3.44it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:03,  3.43it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:03,  3.45it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  3.44it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  3.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  3.42it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:02,  3.47it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  3.45it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.40it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  3.38it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  3.31it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.31it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.276809811592102, 'eval_runtime': 15.0916, 'eval_samples_per_second': 6.626, 'eval_steps_per_second': 3.313, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [10:35<03:33,  8.90s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.28it/s]\u001b[A\n",
      "{'loss': 1.3997, 'grad_norm': 0.5177717208862305, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 938976}\n",
      "{'loss': 1.5286, 'grad_norm': 0.47875064611434937, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:03<02:11,  9.37s/it][INFO|trainer.py:3788] 2024-07-12 05:03:37,976 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:03:37,977 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:03:37,977 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.52it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.35it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.50it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.03it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:13,  3.09it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.12it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.12it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.08it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.91it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:13,  2.79it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:12,  2.92it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.04it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  3.16it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:10,  3.26it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.37it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.37it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.40it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:08,  3.49it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:08,  3.45it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:08,  3.46it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.59it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  3.79it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  3.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.01it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.04it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:05,  3.54it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:05,  3.48it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  3.52it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  3.49it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:04,  3.72it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  3.84it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  3.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  4.07it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.12it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  3.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.97it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.13it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.19it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  4.12it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  4.09it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2738233804702759, 'eval_runtime': 14.0884, 'eval_samples_per_second': 7.098, 'eval_steps_per_second': 3.549, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:17<02:11,  9.37s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.99it/s]\u001b[A\n",
      "{'loss': 1.4641, 'grad_norm': 0.5311319231987, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1088704}\n",
      "{'loss': 1.4093, 'grad_norm': 0.4978927671909332, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [13:43<00:33,  8.48s/it][INFO|trainer.py:3788] 2024-07-12 05:05:18,484 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:05:18,484 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:05:18,484 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.84it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.34it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.28it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  2.98it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.99it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.01it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.00it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.76it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.63it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.59it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.54it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:14,  2.54it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.55it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.54it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:12,  2.60it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.55it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:11,  2.58it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.70it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.63it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.59it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.60it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:09,  2.82it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  3.03it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:07,  3.31it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:06,  3.48it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:06,  3.58it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:05,  3.66it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:05,  3.55it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:05,  3.64it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:04,  3.82it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:04,  3.83it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:04,  3.82it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:11<00:03,  3.93it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:03,  3.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  3.86it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:12<00:03,  3.82it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:12<00:02,  3.84it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:02,  3.90it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  3.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:13<00:02,  3.87it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:13<00:01,  3.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  3.91it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.95it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:14<00:01,  3.91it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:14<00:00,  3.63it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.46it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:15<00:00,  3.41it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.275467872619629, 'eval_runtime': 15.8124, 'eval_samples_per_second': 6.324, 'eval_steps_per_second': 3.162, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [13:59<00:33,  8.48s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  3.38it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 84/84 [14:34<00:00, 10.65s/it][INFO|trainer.py:3478] 2024-07-12 05:06:09,303 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/checkpoint-84\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f25e02c7b50>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 13da91d6-ce5c-4dd5-9f05-7970e6e6b66e)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 05:06:19,783 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/checkpoint-84/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 05:06:19,793 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/checkpoint-84/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 05:06:21,355 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 886.8473, 'train_samples_per_second': 3.044, 'train_steps_per_second': 0.095, 'train_loss': 1.5157244489306496, 'epoch': 2.99, 'num_input_tokens_seen': 1218528}\n",
      "100%|███████████████████████████████████████████| 84/84 [14:46<00:00, 10.56s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 05:06:21,364 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f25d87e66e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 71bea050-3523-4d9c-a554-78ce1c6362ca)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 05:06:31,834 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 05:06:31,843 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =    1218528\n",
      "  total_flos               =  8985046GF\n",
      "  train_loss               =     1.5157\n",
      "  train_runtime            = 0:14:46.84\n",
      "  train_samples_per_second =      3.044\n",
      "  train_steps_per_second   =      0.095\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 05:06:32,702 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:06:32,702 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:06:32,702 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.43it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.2745\n",
      "  eval_runtime            = 0:00:14.96\n",
      "  eval_samples_per_second =      6.681\n",
      "  eval_steps_per_second   =       3.34\n",
      "  num_input_tokens_seen   =    1218528\n",
      "[INFO|modelcard.py:449] 2024-07-12 05:06:47,693 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_max_example=500 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "max_example = 750"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72b73d-6bb7-4671-8ebc-5efb909cfafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 05:07:20,290] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 05:07:49 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 05:07:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,266 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,267 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,267 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,267 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,267 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 05:07:49,267 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 05:07:49,546 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 05:07:49 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 05:07:49 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 750/750 [00:00<00:00, 1724.0\n",
      "07/12/2024 05:07:56 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Converting format of dataset (num_proc=16): 100%|█| 750/750 [00:00<00:00, 1977.9\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1500/1500 [00:02<00:00, 549.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 05:08:01,354 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 05:08:01,356 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 05:08:01 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 05:08:01,397 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 05:08:02,074 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 05:08:02,079 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 05:10:34,440 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 05:10:34,441 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 05:10:34,448 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 05:10:34,448 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 05:10:34 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 05:10:34 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 05:10:34 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 05:10:34 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 05:10:34 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,o_proj,k_proj,down_proj,v_proj,up_proj,q_proj\n",
      "07/12/2024 05:10:46 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 05:10:46,401 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 05:10:46,951 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 05:10:46,951 >>   Num examples = 1,350\n",
      "[INFO|trainer.py:2130] 2024-07-12 05:10:46,951 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 05:10:46,951 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 05:10:46,951 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 05:10:46,951 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 05:10:46,951 >>   Total optimization steps = 126\n",
      "[INFO|trainer.py:2137] 2024-07-12 05:10:46,957 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.9918, 'grad_norm': 0.954384446144104, 'learning_rate': 4.9805980165004304e-05, 'epoch': 0.12, 'num_input_tokens_seen': 72320}\n",
      "{'loss': 1.7507, 'grad_norm': 0.6934303641319275, 'learning_rate': 4.922693215572695e-05, 'epoch': 0.24, 'num_input_tokens_seen': 144768}\n",
      "  8%|███▎                                      | 10/126 [02:22<18:39,  9.65s/it][INFO|trainer.py:3788] 2024-07-12 05:13:09,921 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:13:09,921 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:13:09,921 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:12,  5.99it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:16,  4.28it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:18,  3.82it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:22,  3.05it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:24,  2.86it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:02<00:23,  2.88it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:23,  2.91it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:23,  2.80it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:03<00:22,  2.91it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:03<00:21,  3.04it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:21,  2.94it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:04<00:21,  2.86it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:04<00:21,  2.90it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:20,  2.93it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:05<00:20,  2.94it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:05<00:19,  2.99it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:18,  3.04it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:06<00:19,  2.88it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:06<00:19,  2.83it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:18,  2.86it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:07<00:19,  2.76it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:07<00:19,  2.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:08<00:19,  2.66it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:08<00:18,  2.69it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:08<00:18,  2.58it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:09<00:18,  2.55it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:09<00:18,  2.53it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:10<00:17,  2.59it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:10<00:17,  2.58it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:10<00:16,  2.69it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:11<00:15,  2.70it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:11<00:15,  2.70it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:11<00:14,  2.74it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:12<00:14,  2.70it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:12<00:14,  2.71it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:13<00:15,  2.51it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:13<00:14,  2.49it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:13<00:14,  2.50it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:14<00:14,  2.48it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:14<00:13,  2.55it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:15<00:12,  2.69it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:15<00:11,  2.71it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:15<00:11,  2.69it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:16<00:11,  2.64it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:16<00:10,  2.72it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:16<00:09,  2.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:17<00:08,  3.10it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:17<00:08,  3.15it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:17<00:07,  3.32it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:17<00:07,  3.37it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:18<00:06,  3.38it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:18<00:06,  3.40it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:18<00:05,  3.52it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:19<00:05,  3.48it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:19<00:05,  3.45it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:19<00:05,  3.52it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:19<00:04,  3.46it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:20<00:04,  3.40it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:20<00:04,  3.21it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:20<00:04,  3.11it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:21<00:04,  3.12it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:21<00:03,  3.22it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:21<00:03,  3.20it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:22<00:02,  3.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:22<00:02,  3.47it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:22<00:02,  3.65it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:22<00:01,  3.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:23<00:01,  3.78it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:23<00:01,  3.86it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:23<00:01,  3.88it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:23<00:00,  3.93it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:24<00:00,  3.87it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:24<00:00,  3.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4020776748657227, 'eval_runtime': 24.9652, 'eval_samples_per_second': 6.008, 'eval_steps_per_second': 3.004, 'epoch': 0.24, 'num_input_tokens_seen': 144768}\n",
      "  8%|███▎                                      | 10/126 [02:47<18:39,  9.65s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:24<00:00,  4.02it/s]\u001b[A\n",
      "{'loss': 1.5587, 'grad_norm': 0.6673088073730469, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 213376}\n",
      "{'loss': 1.4856, 'grad_norm': 0.5172924995422363, 'learning_rate': 4.6955539334255716e-05, 'epoch': 0.47, 'num_input_tokens_seen': 284064}\n",
      " 16%|██████▋                                   | 20/126 [04:16<17:10,  9.72s/it][INFO|trainer.py:3788] 2024-07-12 05:15:03,629 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:15:03,630 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:15:03,630 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:13,  5.35it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:19,  3.77it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:01<00:21,  3.34it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:21,  3.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:22,  3.06it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:02<00:23,  2.87it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:24,  2.77it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:24,  2.66it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:03<00:24,  2.60it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:03<00:24,  2.65it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:04<00:24,  2.62it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:04<00:23,  2.59it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:04<00:23,  2.59it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:05<00:23,  2.60it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:05<00:22,  2.65it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:06<00:22,  2.60it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:06<00:21,  2.62it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:06<00:21,  2.59it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:07<00:21,  2.61it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:07<00:20,  2.67it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:07<00:19,  2.76it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:08<00:18,  2.82it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:08<00:18,  2.73it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:08<00:18,  2.77it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:09<00:18,  2.71it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:09<00:18,  2.63it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:10<00:17,  2.67it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:10<00:16,  2.75it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:10<00:17,  2.53it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:11<00:18,  2.40it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:11<00:16,  2.53it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:12<00:14,  2.86it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:12<00:12,  3.20it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:12<00:12,  3.24it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:12<00:11,  3.30it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:13<00:10,  3.49it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:13<00:10,  3.63it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:13<00:09,  3.79it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:13<00:08,  3.89it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:14<00:08,  4.04it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:14<00:07,  4.17it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:14<00:07,  4.14it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:14<00:07,  3.93it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:15<00:07,  3.82it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:15<00:07,  3.68it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:15<00:07,  3.69it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:15<00:07,  3.54it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:16<00:07,  3.45it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:16<00:07,  3.56it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:16<00:06,  3.57it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:17<00:06,  3.45it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:17<00:06,  3.45it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:17<00:05,  3.54it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:17<00:05,  3.46it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:18<00:05,  3.39it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:18<00:05,  3.49it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:18<00:04,  3.52it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:19<00:04,  3.52it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:19<00:04,  3.53it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:19<00:03,  3.53it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:19<00:03,  3.56it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:20<00:03,  3.58it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:20<00:03,  3.67it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:20<00:02,  3.91it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:20<00:02,  4.01it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:21<00:01,  4.17it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:21<00:01,  4.22it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:21<00:01,  4.19it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:21<00:01,  4.18it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:22<00:00,  4.09it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:22<00:00,  4.10it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:22<00:00,  4.05it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:22<00:00,  4.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2901382446289062, 'eval_runtime': 23.4835, 'eval_samples_per_second': 6.387, 'eval_steps_per_second': 3.194, 'epoch': 0.47, 'num_input_tokens_seen': 284064}\n",
      " 16%|██████▋                                   | 20/126 [04:40<17:10,  9.72s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:23<00:00,  4.16it/s]\u001b[A\n",
      "{'loss': 1.5497, 'grad_norm': 0.5066424608230591, 'learning_rate': 4.529845014289642e-05, 'epoch': 0.59, 'num_input_tokens_seen': 357920}\n",
      "{'loss': 1.4564, 'grad_norm': 0.38873523473739624, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 429952}\n",
      " 24%|██████████                                | 30/126 [06:06<13:38,  8.53s/it][INFO|trainer.py:3788] 2024-07-12 05:16:53,263 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:16:53,263 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:16:53,263 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  7.19it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:16,  4.44it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:18,  3.85it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:20,  3.34it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:22,  3.11it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:21,  3.17it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:20,  3.27it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:20,  3.28it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:20,  3.16it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:03<00:20,  3.16it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:19,  3.21it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:19,  3.25it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:04<00:18,  3.26it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:18,  3.30it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:17,  3.28it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:05<00:17,  3.30it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:17,  3.35it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:17,  3.24it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:06<00:18,  3.04it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:17,  3.06it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:17,  3.07it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:16,  3.15it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:07<00:16,  3.11it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:15,  3.14it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:15,  3.17it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:08<00:14,  3.20it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:14,  3.24it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:13,  3.35it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:09<00:14,  3.14it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:09<00:14,  2.96it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:14,  2.93it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:10<00:14,  2.84it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:10<00:13,  3.01it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:10<00:12,  3.11it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:11<00:12,  3.20it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:11<00:12,  3.16it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:11<00:11,  3.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:12<00:11,  3.22it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:12<00:10,  3.26it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:12<00:10,  3.37it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:12<00:09,  3.45it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:13<00:09,  3.42it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:13<00:08,  3.45it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:13<00:08,  3.39it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:14<00:08,  3.31it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:14<00:08,  3.36it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:14<00:07,  3.38it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:15<00:07,  3.32it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:15<00:07,  3.41it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:15<00:07,  3.38it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:15<00:06,  3.33it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:16<00:06,  3.29it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:16<00:06,  3.38it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:16<00:05,  3.35it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:17<00:05,  3.30it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:17<00:05,  3.33it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:17<00:05,  3.35it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:17<00:04,  3.47it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:18<00:04,  3.63it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:18<00:03,  3.76it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:18<00:03,  3.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:18<00:03,  3.76it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:19<00:03,  3.60it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:19<00:02,  3.61it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:19<00:02,  3.66it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:20<00:02,  3.69it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:20<00:01,  3.55it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:20<00:01,  3.31it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:21<00:01,  3.14it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:21<00:01,  3.15it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:21<00:00,  3.22it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:22<00:00,  3.16it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:22<00:00,  3.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2529120445251465, 'eval_runtime': 22.9692, 'eval_samples_per_second': 6.53, 'eval_steps_per_second': 3.265, 'epoch': 0.71, 'num_input_tokens_seen': 429952}\n",
      " 24%|██████████                                | 30/126 [06:29<13:38,  8.53s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:22<00:00,  3.28it/s]\u001b[A\n",
      "{'loss': 1.5014, 'grad_norm': 0.4905202388763428, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.83, 'num_input_tokens_seen': 506784}\n",
      "{'loss': 1.5485, 'grad_norm': 0.5397549867630005, 'learning_rate': 3.856365659664399e-05, 'epoch': 0.95, 'num_input_tokens_seen': 582368}\n",
      " 32%|█████████████▎                            | 40/126 [08:00<13:31,  9.43s/it][INFO|trainer.py:3788] 2024-07-12 05:18:47,698 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:18:47,699 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:18:47,699 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:19,  3.83it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:24,  2.97it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:01<00:26,  2.69it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:27,  2.56it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:02<00:29,  2.32it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:02<00:30,  2.21it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:03<00:29,  2.25it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:03<00:29,  2.27it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:04<00:28,  2.26it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:04<00:25,  2.47it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:04<00:25,  2.52it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:05<00:26,  2.38it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:05<00:25,  2.40it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:06<00:24,  2.42it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:06<00:24,  2.41it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:06<00:23,  2.43it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:07<00:22,  2.51it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:07<00:22,  2.48it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:08<00:22,  2.48it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:08<00:21,  2.55it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:08<00:21,  2.48it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:09<00:20,  2.49it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:09<00:20,  2.44it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:10<00:20,  2.46it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:10<00:19,  2.49it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:10<00:19,  2.50it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:11<00:18,  2.52it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:11<00:17,  2.57it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:12<00:17,  2.54it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:12<00:17,  2.55it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:12<00:16,  2.62it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:13<00:16,  2.60it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:13<00:15,  2.64it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:13<00:14,  2.70it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:14<00:14,  2.77it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:14<00:13,  2.77it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:15<00:13,  2.79it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:15<00:13,  2.70it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:15<00:12,  2.85it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:16<00:11,  3.01it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:16<00:10,  3.04it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:16<00:10,  3.01it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:17<00:10,  2.97it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:17<00:10,  2.73it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:17<00:11,  2.58it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:18<00:10,  2.62it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:18<00:11,  2.45it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:19<00:10,  2.44it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:19<00:10,  2.49it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:19<00:09,  2.58it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:20<00:08,  2.67it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:20<00:08,  2.54it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:21<00:08,  2.52it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:21<00:08,  2.47it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:21<00:07,  2.48it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:22<00:06,  2.63it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:22<00:06,  2.64it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:22<00:06,  2.62it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:23<00:06,  2.41it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:23<00:05,  2.50it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:24<00:04,  2.71it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:24<00:04,  2.87it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:24<00:03,  2.97it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:25<00:03,  3.10it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:25<00:02,  3.24it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:25<00:02,  3.36it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:25<00:02,  3.38it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:26<00:01,  3.36it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:26<00:01,  3.31it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:26<00:01,  3.26it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:27<00:00,  3.29it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:27<00:00,  3.23it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:27<00:00,  3.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2436054944992065, 'eval_runtime': 28.4706, 'eval_samples_per_second': 5.269, 'eval_steps_per_second': 2.634, 'epoch': 0.95, 'num_input_tokens_seen': 582368}\n",
      " 32%|█████████████▎                            | 40/126 [08:29<13:31,  9.43s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:28<00:00,  3.29it/s]\u001b[A\n",
      "{'loss': 1.4283, 'grad_norm': 0.5656865239143372, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 655392}\n",
      "{'loss': 1.4623, 'grad_norm': 0.5212016701698303, 'learning_rate': 3.2962166256292113e-05, 'epoch': 1.18, 'num_input_tokens_seen': 725408}\n",
      " 40%|████████████████▋                         | 50/126 [09:57<11:58,  9.45s/it][INFO|trainer.py:3788] 2024-07-12 05:20:44,419 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:20:44,419 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:20:44,419 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  7.17it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:14,  5.08it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:16,  4.24it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:18,  3.84it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.68it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:19,  3.55it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:18,  3.54it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:19,  3.46it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:18,  3.44it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.49it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.44it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:18,  3.44it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.43it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:17,  3.41it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:17,  3.32it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:17,  3.34it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:17,  3.29it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:17,  3.19it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:18,  2.98it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:18,  2.95it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:17,  2.99it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:16,  3.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:07<00:16,  3.04it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:16,  3.03it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:15,  3.08it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:08<00:15,  3.11it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:15,  3.12it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:14,  3.25it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:14,  3.21it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:09<00:13,  3.24it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:12,  3.35it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:12,  3.29it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:10<00:13,  3.06it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:10<00:13,  2.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:12,  3.08it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:11<00:11,  3.17it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:11<00:11,  3.22it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:11<00:10,  3.29it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:12<00:10,  3.31it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:12<00:09,  3.42it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:12<00:09,  3.43it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:12<00:09,  3.40it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:13<00:09,  3.34it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:13<00:09,  3.25it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:13<00:09,  2.93it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:14<00:09,  2.87it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:14<00:09,  2.85it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:15<00:09,  2.75it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:15<00:08,  2.79it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:15<00:08,  2.84it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:16<00:07,  3.00it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:16<00:07,  3.05it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:16<00:06,  3.20it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:16<00:06,  3.24it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:17<00:05,  3.26it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:17<00:05,  3.36it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:17<00:05,  3.33it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:18<00:04,  3.29it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:18<00:04,  3.28it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:18<00:04,  3.31it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:19<00:03,  3.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:19<00:03,  3.40it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:19<00:03,  3.32it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:19<00:02,  3.43it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:20<00:02,  3.50it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:20<00:02,  3.49it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:20<00:02,  3.38it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:21<00:01,  3.04it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:21<00:01,  2.89it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:21<00:01,  2.98it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:22<00:00,  3.12it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:22<00:00,  3.17it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:22<00:00,  3.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2366706132888794, 'eval_runtime': 23.4479, 'eval_samples_per_second': 6.397, 'eval_steps_per_second': 3.199, 'epoch': 1.18, 'num_input_tokens_seen': 725408}\n",
      " 40%|████████████████▋                         | 50/126 [10:20<11:58,  9.45s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:23<00:00,  3.17it/s]\u001b[A\n",
      "{'loss': 1.4994, 'grad_norm': 0.43231120705604553, 'learning_rate': 2.9953653579984942e-05, 'epoch': 1.3, 'num_input_tokens_seen': 798112}\n",
      "{'loss': 1.3941, 'grad_norm': 0.486319899559021, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 868384}\n",
      " 48%|████████████████████                      | 60/126 [11:40<09:06,  8.28s/it][INFO|trainer.py:3788] 2024-07-12 05:22:27,471 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:22:27,471 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:22:27,471 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  6.84it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:14,  4.84it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:17,  4.04it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:18,  3.75it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.64it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:19,  3.53it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:19,  3.51it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:19,  3.45it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:19,  3.38it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.40it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.40it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:18,  3.40it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:18,  3.37it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:17,  3.36it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:18,  3.25it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:17,  3.25it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:16,  3.37it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:16,  3.43it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:15,  3.46it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:05<00:15,  3.57it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:14,  3.63it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:14,  3.70it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:06<00:14,  3.58it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:14,  3.55it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:13,  3.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:13,  3.58it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:07<00:12,  3.63it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:12,  3.78it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:11,  3.77it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:11,  3.83it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:08<00:11,  3.89it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:10,  3.84it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:10,  3.88it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:09<00:10,  3.88it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:09<00:10,  3.87it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:09,  3.86it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:09,  3.71it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:10<00:09,  3.69it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:10<00:09,  3.73it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:11<00:09,  3.72it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:11<00:08,  3.74it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:11<00:08,  3.69it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:12<00:08,  3.70it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:08,  3.59it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:12<00:08,  3.43it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:12<00:08,  3.46it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:13<00:07,  3.46it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:13<00:07,  3.36it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:13<00:07,  3.45it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:14<00:07,  3.39it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:14<00:06,  3.39it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:14<00:06,  3.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:15<00:06,  3.40it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:15<00:06,  3.33it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:15<00:05,  3.33it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:15<00:05,  3.43it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:16<00:04,  3.47it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:16<00:04,  3.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:04,  3.67it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:16<00:03,  3.80it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:17<00:03,  3.94it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:17<00:03,  3.96it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:17<00:02,  3.86it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:17<00:02,  3.94it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:18<00:02,  3.79it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:18<00:02,  3.93it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:18<00:01,  3.95it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:19<00:01,  3.71it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:19<00:01,  3.35it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:19<00:01,  3.42it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:19<00:00,  3.57it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:20<00:00,  3.62it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:20<00:00,  3.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.232720971107483, 'eval_runtime': 21.0126, 'eval_samples_per_second': 7.139, 'eval_steps_per_second': 3.569, 'epoch': 1.42, 'num_input_tokens_seen': 868384}\n",
      " 48%|████████████████████                      | 60/126 [12:01<09:06,  8.28s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:20<00:00,  3.84it/s]\u001b[A\n",
      "{'loss': 1.5051, 'grad_norm': 0.6111595034599304, 'learning_rate': 2.375385285848257e-05, 'epoch': 1.54, 'num_input_tokens_seen': 944736}\n",
      "{'loss': 1.3111, 'grad_norm': 0.5820989608764648, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.66, 'num_input_tokens_seen': 1015776}\n",
      " 56%|███████████████████████▎                  | 70/126 [13:24<07:58,  8.54s/it][INFO|trainer.py:3788] 2024-07-12 05:24:11,748 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:24:11,748 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:24:11,748 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:12,  5.87it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:15,  4.60it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:18,  3.85it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:19,  3.67it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.65it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:19,  3.56it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:18,  3.57it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:18,  3.50it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:18,  3.45it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.50it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.49it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:17,  3.49it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.47it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:17,  3.47it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:17,  3.41it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:17,  3.37it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:16,  3.42it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:16,  3.39it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:16,  3.32it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:05<00:15,  3.38it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:15,  3.35it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:15,  3.34it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:06<00:15,  3.27it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:15,  3.25it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:14,  3.29it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:14,  3.27it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:14,  3.28it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:13,  3.36it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:13,  3.36it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:13,  3.36it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:12,  3.43it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:12,  3.38it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:12,  3.40it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:10<00:11,  3.42it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:11,  3.52it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:10,  3.66it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:09,  3.71it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:11<00:09,  3.81it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:11<00:09,  3.83it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:11<00:08,  4.00it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:11<00:08,  4.00it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:12<00:07,  4.04it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:12<00:07,  4.16it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:07,  4.12it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:12<00:07,  4.01it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:13<00:06,  4.12it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:13<00:06,  4.13it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:13<00:06,  4.01it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:13<00:06,  4.15it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:14<00:05,  4.10it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:14<00:05,  4.10it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:14<00:05,  3.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:14<00:05,  3.93it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:15<00:05,  3.91it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:15<00:04,  3.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:15<00:04,  4.05it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:15<00:04,  4.04it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:16<00:03,  4.10it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:03,  4.12it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:16<00:03,  4.03it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:16<00:03,  3.99it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:17<00:03,  3.83it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:17<00:03,  3.56it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:17<00:02,  3.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:17<00:02,  3.67it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:18<00:02,  3.74it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:18<00:01,  3.69it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:18<00:01,  3.63it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:19<00:01,  3.58it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:19<00:01,  3.48it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:19<00:00,  3.60it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:19<00:00,  3.68it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:20<00:00,  3.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2322558164596558, 'eval_runtime': 20.7704, 'eval_samples_per_second': 7.222, 'eval_steps_per_second': 3.611, 'epoch': 1.66, 'num_input_tokens_seen': 1015776}\n",
      " 56%|███████████████████████▎                  | 70/126 [13:45<07:58,  8.54s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:20<00:00,  3.83it/s]\u001b[A\n",
      "{'loss': 1.4461, 'grad_norm': 0.47327929735183716, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 1086880}\n",
      "{'loss': 1.4094, 'grad_norm': 0.4521215856075287, 'learning_rate': 1.4717822421734718e-05, 'epoch': 1.89, 'num_input_tokens_seen': 1158944}\n",
      " 63%|██████████████████████████▋               | 80/126 [15:08<06:48,  8.88s/it][INFO|trainer.py:3788] 2024-07-12 05:25:55,755 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:25:55,755 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:25:55,755 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  6.74it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:14,  4.92it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:17,  4.14it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:18,  3.83it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.70it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:18,  3.60it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:18,  3.57it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:18,  3.51it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:18,  3.44it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.49it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.50it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:17,  3.51it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.53it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:16,  3.60it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:16,  3.61it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:16,  3.51it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:04<00:16,  3.48it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:16,  3.43it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:15,  3.49it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:05<00:15,  3.54it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:15,  3.46it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:15,  3.45it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:06<00:15,  3.30it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:06<00:15,  3.30it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:15,  3.26it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:14,  3.27it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:07<00:14,  3.27it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:13,  3.40it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:14,  3.18it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:14,  3.06it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:13,  3.12it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:12,  3.24it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:12,  3.36it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:10<00:11,  3.39it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:11,  3.42it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:11,  3.39it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:11,  3.36it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:11<00:10,  3.40it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:11<00:10,  3.43it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:11<00:09,  3.50it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:12<00:09,  3.57it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:12<00:09,  3.53it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:12<00:08,  3.59it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:08,  3.56it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:13<00:08,  3.42it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:13<00:08,  3.46it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:13<00:07,  3.44it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:14<00:07,  3.38it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:14<00:07,  3.55it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:14<00:06,  3.63it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:14<00:06,  3.69it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:15<00:05,  3.78it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:15<00:05,  3.90it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:15<00:05,  3.85it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:15<00:05,  3.72it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:16<00:04,  3.87it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:16<00:04,  3.60it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:16<00:04,  3.69it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:04,  3.69it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:17<00:03,  3.58it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:17<00:03,  3.57it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:17<00:03,  3.50it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:18<00:03,  3.40it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:18<00:02,  3.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:18<00:02,  3.56it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:18<00:02,  3.64it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:19<00:01,  3.61it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:19<00:01,  3.52it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:19<00:01,  3.47it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:20<00:01,  3.41it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:20<00:00,  3.43it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:20<00:00,  3.27it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:21<00:00,  3.29it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2288988828659058, 'eval_runtime': 21.7071, 'eval_samples_per_second': 6.91, 'eval_steps_per_second': 3.455, 'epoch': 1.89, 'num_input_tokens_seen': 1158944}\n",
      " 63%|██████████████████████████▋               | 80/126 [15:30<06:48,  8.88s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:21<00:00,  3.34it/s]\u001b[A\n",
      "{'loss': 1.5407, 'grad_norm': 0.5822477340698242, 'learning_rate': 1.196411991551255e-05, 'epoch': 2.01, 'num_input_tokens_seen': 1236192}\n",
      "{'loss': 1.4221, 'grad_norm': 0.47661393880844116, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1304608}\n",
      " 71%|██████████████████████████████            | 90/126 [16:55<05:05,  8.48s/it][INFO|trainer.py:3788] 2024-07-12 05:27:42,364 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:27:42,364 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:27:42,364 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:12,  5.95it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:15,  4.69it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:17,  4.11it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:18,  3.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.70it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:18,  3.62it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:18,  3.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:18,  3.48it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:19,  3.40it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.47it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.48it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:17,  3.48it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:18,  3.20it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:19,  2.98it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:19,  2.93it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:17,  3.24it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:16,  3.44it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:15,  3.57it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:05<00:15,  3.45it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:14,  3.56it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:14,  3.66it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:06<00:13,  3.65it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:13,  3.71it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:13,  3.70it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:12,  3.75it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:07<00:12,  3.83it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:11,  4.02it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:11,  3.98it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:10,  4.01it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:08<00:10,  4.13it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:10,  4.07it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:10,  3.98it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:09<00:09,  4.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:09<00:09,  4.02it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:09,  4.05it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:09,  4.00it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:10<00:08,  4.05it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:10<00:08,  4.09it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:10<00:08,  4.25it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:11<00:07,  4.29it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:11<00:08,  3.94it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:11<00:08,  3.83it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:07,  3.85it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:12<00:07,  3.81it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:12<00:07,  3.92it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:12<00:06,  3.90it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:13<00:06,  3.86it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:13<00:06,  4.00it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:13<00:06,  3.91it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:13<00:05,  3.87it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:14<00:05,  3.77it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:14<00:05,  3.81it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:14<00:05,  3.64it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:14<00:05,  3.51it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:15<00:05,  3.54it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:15<00:04,  3.53it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:15<00:04,  3.53it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:04,  3.55it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:16<00:03,  3.58it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:16<00:03,  3.63it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:16<00:03,  3.64it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:17<00:03,  3.53it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:17<00:02,  3.59it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:17<00:02,  3.66it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:17<00:02,  3.72it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:18<00:01,  3.66it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:18<00:01,  3.56it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:18<00:01,  3.49it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:19<00:01,  3.39it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:19<00:00,  3.41it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:19<00:00,  3.36it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:20<00:00,  3.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2297214269638062, 'eval_runtime': 20.813, 'eval_samples_per_second': 7.207, 'eval_steps_per_second': 3.604, 'epoch': 2.13, 'num_input_tokens_seen': 1304608}\n",
      " 71%|██████████████████████████████            | 90/126 [17:16<05:05,  8.48s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:20<00:00,  3.39it/s]\u001b[A\n",
      "{'loss': 1.3823, 'grad_norm': 0.4784414768218994, 'learning_rate': 7.103328768507039e-06, 'epoch': 2.25, 'num_input_tokens_seen': 1380480}\n",
      "{'loss': 1.4842, 'grad_norm': 0.5355663895606995, 'learning_rate': 5.071687319426946e-06, 'epoch': 2.37, 'num_input_tokens_seen': 1458368}\n",
      " 79%|████████████████████████████████▌        | 100/126 [18:44<03:56,  9.09s/it][INFO|trainer.py:3788] 2024-07-12 05:29:31,463 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:29:31,463 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:29:31,463 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:11,  6.64it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:15,  4.74it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:17,  4.02it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:18,  3.69it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:19,  3.58it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:20,  3.35it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:19,  3.38it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:20,  3.24it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:20,  3.21it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:03<00:19,  3.29it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.33it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:18,  3.38it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.39it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:17,  3.34it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:18,  3.26it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:18,  3.19it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:17,  3.25it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:17,  3.25it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:16,  3.28it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:16,  3.34it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:15,  3.33it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:15,  3.36it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:07<00:15,  3.28it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:15,  3.17it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:15,  3.23it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:15,  3.18it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:14,  3.19it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:14,  3.18it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:09<00:15,  2.89it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:09<00:15,  2.85it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:14,  2.94it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:10<00:14,  2.96it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:10<00:13,  3.04it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:10<00:12,  3.16it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:12,  3.20it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:11<00:11,  3.24it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:11<00:11,  3.22it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:11<00:11,  3.26it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:12<00:10,  3.29it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:12<00:09,  3.41it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:12<00:09,  3.41it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:13<00:09,  3.43it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:13<00:09,  3.37it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:13<00:09,  3.29it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:13<00:08,  3.27it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:14<00:08,  3.38it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:14<00:07,  3.41it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:14<00:07,  3.34it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:15<00:07,  3.42it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:15<00:07,  3.39it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:15<00:06,  3.33it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:16<00:06,  3.30it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:16<00:06,  3.32it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:16<00:06,  3.29it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:16<00:05,  3.27it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:17<00:05,  3.35it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:17<00:05,  3.34it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:17<00:04,  3.39it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:18<00:04,  3.36it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:18<00:03,  3.51it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:18<00:03,  3.63it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:18<00:03,  3.53it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:19<00:03,  3.47it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:19<00:02,  3.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:19<00:02,  3.75it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:19<00:02,  3.83it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:20<00:01,  3.83it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:20<00:01,  3.89it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:20<00:01,  3.89it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:20<00:01,  3.85it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:21<00:00,  3.92it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:21<00:00,  3.82it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:21<00:00,  3.88it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2278231382369995, 'eval_runtime': 22.34, 'eval_samples_per_second': 6.714, 'eval_steps_per_second': 3.357, 'epoch': 2.37, 'num_input_tokens_seen': 1458368}\n",
      " 79%|████████████████████████████████▌        | 100/126 [19:06<03:56,  9.09s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:22<00:00,  3.90it/s]\u001b[A\n",
      "{'loss': 1.5683, 'grad_norm': 0.5707451105117798, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1533696}\n",
      "{'loss': 1.4224, 'grad_norm': 0.4711974859237671, 'learning_rate': 1.9630947032398067e-06, 'epoch': 2.6, 'num_input_tokens_seen': 1607328}\n",
      " 87%|███████████████████████████████████▊     | 110/126 [20:33<02:26,  9.18s/it][INFO|trainer.py:3788] 2024-07-12 05:31:20,648 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:31:20,648 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:31:20,648 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  6.76it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:14,  4.81it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:19,  3.72it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:19,  3.54it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:19,  3.56it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:19,  3.52it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:19,  3.48it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:19,  3.44it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:19,  3.34it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:03<00:18,  3.41it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.38it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:18,  3.40it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:17,  3.41it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:18,  3.18it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:20,  2.94it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:19,  3.00it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:18,  3.15it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:17,  3.23it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:16,  3.33it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:15,  3.45it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:15,  3.43it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:14,  3.47it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:07<00:15,  3.34it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:15,  3.21it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:14,  3.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:13,  3.46it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:13,  3.56it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:12,  3.78it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:11,  3.81it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:11,  3.88it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:10,  4.01it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:10,  4.03it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:10,  4.08it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:09<00:09,  4.14it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:09,  4.08it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:09,  3.99it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:09,  3.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:10<00:08,  4.04it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:11<00:08,  4.04it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:11<00:08,  4.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:11<00:08,  4.12it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:11<00:07,  4.11it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:12<00:07,  4.17it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:08,  3.73it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:12<00:07,  3.65it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:12<00:07,  3.82it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:13<00:06,  3.88it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:13<00:06,  3.79it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:13<00:06,  3.82it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:13<00:06,  3.65it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:14<00:06,  3.49it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:14<00:06,  3.44it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:14<00:06,  3.48it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:15<00:05,  3.44it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:15<00:05,  3.40it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:15<00:05,  3.46it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:16<00:05,  3.39it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:16<00:04,  3.42it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:04,  3.42it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:16<00:04,  3.41it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:17<00:03,  3.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:17<00:03,  3.46it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:17<00:03,  3.39it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:18<00:02,  3.49it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:18<00:02,  3.54it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:18<00:02,  3.66it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:18<00:01,  3.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:19<00:01,  3.78it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:19<00:01,  3.86it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:19<00:01,  3.86it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:19<00:00,  3.81it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:20<00:00,  3.59it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:20<00:00,  3.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.228110909461975, 'eval_runtime': 21.0083, 'eval_samples_per_second': 7.14, 'eval_steps_per_second': 3.57, 'epoch': 2.6, 'num_input_tokens_seen': 1607328}\n",
      " 87%|███████████████████████████████████▊     | 110/126 [20:54<02:26,  9.18s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:20<00:00,  3.83it/s]\u001b[A\n",
      "{'loss': 1.3557, 'grad_norm': 0.514594316482544, 'learning_rate': 9.343938262496993e-07, 'epoch': 2.72, 'num_input_tokens_seen': 1673792}\n",
      "{'loss': 1.3728, 'grad_norm': 0.5942752957344055, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1741760}\n",
      " 95%|███████████████████████████████████████  | 120/126 [22:14<00:49,  8.27s/it][INFO|trainer.py:3788] 2024-07-12 05:33:01,928 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:33:01,928 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:33:01,928 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|█▏                                          | 2/75 [00:00<00:10,  7.13it/s]\u001b[A\n",
      "  4%|█▊                                          | 3/75 [00:00<00:14,  5.02it/s]\u001b[A\n",
      "  5%|██▎                                         | 4/75 [00:00<00:17,  4.10it/s]\u001b[A\n",
      "  7%|██▉                                         | 5/75 [00:01<00:19,  3.68it/s]\u001b[A\n",
      "  8%|███▌                                        | 6/75 [00:01<00:18,  3.65it/s]\u001b[A\n",
      "  9%|████                                        | 7/75 [00:01<00:18,  3.59it/s]\u001b[A\n",
      " 11%|████▋                                       | 8/75 [00:02<00:18,  3.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 9/75 [00:02<00:19,  3.47it/s]\u001b[A\n",
      " 13%|█████▋                                     | 10/75 [00:02<00:19,  3.40it/s]\u001b[A\n",
      " 15%|██████▎                                    | 11/75 [00:02<00:18,  3.45it/s]\u001b[A\n",
      " 16%|██████▉                                    | 12/75 [00:03<00:18,  3.44it/s]\u001b[A\n",
      " 17%|███████▍                                   | 13/75 [00:03<00:18,  3.38it/s]\u001b[A\n",
      " 19%|████████                                   | 14/75 [00:03<00:18,  3.32it/s]\u001b[A\n",
      " 20%|████████▌                                  | 15/75 [00:04<00:18,  3.32it/s]\u001b[A\n",
      " 21%|█████████▏                                 | 16/75 [00:04<00:17,  3.30it/s]\u001b[A\n",
      " 23%|█████████▋                                 | 17/75 [00:04<00:17,  3.30it/s]\u001b[A\n",
      " 24%|██████████▎                                | 18/75 [00:05<00:17,  3.31it/s]\u001b[A\n",
      " 25%|██████████▉                                | 19/75 [00:05<00:18,  2.97it/s]\u001b[A\n",
      " 27%|███████████▍                               | 20/75 [00:05<00:19,  2.88it/s]\u001b[A\n",
      " 28%|████████████                               | 21/75 [00:06<00:17,  3.01it/s]\u001b[A\n",
      " 29%|████████████▌                              | 22/75 [00:06<00:17,  3.09it/s]\u001b[A\n",
      " 31%|█████████████▏                             | 23/75 [00:06<00:16,  3.17it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 24/75 [00:07<00:15,  3.29it/s]\u001b[A\n",
      " 33%|██████████████▎                            | 25/75 [00:07<00:14,  3.45it/s]\u001b[A\n",
      " 35%|██████████████▉                            | 26/75 [00:07<00:13,  3.58it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 27/75 [00:07<00:13,  3.57it/s]\u001b[A\n",
      " 37%|████████████████                           | 28/75 [00:08<00:12,  3.67it/s]\u001b[A\n",
      " 39%|████████████████▋                          | 29/75 [00:08<00:12,  3.79it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 30/75 [00:08<00:13,  3.36it/s]\u001b[A\n",
      " 41%|█████████████████▊                         | 31/75 [00:08<00:12,  3.51it/s]\u001b[A\n",
      " 43%|██████████████████▎                        | 32/75 [00:09<00:11,  3.71it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 33/75 [00:09<00:11,  3.78it/s]\u001b[A\n",
      " 45%|███████████████████▍                       | 34/75 [00:09<00:10,  3.87it/s]\u001b[A\n",
      " 47%|████████████████████                       | 35/75 [00:09<00:10,  3.92it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 36/75 [00:10<00:10,  3.83it/s]\u001b[A\n",
      " 49%|█████████████████████▏                     | 37/75 [00:10<00:09,  3.83it/s]\u001b[A\n",
      " 51%|█████████████████████▊                     | 38/75 [00:10<00:09,  3.75it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 39/75 [00:11<00:09,  3.82it/s]\u001b[A\n",
      " 53%|██████████████████████▉                    | 40/75 [00:11<00:09,  3.85it/s]\u001b[A\n",
      " 55%|███████████████████████▌                   | 41/75 [00:11<00:08,  3.98it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 42/75 [00:11<00:08,  4.06it/s]\u001b[A\n",
      " 57%|████████████████████████▋                  | 43/75 [00:11<00:07,  4.02it/s]\u001b[A\n",
      " 59%|█████████████████████████▏                 | 44/75 [00:12<00:07,  4.08it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:12<00:07,  4.01it/s]\u001b[A\n",
      " 61%|██████████████████████████▎                | 46/75 [00:12<00:07,  3.92it/s]\u001b[A\n",
      " 63%|██████████████████████████▉                | 47/75 [00:12<00:07,  4.00it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 48/75 [00:13<00:06,  3.93it/s]\u001b[A\n",
      " 65%|████████████████████████████               | 49/75 [00:13<00:06,  3.82it/s]\u001b[A\n",
      " 67%|████████████████████████████▋              | 50/75 [00:13<00:06,  3.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:14<00:06,  3.86it/s]\u001b[A\n",
      " 69%|█████████████████████████████▊             | 52/75 [00:14<00:05,  3.87it/s]\u001b[A\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:14<00:05,  3.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 54/75 [00:14<00:05,  3.92it/s]\u001b[A\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:15<00:05,  3.75it/s]\u001b[A\n",
      " 75%|████████████████████████████████           | 56/75 [00:15<00:05,  3.62it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:15<00:04,  3.80it/s]\u001b[A\n",
      " 77%|█████████████████████████████████▎         | 58/75 [00:15<00:04,  3.82it/s]\u001b[A\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:16<00:04,  3.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 60/75 [00:16<00:03,  3.89it/s]\u001b[A\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:16<00:03,  3.89it/s]\u001b[A\n",
      " 83%|███████████████████████████████████▌       | 62/75 [00:16<00:03,  3.99it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 63/75 [00:17<00:03,  3.97it/s]\u001b[A\n",
      " 85%|████████████████████████████████████▋      | 64/75 [00:17<00:02,  3.89it/s]\u001b[A\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:17<00:02,  3.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 66/75 [00:17<00:02,  4.08it/s]\u001b[A\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:18<00:01,  4.12it/s]\u001b[A\n",
      " 91%|██████████████████████████████████████▉    | 68/75 [00:18<00:01,  3.69it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:18<00:01,  3.68it/s]\u001b[A\n",
      " 93%|████████████████████████████████████████▏  | 70/75 [00:18<00:01,  3.72it/s]\u001b[A\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:19<00:01,  3.73it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 72/75 [00:19<00:00,  3.78it/s]\u001b[A\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:19<00:00,  3.69it/s]\u001b[A\n",
      " 99%|██████████████████████████████████████████▍| 74/75 [00:20<00:00,  3.75it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2289167642593384, 'eval_runtime': 20.6314, 'eval_samples_per_second': 7.27, 'eval_steps_per_second': 3.635, 'epoch': 2.84, 'num_input_tokens_seen': 1741760}\n",
      " 95%|███████████████████████████████████████  | 120/126 [22:35<00:49,  8.27s/it]\n",
      "100%|███████████████████████████████████████████| 75/75 [00:20<00:00,  3.75it/s]\u001b[A\n",
      "{'loss': 1.3039, 'grad_norm': 0.5342347025871277, 'learning_rate': 7.770449979593864e-09, 'epoch': 2.96, 'num_input_tokens_seen': 1813344}\n",
      "100%|█████████████████████████████████████████| 126/126 [23:26<00:00,  9.81s/it][INFO|trainer.py:3478] 2024-07-12 05:34:13,107 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/checkpoint-126\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f813ca0d570>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 73578b31-ed45-4e1f-9263-4ba869659c35)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 05:34:23,440 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/checkpoint-126/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 05:34:23,448 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/checkpoint-126/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 05:34:24,407 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1417.4501, 'train_samples_per_second': 2.857, 'train_steps_per_second': 0.089, 'train_loss': 1.4874236725625538, 'epoch': 2.98, 'num_input_tokens_seen': 1828416}\n",
      "100%|█████████████████████████████████████████| 126/126 [23:37<00:00, 11.25s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 05:34:24,416 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f813caaef20>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 9c5e3691-500d-4ea8-8bbf-de4c67b1fb15)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 05:34:34,651 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 05:34:34,659 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9822\n",
      "  num_input_tokens_seen    =    1828416\n",
      "  total_flos               = 13482170GF\n",
      "  train_loss               =     1.4874\n",
      "  train_runtime            = 0:23:37.45\n",
      "  train_samples_per_second =      2.857\n",
      "  train_steps_per_second   =      0.089\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 05:34:35,588 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 05:34:35,588 >>   Num examples = 150\n",
      "[INFO|trainer.py:3793] 2024-07-12 05:34:35,588 >>   Batch size = 2\n",
      " 64%|███████████████████████████▌               | 48/75 [00:14<00:07,  3.42it/s]"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 750 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_max_example=750 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b763f72e-ad45-4916-927f-77ee0f0a28c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af310e2-eef4-401a-b171-c9571fbc81d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
