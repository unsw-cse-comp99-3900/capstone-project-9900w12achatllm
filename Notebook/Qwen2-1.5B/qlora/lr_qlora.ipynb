{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting transformers>=4.41.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/dc/23c26b7b0bce5aaccf2b767db3e9c4f5ae4331bd47688c1f2ef091b23696/transformers-4.42.4-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets>=2.16.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/2d/963b266bb8f88492d5ab4232d74292af8beb5b6fdae97902df9e284d4c32/datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.30.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/74/564f621699b049b0358f7ad83d7437f8219a5d6efb69bbfcca328b60152f/accelerate-0.32.1-py3-none-any.whl (314 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting peft>=0.11.1\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/19/99/c5e0292a6d2a62e95c3dfe674ce9e8f8a9fe5d4835d3c9bb9b3e016f02ae/peft-0.11.1-py3-none-any.whl (251 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting trl>=0.8.6\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Collecting tiktoken\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e7/8c/7d1007557b343d5cf18349802e94d3a14397121e9105b4661f8cd753f9bf/tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.17.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b0/54/eb7fcfc0e1ec6a8404cadd11ac957b3ee4fd0774225cafe3ffe6287861cb/pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (1.5.3)\n",
      "Collecting requests>=2.32.2 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.66.3 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/18/eb/fdb7eb9e48b7b02554e1664afd3bd3f117f6b6d6c5881438a0b055554f9b/tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2023.1.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m402.6/402.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.41.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/40/4f/eb78de4af3b17b589f43a369cbf0c3a7173f25c3d2cd93068852c07689aa/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tyro>=0.5.11 (from trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f7/ea/c8967a64769ec465a2d49bf81e1e135999741704a36993b6b51465ce8503/tyro-0.8.5-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub (from accelerate>=0.30.1)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/78/71/6ce4136149cb42b98599d49c39b3a39dd6858b5f9307490998c40e26a51e/huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.5.0,>=2023.1.0 (from datasets>=2.16.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/a3/16e9fe32187e9c8bc7f9b7bcd9728529faa725231a0c96f2f98714ff2fc5/fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e2/d1/a1d3189e7873408b9dc396aef0d7926c198b0df2aa3ddb5b539d3e89a70f/shtab-1.7.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=6afc705e65612b99819f6692a5cced186c4813d9ab412e4df1b773bed2cbd050\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_73rgnkc/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: tqdm, shtab, requests, pyarrow, fsspec, docstring-parser, tiktoken, huggingface-hub, tyro, tokenizers, transformers, datasets, accelerate, trl, peft, llamafactory\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.1\n",
      "    Uninstalling pyarrow-14.0.1:\n",
      "      Successfully uninstalled pyarrow-14.0.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.1.0\n",
      "    Uninstalling fsspec-2023.1.0:\n",
      "      Successfully uninstalled fsspec-2023.1.0\n",
      "  Attempting uninstall: docstring-parser\n",
      "    Found existing installation: docstring-parser 0.15\n",
      "    Uninstalling docstring-parser-0.15:\n",
      "      Successfully uninstalled docstring-parser-0.15\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.0\n",
      "    Uninstalling tokenizers-0.15.0:\n",
      "      Successfully uninstalled tokenizers-0.15.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.35.2\n",
      "    Uninstalling transformers-4.35.2:\n",
      "      Successfully uninstalled transformers-4.35.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.7\n",
      "    Uninstalling datasets-2.14.7:\n",
      "      Successfully uninstalled datasets-2.14.7\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.24.1\n",
      "    Uninstalling accelerate-0.24.1:\n",
      "      Successfully uninstalled accelerate-0.24.1\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchx 0.6.0 requires fsspec==2023.1.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed accelerate-0.32.1 datasets-2.20.0 docstring-parser-0.16 fsspec-2024.5.0 huggingface-hub-0.23.4 llamafactory-0.8.2.dev0 peft-0.11.1 pyarrow-16.1.0 requests-2.32.3 shtab-1.7.1 tiktoken-0.7.0 tokenizers-0.19.1 tqdm-4.66.4 transformers-4.42.4 trl-0.9.6 tyro-0.8.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (1.5.3)\n",
      "Collecting pandas\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/89/1b/12521efcbc6058e2673583bb096c2b5046a9df39bd73eca392c1efed24e5/pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cuda 23.10.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pandas-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting optimum\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/e4/f832e42a1eb9d5ac4fa6379295e05aebeae507d171babc1786bfa0210299/optimum-1.21.2-py3-none-any.whl (424 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "Installing collected packages: optimum\n",
      "Successfully installed optimum-1.21.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Found existing installation: apex 0.9.10.dev0\n",
      "Uninstalling apex-0.9.10.dev0:\n",
      "  Successfully uninstalled apex-0.9.10.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018bd83f-942a-4b63-9e4e-e08578379499",
   "metadata": {},
   "source": [
    "lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 10:22:24,852] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 10:22:30 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 10:22:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:22:30,169 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 10:22:30,704 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 10:22:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 10:22:30 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "Generating train split: 10178 examples [00:00, 101175.60 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1274.8\n",
      "07/12/2024 10:22:37 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "Generating train split: 900 examples [00:00, 17079.48 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1272.9\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:02<00:00, 363.\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 10:22:43,480 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 10:22:43,482 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 10:22:43 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 10:22:43,528 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 10:22:51,797 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 10:22:51,800 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 10:25:18,335 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 10:25:18,336 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 10:25:18,351 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 10:25:18,351 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 10:25:18 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 10:25:18 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 10:25:18 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 10:25:18 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 10:25:18 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,down_proj,o_proj,v_proj,gate_proj,up_proj\n",
      "07/12/2024 10:25:29 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 10:25:29,590 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 10:25:30,039 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 10:25:30,039 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-12 10:25:30,039 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 10:25:30,039 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 10:25:30,039 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 10:25:30,039 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 10:25:30,039 >>   Total optimization steps = 84\n",
      "[INFO|trainer.py:2137] 2024-07-12 10:25:30,044 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 2.0385, 'grad_norm': 1.2942103147506714, 'learning_rate': 9.912832366166443e-06, 'epoch': 0.18, 'num_input_tokens_seen': 72832}\n",
      "{'loss': 2.0292, 'grad_norm': 1.27815580368042, 'learning_rate': 9.654368743221022e-06, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [01:57<12:03,  9.78s/it][INFO|trainer.py:3788] 2024-07-12 10:27:27,425 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:27:27,425 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:27:27,425 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:06,  6.95it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.68it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  3.86it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.60it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.39it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.29it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.50it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:11,  3.43it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.22it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.19it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.25it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.24it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:10,  3.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:09,  3.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.36it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.25it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:09,  3.20it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  3.00it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:09,  3.00it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:09,  2.95it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:08,  3.10it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:07,  3.24it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:07,  3.32it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:07,  3.20it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:07,  2.92it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:07,  2.84it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:06,  2.87it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:06,  2.74it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:06,  2.88it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:05,  2.85it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:05,  2.93it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:05,  2.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:04,  2.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:04,  2.98it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:04,  2.96it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:12<00:03,  2.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:03,  2.92it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:13<00:03,  2.82it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:13<00:02,  3.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:13<00:02,  3.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  3.54it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:14<00:01,  3.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:14<00:01,  3.82it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:14<00:00,  3.81it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.79it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:15<00:00,  3.92it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7345913648605347, 'eval_runtime': 15.5844, 'eval_samples_per_second': 6.417, 'eval_steps_per_second': 3.208, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:12<12:03,  9.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  3.89it/s]\u001b[A\n",
      "{'loss': 1.7918, 'grad_norm': 1.263257622718811, 'learning_rate': 9.233620996141421e-06, 'epoch': 0.53, 'num_input_tokens_seen': 215776}\n",
      "{'loss': 1.7885, 'grad_norm': 1.4316221475601196, 'learning_rate': 8.665259359149132e-06, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [03:36<09:17,  8.71s/it][INFO|trainer.py:3788] 2024-07-12 10:29:06,364 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:29:06,364 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:29:06,364 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.43it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.24it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.41it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.16it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.03it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.94it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.99it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.00it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.01it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.99it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:13,  2.80it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:12,  2.86it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:12,  2.91it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  2.98it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.04it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:10,  3.16it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:10,  3.11it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  3.03it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:10,  2.96it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  2.91it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:09,  2.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:09,  2.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:08,  3.04it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  3.11it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:07,  3.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:07,  3.23it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:06,  3.26it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:06,  3.32it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:06,  3.27it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:05,  3.24it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:05,  3.24it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:05,  3.01it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:05,  2.88it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:11<00:05,  2.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:04,  2.93it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:12<00:04,  2.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:12<00:04,  2.94it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:12<00:03,  2.86it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:13<00:03,  2.88it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:13<00:03,  2.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:13<00:02,  2.87it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:14<00:02,  2.84it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:14<00:02,  2.89it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:14<00:01,  2.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:15<00:01,  3.07it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:15<00:00,  3.00it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:15<00:00,  2.84it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:16<00:00,  2.83it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6348867416381836, 'eval_runtime': 16.834, 'eval_samples_per_second': 5.94, 'eval_steps_per_second': 2.97, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [03:53<09:17,  8.71s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:16<00:00,  2.89it/s]\u001b[A\n",
      "{'loss': 1.7064, 'grad_norm': 1.1335104703903198, 'learning_rate': 7.969100927867508e-06, 'epoch': 0.89, 'num_input_tokens_seen': 357632}\n",
      "{'loss': 1.8144, 'grad_norm': 0.9385630488395691, 'learning_rate': 7.169418695587791e-06, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [05:23<08:47,  9.77s/it][INFO|trainer.py:3788] 2024-07-12 10:30:54,001 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:30:54,002 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:30:54,002 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.64it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.53it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.26it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.01it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  2.98it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  3.04it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.07it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:14,  2.90it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  2.97it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.83it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:14,  2.71it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.64it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.57it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.55it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:12,  2.56it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.53it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:12,  2.53it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.70it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:10,  2.89it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:08,  3.11it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:08,  3.32it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:07,  3.49it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:06,  3.67it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:06,  3.84it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:05,  3.85it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:05,  3.85it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:05,  3.89it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:05,  3.92it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  3.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:04,  4.03it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:04,  4.08it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:03,  4.12it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.20it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:03,  4.16it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  4.16it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:02,  4.06it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  4.09it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:02,  4.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  4.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:01,  4.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.13it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  4.11it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  4.12it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:00,  4.15it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  4.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  4.04it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5655676126480103, 'eval_runtime': 14.868, 'eval_samples_per_second': 6.726, 'eval_steps_per_second': 3.363, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [05:38<08:47,  9.77s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.98it/s]\u001b[A\n",
      "{'loss': 1.7617, 'grad_norm': 0.9539620876312256, 'learning_rate': 6.294095225512604e-06, 'epoch': 1.24, 'num_input_tokens_seen': 507104}\n",
      "{'loss': 1.7058, 'grad_norm': 0.8379418849945068, 'learning_rate': 5.373650467932122e-06, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:07<06:51,  9.35s/it][INFO|trainer.py:3788] 2024-07-12 10:32:37,243 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:32:37,244 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:32:37,244 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.62it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.70it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  4.15it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.95it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:11,  3.76it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:11,  3.66it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:11,  3.61it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.60it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.58it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:11,  3.48it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.39it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.39it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:10,  3.38it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.42it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.41it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:09,  3.45it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:09,  3.41it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.35it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:09,  3.21it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:09,  3.03it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:09,  2.95it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:09,  2.93it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:08,  3.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:08,  2.89it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:08,  2.81it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:08,  2.73it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:08,  2.64it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:07,  2.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:06,  2.89it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:06,  3.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:05,  3.19it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:05,  3.26it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:04,  3.31it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:04,  3.43it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:04,  3.39it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  3.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:03,  3.40it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:03,  3.43it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:02,  3.43it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  3.34it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  3.38it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.41it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  3.43it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:14<00:00,  3.38it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  3.30it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5192428827285767, 'eval_runtime': 15.3937, 'eval_samples_per_second': 6.496, 'eval_steps_per_second': 3.248, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:22<06:51,  9.35s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  3.30it/s]\u001b[A\n",
      "{'loss': 1.6441, 'grad_norm': 0.6978831887245178, 'learning_rate': 4.4401776194834615e-06, 'epoch': 1.6, 'num_input_tokens_seen': 651104}\n",
      "{'loss': 1.6659, 'grad_norm': 0.7392528653144836, 'learning_rate': 3.526224127945479e-06, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [08:46<05:10,  9.13s/it][INFO|trainer.py:3788] 2024-07-12 10:34:16,944 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:34:16,945 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:34:16,945 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.64it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.67it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  4.11it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.85it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.66it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:12,  3.56it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.49it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.51it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.52it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:11,  3.46it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.38it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:10,  3.41it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.42it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.42it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:09,  3.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.63it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.71it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  3.88it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  3.89it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:07,  3.94it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.99it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  4.07it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:05,  4.08it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.05it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.02it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  4.05it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:05,  3.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  3.91it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.96it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:04,  4.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.11it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.06it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.01it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:09<00:03,  3.99it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.01it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.03it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:10<00:02,  4.00it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.00it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  3.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  3.99it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:11<00:01,  3.99it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  3.92it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  3.96it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4913547039031982, 'eval_runtime': 13.2551, 'eval_samples_per_second': 7.544, 'eval_steps_per_second': 3.772, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [09:00<05:10,  9.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  3.91it/s]\u001b[A\n",
      "{'loss': 1.6712, 'grad_norm': 0.7962809801101685, 'learning_rate': 2.66365685863469e-06, 'epoch': 1.96, 'num_input_tokens_seen': 793792}\n",
      "{'loss': 1.678, 'grad_norm': 0.7866626381874084, 'learning_rate': 1.8825509907063328e-06, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [10:28<03:38,  9.12s/it][INFO|trainer.py:3788] 2024-07-12 10:35:58,246 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:35:58,246 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:35:58,246 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.35it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.56it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  4.02it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.35it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.22it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.12it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.10it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:13,  3.07it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.06it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.09it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:12,  3.05it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.07it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:10,  3.12it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:10,  3.22it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.27it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.30it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.53it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.63it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.67it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.87it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  3.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.06it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.10it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.06it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  3.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.06it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.08it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.11it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.22it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.20it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.17it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.15it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.12it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.19it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.14it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.17it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.16it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.16it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.15it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.05it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4773414134979248, 'eval_runtime': 13.621, 'eval_samples_per_second': 7.342, 'eval_steps_per_second': 3.671, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [10:41<03:38,  9.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.04it/s]\u001b[A\n",
      "{'loss': 1.6102, 'grad_norm': 0.7109265923500061, 'learning_rate': 1.2101413842727345e-06, 'epoch': 2.31, 'num_input_tokens_seen': 938976}\n",
      "{'loss': 1.7465, 'grad_norm': 0.597107470035553, 'learning_rate': 6.698729810778065e-07, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:12<02:12,  9.47s/it][INFO|trainer.py:3788] 2024-07-12 10:37:42,174 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:37:42,174 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:37:42,174 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.57it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.75it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.26it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.19it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  2.97it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.90it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.04it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.13it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:12,  3.19it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.98it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:13,  2.82it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.81it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.50it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:08,  3.73it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.79it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:07,  3.89it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  4.05it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  4.01it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  3.97it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  4.06it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.15it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.20it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.18it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.14it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:04,  4.21it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.18it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.14it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.26it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:03,  4.28it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.26it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.27it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.24it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.22it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.21it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.24it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.25it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.26it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.17it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.24it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.26it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.29it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.28it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.19it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4716659784317017, 'eval_runtime': 13.4183, 'eval_samples_per_second': 7.453, 'eval_steps_per_second': 3.726, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:25<02:12,  9.47s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.10it/s]\u001b[A\n",
      "{'loss': 1.6837, 'grad_norm': 0.7250687479972839, 'learning_rate': 2.8058334845816214e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1088704}\n",
      "{'loss': 1.6287, 'grad_norm': 0.7341209650039673, 'learning_rate': 5.584586887435739e-08, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [13:52<00:34,  8.63s/it][INFO|trainer.py:3788] 2024-07-12 10:39:22,119 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:39:22,119 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:39:22,119 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.37it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.11it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:12,  3.58it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.34it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.23it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.18it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.16it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:12,  3.13it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.17it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.17it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.36it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:10,  3.55it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:09,  3.66it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:08,  3.79it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  3.95it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.90it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:07,  3.90it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  4.01it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  3.96it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.92it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.94it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  4.01it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.03it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.04it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  3.97it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  3.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.96it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  3.82it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.88it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.86it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  3.92it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.09it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.03it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.06it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.10it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.05it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.97it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.04it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.08it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.14it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.17it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.07it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.03it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4666991233825684, 'eval_runtime': 13.3691, 'eval_samples_per_second': 7.48, 'eval_steps_per_second': 3.74, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [14:05<00:34,  8.63s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.98it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 84/84 [14:40<00:00, 10.52s/it][INFO|trainer.py:3478] 2024-07-12 10:40:10,639 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/checkpoint-84\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a7b92a050>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 31ec8fe4-320f-4c8a-9286-50addde52302)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 10:40:21,174 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/checkpoint-84/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 10:40:21,184 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/checkpoint-84/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 10:40:22,493 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 892.4491, 'train_samples_per_second': 3.025, 'train_steps_per_second': 0.094, 'train_loss': 1.748181592850458, 'epoch': 2.99, 'num_input_tokens_seen': 1218528}\n",
      "100%|███████████████████████████████████████████| 84/84 [14:52<00:00, 10.62s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 10:40:22,504 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a7b96fa90>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f00c47d7-edf3-4714-8ead-7b60415261e7)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 10:40:32,840 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 10:40:32,850 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =    1218528\n",
      "  total_flos               =  8985046GF\n",
      "  train_loss               =     1.7482\n",
      "  train_runtime            = 0:14:52.44\n",
      "  train_samples_per_second =      3.025\n",
      "  train_steps_per_second   =      0.094\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 10:40:33,713 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:40:33,713 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:40:33,713 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.75it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.4704\n",
      "  eval_runtime            = 0:00:13.58\n",
      "  eval_samples_per_second =      7.361\n",
      "  eval_steps_per_second   =       3.68\n",
      "  num_input_tokens_seen   =    1218528\n",
      "[INFO|modelcard.py:449] 2024-07-12 10:40:47,326 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 10:41:07,081] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 10:41:11 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 10:41:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:41:11,604 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 10:41:12,183 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 10:41:12 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 10:41:12 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/12/2024 10:41:17 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 10:41:18,750 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 10:41:18,751 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 10:41:18 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 10:41:18,807 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 10:41:21,025 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 10:41:21,030 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 10:43:37,689 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 10:43:37,689 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 10:43:37,697 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 10:43:37,698 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 10:43:38 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 10:43:38 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 10:43:38 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 10:43:38 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 10:43:38 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,k_proj,q_proj,gate_proj,down_proj,up_proj,o_proj\n",
      "07/12/2024 10:43:51 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 10:43:51,215 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 10:43:51,684 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 10:43:51,684 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-12 10:43:51,684 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 10:43:51,684 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 10:43:51,684 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 10:43:51,684 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 10:43:51,684 >>   Total optimization steps = 84\n",
      "[INFO|trainer.py:2137] 2024-07-12 10:43:51,688 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.8817, 'grad_norm': 0.737947404384613, 'learning_rate': 9.912832366166442e-05, 'epoch': 0.18, 'num_input_tokens_seen': 72832}\n",
      "{'loss': 1.6705, 'grad_norm': 0.5499114394187927, 'learning_rate': 9.654368743221022e-05, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:22<12:25, 10.07s/it][INFO|trainer.py:3788] 2024-07-12 10:46:14,471 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:46:14,472 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:46:14,472 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.23it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.42it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:12,  3.66it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.31it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.12it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:13,  3.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.27it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.35it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:12,  3.20it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.19it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.12it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:12,  2.99it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:12,  3.00it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  2.95it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:11,  3.09it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.37it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.54it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.68it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  3.85it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.85it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.87it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  3.90it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  3.92it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:06,  3.80it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  3.80it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.93it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.93it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  3.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.04it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.03it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.02it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.02it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  4.00it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.05it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.07it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.06it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.03it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.09it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.10it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.98it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  3.95it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3484864234924316, 'eval_runtime': 13.7118, 'eval_samples_per_second': 7.293, 'eval_steps_per_second': 3.647, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:36<12:25, 10.07s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.95it/s]\u001b[A\n",
      "{'loss': 1.4014, 'grad_norm': 0.45452433824539185, 'learning_rate': 9.233620996141421e-05, 'epoch': 0.53, 'num_input_tokens_seen': 215776}\n",
      "{'loss': 1.419, 'grad_norm': 0.5751827359199524, 'learning_rate': 8.665259359149132e-05, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [04:00<09:21,  8.78s/it][INFO|trainer.py:3788] 2024-07-12 10:47:52,028 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:47:52,029 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:47:52,029 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.12it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.58it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.31it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.22it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.13it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  3.03it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.97it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.01it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.04it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  2.98it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.82it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.70it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.64it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.61it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.80it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:11,  2.86it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  2.88it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:10,  2.94it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:10,  2.76it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.78it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:09,  2.93it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:08,  3.19it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:07,  3.43it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:06,  3.61it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:06,  3.73it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:05,  3.78it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:05,  3.87it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:05,  3.88it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  3.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:04,  3.99it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:04,  4.00it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:03,  4.03it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.10it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  3.99it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  3.88it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:03,  3.94it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  3.99it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.04it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  4.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:01,  4.04it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.04it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.03it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  4.04it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:00,  4.03it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  3.96it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.96it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.99it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2825016975402832, 'eval_runtime': 14.8924, 'eval_samples_per_second': 6.715, 'eval_steps_per_second': 3.357, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [04:15<09:21,  8.78s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.90it/s]\u001b[A\n",
      "{'loss': 1.3818, 'grad_norm': 0.4684315621852875, 'learning_rate': 7.969100927867507e-05, 'epoch': 0.89, 'num_input_tokens_seen': 357632}\n",
      "{'loss': 1.4949, 'grad_norm': 0.4488191604614258, 'learning_rate': 7.169418695587791e-05, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [05:46<08:49,  9.81s/it][INFO|trainer.py:3788] 2024-07-12 10:49:37,956 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:49:37,956 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:49:37,956 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.36it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  3.93it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.28it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  2.99it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  3.03it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.05it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.05it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.05it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.14it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.10it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:12,  2.92it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:12,  2.89it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  2.99it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.05it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:10,  3.15it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:10,  3.19it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  3.08it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:09,  3.17it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  3.21it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:08,  3.29it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:08,  3.24it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:08,  3.25it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:07,  3.49it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:06,  3.69it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:06,  3.76it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  3.82it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.99it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:04,  4.01it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  3.99it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.13it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.14it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.27it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  4.19it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.12it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.14it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  4.15it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.17it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.18it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.13it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.15it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.15it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.14it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  4.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  4.06it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.05it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2751853466033936, 'eval_runtime': 14.2634, 'eval_samples_per_second': 7.011, 'eval_steps_per_second': 3.505, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [06:00<08:49,  9.81s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.95it/s]\u001b[A\n",
      "{'loss': 1.4426, 'grad_norm': 0.48498907685279846, 'learning_rate': 6.294095225512603e-05, 'epoch': 1.24, 'num_input_tokens_seen': 507104}\n",
      "{'loss': 1.4116, 'grad_norm': 0.4947367012500763, 'learning_rate': 5.373650467932122e-05, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:29<06:53,  9.39s/it][INFO|trainer.py:3788] 2024-07-12 10:51:21,147 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:51:21,147 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:51:21,147 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.65it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  4.01it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.87it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:11,  3.72it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:11,  3.60it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.30it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.23it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:12,  3.22it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.20it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.17it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.22it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.11it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  2.97it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:11,  2.89it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.90it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:10,  2.94it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:10,  2.91it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:10,  2.93it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  3.18it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:08,  3.42it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.62it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  3.80it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  3.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.09it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.12it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.11it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.18it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.13it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.03it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.14it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.14it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.14it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.21it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.15it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.13it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.06it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.10it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2711620330810547, 'eval_runtime': 13.7475, 'eval_samples_per_second': 7.274, 'eval_steps_per_second': 3.637, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:43<06:53,  9.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.02it/s]\u001b[A\n",
      "{'loss': 1.391, 'grad_norm': 0.49095380306243896, 'learning_rate': 4.4401776194834613e-05, 'epoch': 1.6, 'num_input_tokens_seen': 651104}\n",
      "{'loss': 1.4069, 'grad_norm': 0.44545283913612366, 'learning_rate': 3.5262241279454785e-05, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [09:07<05:08,  9.07s/it][INFO|trainer.py:3788] 2024-07-12 10:52:59,486 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:52:59,486 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:52:59,486 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.75it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:09,  4.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:10,  4.19it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.91it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:11,  3.71it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:11,  3.66it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:11,  3.62it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.68it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.88it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:10,  3.83it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:09,  3.84it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:09,  3.96it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.90it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:09,  3.77it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  3.70it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.56it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:08,  3.53it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.58it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:08,  3.44it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:08,  3.39it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:07,  3.43it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:07,  3.45it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:06,  3.63it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.75it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  3.79it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  3.89it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.76it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.60it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.62it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.56it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  3.52it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:04,  3.55it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:04,  3.46it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  3.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.32it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:03,  3.20it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:03,  3.06it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  3.01it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  2.81it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:02,  2.89it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:02,  2.90it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  2.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  3.08it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:01,  2.89it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  2.89it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.267992615699768, 'eval_runtime': 14.6776, 'eval_samples_per_second': 6.813, 'eval_steps_per_second': 3.407, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [09:22<05:08,  9.07s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.31it/s]\u001b[A\n",
      "{'loss': 1.4255, 'grad_norm': 0.4807671308517456, 'learning_rate': 2.66365685863469e-05, 'epoch': 1.96, 'num_input_tokens_seen': 793792}\n",
      "{'loss': 1.4123, 'grad_norm': 0.4300272762775421, 'learning_rate': 1.8825509907063327e-05, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [10:50<03:37,  9.05s/it][INFO|trainer.py:3788] 2024-07-12 10:54:42,254 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:54:42,254 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:54:42,254 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.57it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.58it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  4.05it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.81it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.45it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.16it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.38it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.58it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.71it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:10,  3.77it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.74it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:09,  3.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.85it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:08,  3.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:08,  3.92it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  4.05it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:07,  4.06it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:07,  4.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  4.13it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  4.04it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:06,  4.00it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:06,  3.97it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  4.00it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  4.01it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:05,  4.07it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:06<00:05,  4.06it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.01it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  4.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:04,  4.00it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:07<00:04,  3.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.08it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  4.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:03,  4.11it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:08<00:03,  4.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.05it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:09<00:02,  4.01it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:09<00:02,  4.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.05it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:10<00:01,  4.11it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:10<00:01,  4.11it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.08it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.11it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:11<00:00,  4.13it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:11<00:00,  4.03it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  3.99it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.09it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2671838998794556, 'eval_runtime': 12.8895, 'eval_samples_per_second': 7.758, 'eval_steps_per_second': 3.879, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [11:03<03:37,  9.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  3.95it/s]\u001b[A\n",
      "{'loss': 1.3594, 'grad_norm': 0.5333118438720703, 'learning_rate': 1.2101413842727345e-05, 'epoch': 2.31, 'num_input_tokens_seen': 938976}\n",
      "{'loss': 1.4871, 'grad_norm': 0.49310237169265747, 'learning_rate': 6.698729810778065e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:33<02:11,  9.42s/it][INFO|trainer.py:3788] 2024-07-12 10:56:25,244 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:56:25,244 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:56:25,244 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.60it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  3.98it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.43it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.23it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.91it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.96it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.11it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:12,  3.20it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:12,  3.25it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:11,  3.27it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:11,  3.33it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:10,  3.37it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.43it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.44it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.52it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.50it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.51it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.68it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.79it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.89it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  3.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  4.03it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.13it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.18it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.11it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  3.99it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.11it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.13it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.09it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.16it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.14it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.10it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.17it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  4.19it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.15it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.09it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.16it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.16it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2675180435180664, 'eval_runtime': 13.5788, 'eval_samples_per_second': 7.364, 'eval_steps_per_second': 3.682, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:47<02:11,  9.42s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.03it/s]\u001b[A\n",
      "{'loss': 1.42, 'grad_norm': 0.5123867988586426, 'learning_rate': 2.8058334845816213e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1088704}\n",
      "{'loss': 1.3649, 'grad_norm': 0.5192175507545471, 'learning_rate': 5.584586887435739e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [14:16<00:36,  9.19s/it][INFO|trainer.py:3788] 2024-07-12 10:58:08,488 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:58:08,488 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:58:08,488 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.97it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.92it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.79it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.85it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:14,  2.74it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.75it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.69it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:13,  2.72it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.73it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.69it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:12,  2.77it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.06it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.53it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:08,  3.70it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  3.91it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.94it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.95it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:06,  3.98it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  4.07it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.14it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.16it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:05,  4.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.13it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.17it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.12it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  4.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.21it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.07it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  4.15it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.14it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.10it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  4.13it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.09it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.14it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.18it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.17it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.92it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.01it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2672697305679321, 'eval_runtime': 14.1463, 'eval_samples_per_second': 7.069, 'eval_steps_per_second': 3.534, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [14:30<00:36,  9.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.99it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 84/84 [15:06<00:00, 10.85s/it][INFO|trainer.py:3478] 2024-07-12 10:58:58,527 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/checkpoint-84\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f131c52faf0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: f6e38e5e-12e8-44c4-85a9-da0a8a562a44)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 10:59:09,045 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/checkpoint-84/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 10:59:09,056 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/checkpoint-84/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 10:59:10,225 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 918.5369, 'train_samples_per_second': 2.939, 'train_steps_per_second': 0.091, 'train_loss': 1.4622974622817266, 'epoch': 2.99, 'num_input_tokens_seen': 1218528}\n",
      "100%|███████████████████████████████████████████| 84/84 [15:18<00:00, 10.94s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 10:59:10,236 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f131c39e680>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 3ee2a583-6047-4263-8d40-013c3dc02f1c)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 10:59:20,777 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 10:59:20,788 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =    1218528\n",
      "  total_flos               =  8985046GF\n",
      "  train_loss               =     1.4623\n",
      "  train_runtime            = 0:15:18.53\n",
      "  train_samples_per_second =      2.939\n",
      "  train_steps_per_second   =      0.091\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 10:59:21,560 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 10:59:21,560 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 10:59:21,560 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  3.99it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.2674\n",
      "  eval_runtime            = 0:00:12.90\n",
      "  eval_samples_per_second =       7.75\n",
      "  eval_steps_per_second   =      3.875\n",
      "  num_input_tokens_seen   =    1218528\n",
      "[INFO|modelcard.py:449] 2024-07-12 10:59:34,484 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_lr=1e-4 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "lr=5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e72b73d-6bb7-4671-8ebc-5efb909cfafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-12 10:59:50,835] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/12/2024 10:59:56 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/12/2024 10:59:56 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-12 10:59:56,208 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-12 10:59:56,843 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/12/2024 10:59:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/12/2024 10:59:56 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/12/2024 11:00:02 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-12 11:00:03,482 >> loading configuration file model/Qwen2-1.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-12 11:00:03,483 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-1.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/12/2024 11:00:03 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-12 11:00:03,536 >> loading weights file model/Qwen2-1.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-12 11:00:04,792 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 11:00:04,795 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-12 11:03:01,547 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-12 11:03:01,547 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-1.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-12 11:03:01,572 >> loading configuration file model/Qwen2-1.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-12 11:03:01,572 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/12/2024 11:03:02 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/12/2024 11:03:02 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/12/2024 11:03:02 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/12/2024 11:03:02 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/12/2024 11:03:02 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,down_proj,v_proj,gate_proj,q_proj,up_proj,k_proj\n",
      "07/12/2024 11:03:13 - INFO - llamafactory.model.loader - trainable params: 9232384 || all params: 1552946688 || trainable%: 0.5945\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-12 11:03:13,205 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-12 11:03:13,738 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-12 11:03:13,739 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-12 11:03:13,739 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-12 11:03:13,739 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2134] 2024-07-12 11:03:13,739 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:2135] 2024-07-12 11:03:13,739 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-12 11:03:13,739 >>   Total optimization steps = 84\n",
      "[INFO|trainer.py:2137] 2024-07-12 11:03:13,744 >>   Number of trainable parameters = 9,232,384\n",
      "{'loss': 1.7203, 'grad_norm': 0.49934545159339905, 'learning_rate': 0.0004956416183083221, 'epoch': 0.18, 'num_input_tokens_seen': 72832}\n",
      "{'loss': 1.5476, 'grad_norm': 0.4832383394241333, 'learning_rate': 0.00048271843716105105, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:45<12:52, 10.44s/it][INFO|trainer.py:3788] 2024-07-12 11:05:59,527 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:05:59,528 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:05:59,528 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.90it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.11it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:12,  3.66it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.51it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:12,  3.32it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.18it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.11it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.46it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:10,  3.66it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.70it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:09,  3.88it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  4.00it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:08,  4.12it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:08,  4.17it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:07,  4.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:07,  4.25it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:07,  4.26it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:06,  4.35it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:06,  4.24it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:06,  4.23it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:06,  4.21it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  4.24it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:05,  4.33it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:05,  4.35it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:06<00:05,  4.30it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.24it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:04,  4.28it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:05,  3.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:07<00:05,  3.44it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:05,  3.42it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.48it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:04,  3.70it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:08<00:03,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  3.63it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  3.68it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:09<00:03,  3.78it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  3.91it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.00it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  4.08it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:10<00:01,  4.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:10<00:01,  4.16it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.18it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.22it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:11<00:00,  4.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:11<00:00,  4.14it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2880738973617554, 'eval_runtime': 13.0464, 'eval_samples_per_second': 7.665, 'eval_steps_per_second': 3.832, 'epoch': 0.36, 'num_input_tokens_seen': 147776}\n",
      " 12%|█████                                      | 10/84 [02:58<12:52, 10.44s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:12<00:00,  4.08it/s]\u001b[A\n",
      "{'loss': 1.3556, 'grad_norm': 0.43673232197761536, 'learning_rate': 0.00046168104980707104, 'epoch': 0.53, 'num_input_tokens_seen': 215776}\n",
      "{'loss': 1.4021, 'grad_norm': 0.3550977110862732, 'learning_rate': 0.00043326296795745655, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [04:24<09:37,  9.03s/it][INFO|trainer.py:3788] 2024-07-12 11:07:38,569 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:07:38,569 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:07:38,569 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.63it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.15it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.38it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.05it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  2.97it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.95it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.05it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  2.96it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:12,  3.04it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:11,  3.30it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:10,  3.47it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:09,  3.63it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.67it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.53it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:09,  3.26it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:09,  3.29it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:08,  3.40it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:08,  3.57it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.71it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.82it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  3.95it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.06it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.12it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  4.08it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.20it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.14it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.09it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.23it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.25it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.30it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.24it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.17it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.19it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.21it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.26it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.30it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.22it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.21it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.17it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:11<00:01,  4.20it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.21it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.10it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.10it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2649255990982056, 'eval_runtime': 13.6022, 'eval_samples_per_second': 7.352, 'eval_steps_per_second': 3.676, 'epoch': 0.71, 'num_input_tokens_seen': 283296}\n",
      " 24%|██████████▏                                | 20/84 [04:38<09:37,  9.03s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.01it/s]\u001b[A\n",
      "{'loss': 1.3711, 'grad_norm': 0.42169061303138733, 'learning_rate': 0.0003984550463933754, 'epoch': 0.89, 'num_input_tokens_seen': 357632}\n",
      "{'loss': 1.4449, 'grad_norm': 0.3581252694129944, 'learning_rate': 0.00035847093477938953, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [06:05<08:02,  8.94s/it][INFO|trainer.py:3788] 2024-07-12 11:09:19,016 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:09:19,016 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:09:19,016 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.11it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.76it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.19it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.96it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.77it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:14,  2.82it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.85it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.73it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:13,  2.73it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.79it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:11,  3.03it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:10,  3.32it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:09,  3.54it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:08,  3.75it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.82it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:07,  3.95it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  4.14it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  4.12it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:06,  4.14it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:06,  4.12it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  4.19it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:05,  4.24it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.28it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:05,  4.24it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.15it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:04,  4.23it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.20it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:04,  4.16it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  4.28it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.24it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.26it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.16it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  4.16it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.15it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.12it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.19it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  4.20it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.25it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.24it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2673029899597168, 'eval_runtime': 13.6775, 'eval_samples_per_second': 7.311, 'eval_steps_per_second': 3.656, 'epoch': 1.07, 'num_input_tokens_seen': 434848}\n",
      " 36%|███████████████▎                           | 30/84 [06:18<08:02,  8.94s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.09it/s]\u001b[A\n",
      "{'loss': 1.3237, 'grad_norm': 0.38174307346343994, 'learning_rate': 0.00031470476127563017, 'epoch': 1.24, 'num_input_tokens_seen': 507104}\n",
      "{'loss': 1.2588, 'grad_norm': 0.4517973065376282, 'learning_rate': 0.00026868252339660607, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [07:45<06:26,  8.79s/it][INFO|trainer.py:3788] 2024-07-12 11:10:59,646 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:10:59,647 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:10:59,647 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.80it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.19it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.84it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:16,  2.70it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.68it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.64it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.54it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.48it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.46it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:15,  2.45it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:15,  2.45it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.54it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:12,  2.74it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.00it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:10,  3.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:09,  3.44it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:08,  3.59it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  3.78it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:07,  3.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:07,  3.81it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:07,  3.78it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  3.79it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:06,  3.86it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:06,  3.95it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:05,  3.96it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  3.90it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:05,  3.97it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:05,  3.95it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  3.92it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:04,  3.95it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:10<00:03,  4.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.12it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  4.11it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:11<00:03,  4.09it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:11<00:02,  4.01it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  3.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  3.98it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:12<00:02,  3.96it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  3.93it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  3.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  3.96it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.96it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  4.00it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  3.91it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.84it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  3.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2761027812957764, 'eval_runtime': 14.8025, 'eval_samples_per_second': 6.756, 'eval_steps_per_second': 3.378, 'epoch': 1.42, 'num_input_tokens_seen': 582624}\n",
      " 48%|████████████████████▍                      | 40/84 [08:00<06:26,  8.79s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:14<00:00,  3.84it/s]\u001b[A\n",
      "{'loss': 1.2349, 'grad_norm': 0.5429754257202148, 'learning_rate': 0.00022200888097417305, 'epoch': 1.6, 'num_input_tokens_seen': 651104}\n",
      "{'loss': 1.2536, 'grad_norm': 0.49964556097984314, 'learning_rate': 0.00017631120639727393, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [09:25<04:59,  8.82s/it][INFO|trainer.py:3788] 2024-07-12 11:12:39,153 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:12:39,153 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:12:39,153 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.22it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.49it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.14it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.89it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.80it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.81it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:14,  2.86it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  2.93it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.77it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.63it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.63it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.59it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.67it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.75it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.83it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.65it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:11,  2.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.67it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.63it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.76it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:09,  2.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:09,  2.74it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:09,  2.72it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:08,  2.69it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:08,  2.67it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:07,  2.79it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:07,  2.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:06,  2.72it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:06,  2.77it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:06,  2.76it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  2.75it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.86it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:12<00:04,  2.95it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:04,  2.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.86it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:03,  2.87it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.88it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.90it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:02,  2.91it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.67it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:02,  2.57it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  2.51it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.67it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:01,  2.80it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  3.08it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  3.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.2923184633255005, 'eval_runtime': 17.9249, 'eval_samples_per_second': 5.579, 'eval_steps_per_second': 2.789, 'epoch': 1.78, 'num_input_tokens_seen': 720448}\n",
      " 60%|█████████████████████████▌                 | 50/84 [09:43<04:59,  8.82s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  3.53it/s]\u001b[A\n",
      "{'loss': 1.2326, 'grad_norm': 0.6066992282867432, 'learning_rate': 0.0001331828429317345, 'epoch': 1.96, 'num_input_tokens_seen': 793792}\n",
      "{'loss': 1.1297, 'grad_norm': 0.5087156295776367, 'learning_rate': 9.412754953531663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [11:08<03:37,  9.04s/it][INFO|trainer.py:3788] 2024-07-12 11:14:22,050 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:14:22,050 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:14:22,050 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.91it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.48it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.34it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.13it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.91it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.71it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:14,  2.77it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.84it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.85it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:13,  2.73it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.69it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:12,  2.74it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.06it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:08,  3.56it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:08,  3.70it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  3.89it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.87it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.93it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:06,  3.95it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  4.07it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.15it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:05,  4.24it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:05,  4.23it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.20it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:04,  4.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:04,  4.25it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  4.20it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.35it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:03,  4.32it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:03,  4.35it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.39it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  4.33it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.30it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.24it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  4.19it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.19it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.28it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.24it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  4.30it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.29it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  3.92it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:01,  3.49it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.48it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.67it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  3.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3117181062698364, 'eval_runtime': 14.2178, 'eval_samples_per_second': 7.033, 'eval_steps_per_second': 3.517, 'epoch': 2.13, 'num_input_tokens_seen': 864032}\n",
      " 71%|██████████████████████████████▋            | 60/84 [11:22<03:37,  9.04s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.58it/s]\u001b[A\n",
      "{'loss': 1.0567, 'grad_norm': 0.6953117847442627, 'learning_rate': 6.0507069213636716e-05, 'epoch': 2.31, 'num_input_tokens_seen': 938976}\n",
      "{'loss': 1.1661, 'grad_norm': 0.7078378796577454, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [12:49<02:08,  9.19s/it][INFO|trainer.py:3788] 2024-07-12 11:16:03,469 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:16:03,469 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:16:03,469 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.47it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:15,  2.99it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.76it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:16,  2.64it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.74it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.72it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.65it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.58it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:15,  2.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.52it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.52it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.56it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.70it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:11,  2.93it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:10,  3.05it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:09,  3.17it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:09,  3.31it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:08,  3.32it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:08,  3.36it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:08,  3.32it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:08,  3.15it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  2.98it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:08,  2.99it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:07,  2.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:07,  2.81it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:07,  2.86it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.82it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:06,  2.78it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:05,  3.13it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:05,  3.37it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:04,  3.63it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:11<00:03,  3.89it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:03,  3.99it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:12<00:03,  4.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:12<00:02,  4.11it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:12<00:02,  4.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:12<00:02,  4.15it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:13<00:02,  4.23it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:13<00:01,  4.22it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:13<00:01,  4.20it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:13<00:01,  4.20it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:14<00:01,  4.23it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:14<00:00,  4.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:14<00:00,  4.07it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  4.07it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:15<00:00,  4.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3431347608566284, 'eval_runtime': 15.7192, 'eval_samples_per_second': 6.362, 'eval_steps_per_second': 3.181, 'epoch': 2.49, 'num_input_tokens_seen': 1013696}\n",
      " 83%|███████████████████████████████████▊       | 70/84 [13:05<02:08,  9.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  4.06it/s]\u001b[A\n",
      "{'loss': 1.0822, 'grad_norm': 0.7427682280540466, 'learning_rate': 1.4029167422908107e-05, 'epoch': 2.67, 'num_input_tokens_seen': 1088704}\n",
      "{'loss': 1.0238, 'grad_norm': 0.6429394483566284, 'learning_rate': 2.7922934437178693e-06, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [14:32<00:35,  8.82s/it][INFO|trainer.py:3788] 2024-07-12 11:17:46,010 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:17:46,010 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:17:46,010 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.89it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.41it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:15,  2.96it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.82it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:16,  2.70it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.59it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.73it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.67it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.62it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:13,  2.76it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:11,  3.01it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:10,  3.23it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:09,  3.45it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:08,  3.56it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:08,  3.72it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:07,  3.90it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:07,  3.89it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:07,  3.87it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:06,  3.88it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:06,  3.98it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:06,  4.09it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:05,  4.12it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:05,  4.06it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:08<00:05,  4.05it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  4.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:04,  4.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:09<00:04,  4.04it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:04,  4.17it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  4.06it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  4.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:10<00:03,  4.12it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:03,  4.06it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  4.03it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:02,  4.00it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:02,  4.08it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  4.11it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  4.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:01,  4.09it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:01,  4.14it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  4.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  4.12it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:00,  4.04it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  4.04it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  4.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.3511885404586792, 'eval_runtime': 14.3122, 'eval_samples_per_second': 6.987, 'eval_steps_per_second': 3.494, 'epoch': 2.84, 'num_input_tokens_seen': 1155744}\n",
      " 95%|████████████████████████████████████████▉  | 80/84 [14:46<00:35,  8.82s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.05it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 84/84 [15:21<00:00, 10.43s/it][INFO|trainer.py:3478] 2024-07-12 11:18:35,439 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/checkpoint-84\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fada0563bb0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: e7008e11-b7e3-4498-8be0-1bcdeadcaad2)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 11:18:36,013 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/checkpoint-84/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 11:18:36,023 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/checkpoint-84/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-12 11:18:37,165 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 923.4213, 'train_samples_per_second': 2.924, 'train_steps_per_second': 0.091, 'train_loss': 1.2804958025614421, 'epoch': 2.99, 'num_input_tokens_seen': 1218528}\n",
      "100%|███████████████████████████████████████████| 84/84 [15:23<00:00, 10.99s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-12 11:18:37,176 >> Saving model checkpoint to saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/other.py:611: UserWarning: Unable to fetch remote file due to the following error (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /model/Qwen2-1.5B-Instruct/resolve/main/config.json (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fada041a740>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 9e0fd0a4-b73f-4f1a-b435-785f6c06b125)') - silently ignoring the lookup for the file config.json in model/Qwen2-1.5B-Instruct.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in model/Qwen2-1.5B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-12 11:18:37,618 >> tokenizer config file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-12 11:18:37,627 >> Special tokens file saved in saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =    1218528\n",
      "  total_flos               =  8985046GF\n",
      "  train_loss               =     1.2805\n",
      "  train_runtime            = 0:15:23.42\n",
      "  train_samples_per_second =      2.924\n",
      "  train_steps_per_second   =      0.091\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/training_loss.png\n",
      "Figure saved at: saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-12 11:18:38,407 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-12 11:18:38,407 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-12 11:18:38,408 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  2.84it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.3471\n",
      "  eval_runtime            = 0:00:18.01\n",
      "  eval_samples_per_second =      5.549\n",
      "  eval_steps_per_second   =      2.775\n",
      "  num_input_tokens_seen   =    1218528\n",
      "[INFO|modelcard.py:449] 2024-07-12 11:18:56,450 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-1.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-1.5B-Instruct/qlora/train_lr=5e-4 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
