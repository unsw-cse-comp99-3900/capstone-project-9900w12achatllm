{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfo6PZqAjLTB",
        "outputId": "6badf976-28e4-4297-a4e9-8d223fb409de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lmazWzUsjOi5",
        "outputId": "3b884732-0a1e-4e38-d4b8-c64dc381d7b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15858, done.\u001b[K\n",
            "remote: Counting objects: 100% (6931/6931), done.\u001b[K\n",
            "remote: Compressing objects: 100% (525/525), done.\u001b[K\n",
            "remote: Total 15858 (delta 6544), reused 6460 (delta 6403), pack-reused 8927\u001b[K\n",
            "Receiving objects: 100% (15858/15858), 222.27 MiB | 12.21 MiB/s, done.\n",
            "Resolving deltas: 100% (11694/11694), done.\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.42.4)\n",
            "Collecting datasets>=2.16.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.32.1)\n",
            "Collecting peft>=0.11.1 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl>=0.8.6 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.8.4.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.8.4.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.8.4.dev0)\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Collecting fastapi (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.8.4.dev0)\n",
            "  Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.8.4.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.4.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.1 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ruff-0.5.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.8.4.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.8.4.dev0) (1.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.2.2)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.1.2)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m121.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.5.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, ffmpy\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=20629 sha256=ab3bce4617d086e104195c33c7e0a9bdad5bf819880ab1734f50cb6fe13af375\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-joo2mzr0/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=0a9ad67ae19888e34705d941a48779046174bce65170279abd85151cfff896f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=9c1d4e66ebb6ab6248866bc89946c9b50f79d3823124979f4fd83bc6ff8825fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built llamafactory fire ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, uvloop, tomlkit, shtab, semantic-version, ruff, rouge-chinese, requests, python-multipart, python-dotenv, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, dill, aiofiles, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, email_validator, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, datasets, fastapi, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 datasets-2.20.0 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.39.0 gradio-client-1.1.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.8.4.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.6 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.32.3 rouge-chinese-1.0.3 ruff-0.5.4 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.5 uvicorn-0.30.3 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m79tRHgxn_dB",
        "outputId": "63e8d2fd-fb0a-4523-b864-4bdb7c748980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes==0.43.1) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes==0.43.1) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes==0.43.1) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n"
          ]
        }
      ],
      "source": [
        "!pip install bitsandbytes==0.43.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-04 lora qwen05"
      ],
      "metadata": {
        "id": "8HpZ1JavDhLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eZIVC0eDWsV",
        "outputId": "2429abe2-153d-4166-f1ac-c97d8cf3a808",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 16:31:34.715734: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 16:31:34.715779: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 16:31:34.717271: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 16:31:34.727371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 16:31:35.857724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 16:31:42 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:31:42,409 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 16:31:42,660 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 16:31:42 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 16:31:42 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 16:31:43 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:31:44,293 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:31:44,295 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 16:31:44,323 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 16:31:44,332 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:31:44,334 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 16:31:45,495 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 16:31:45,495 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 16:31:45,698 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:31:45,699 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 16:31:45 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 16:31:45 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 16:31:45 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 16:31:45 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 16:31:45 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,o_proj,up_proj,gate_proj,down_proj,k_proj,v_proj\n",
            "07/23/2024 16:31:46 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:642] 2024-07-23 16:31:46,144 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 16:31:46,573 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 16:31:46,573 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 16:31:46,573 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 16:31:46,573 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 16:31:46,573 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 16:31:46,573 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 16:31:46,573 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 16:31:46,576 >>   Number of trainable parameters = 4,399,104\n",
            "{'loss': 1.9346, 'grad_norm': 1.0741908550262451, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.8294, 'grad_norm': 0.7363393902778625, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:27<06:24,  8.75s/it][INFO|trainer.py:3788] 2024-07-23 16:33:14,265 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:33:14,265 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:33:14,265 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 20.14it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:03, 12.98it/s]\u001b[A\n",
            " 16% 8/50 [00:00<00:03, 11.19it/s]\u001b[A\n",
            " 20% 10/50 [00:00<00:03, 10.15it/s]\u001b[A\n",
            " 24% 12/50 [00:01<00:03, 10.21it/s]\u001b[A\n",
            " 28% 14/50 [00:01<00:03, 10.20it/s]\u001b[A\n",
            " 32% 16/50 [00:01<00:03, 11.08it/s]\u001b[A\n",
            " 36% 18/50 [00:01<00:02, 11.73it/s]\u001b[A\n",
            " 40% 20/50 [00:01<00:02, 10.64it/s]\u001b[A\n",
            " 44% 22/50 [00:01<00:02, 10.98it/s]\u001b[A\n",
            " 48% 24/50 [00:02<00:02, 11.26it/s]\u001b[A\n",
            " 52% 26/50 [00:02<00:02, 10.45it/s]\u001b[A\n",
            " 56% 28/50 [00:02<00:01, 11.06it/s]\u001b[A\n",
            " 60% 30/50 [00:02<00:01, 11.17it/s]\u001b[A\n",
            " 64% 32/50 [00:02<00:01, 10.00it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.69it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.51it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.38it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.16it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.93it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.34it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00, 10.02it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.81it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  9.11it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2301363945007324, 'eval_runtime': 4.9364, 'eval_samples_per_second': 20.258, 'eval_steps_per_second': 10.129, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:32<06:24,  8.75s/it]\n",
            "100% 50/50 [00:04<00:00,  9.08it/s]\u001b[A\n",
            "{'loss': 1.725, 'grad_norm': 0.5970982909202576, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.5591, 'grad_norm': 0.5343315005302429, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [02:56<04:50,  8.53s/it][INFO|trainer.py:3788] 2024-07-23 16:34:42,779 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:34:42,779 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:34:42,779 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 18.71it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.52it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.31it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04, 10.16it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:03,  9.78it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.59it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:02, 11.35it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.88it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.79it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.76it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.46it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.80it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.91it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.49it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.79it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.40it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.32it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.18it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.41it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.87it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.23it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.98it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.86it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  9.14it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.218116283416748, 'eval_runtime': 4.957, 'eval_samples_per_second': 20.174, 'eval_steps_per_second': 10.087, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [03:01<04:50,  8.53s/it]\n",
            "100% 50/50 [00:04<00:00,  9.14it/s]\u001b[A\n",
            "{'loss': 1.5552, 'grad_norm': 0.5936908721923828, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.5633, 'grad_norm': 0.5585569739341736, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:24<03:24,  8.51s/it][INFO|trainer.py:3788] 2024-07-23 16:36:11,278 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:36:11,278 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:36:11,278 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.76it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.53it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.36it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04, 10.09it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:03,  9.78it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03, 10.31it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.55it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:02, 11.25it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.85it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.85it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.93it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.61it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.80it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.88it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.48it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.78it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.31it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.16it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.37it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.88it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.26it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.93it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.70it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  9.10it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2183427810668945, 'eval_runtime': 4.9619, 'eval_samples_per_second': 20.154, 'eval_steps_per_second': 10.077, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:29<03:24,  8.51s/it]\n",
            "100% 50/50 [00:04<00:00,  9.08it/s]\u001b[A\n",
            "{'loss': 1.5432, 'grad_norm': 0.6437570452690125, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.4945, 'grad_norm': 0.6467742919921875, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [05:56<02:00,  8.62s/it][INFO|trainer.py:3788] 2024-07-23 16:37:42,690 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:37:42,691 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:37:42,691 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.69it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.39it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.23it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04, 10.07it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:03,  9.80it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03, 10.27it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.46it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:02, 11.17it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.71it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.77it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.85it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.48it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.72it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.81it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.37it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.70it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.35it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.20it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.10it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.36it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.82it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.27it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.96it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.71it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  9.16it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2214750051498413, 'eval_runtime': 4.9854, 'eval_samples_per_second': 20.059, 'eval_steps_per_second': 10.029, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [06:01<02:00,  8.62s/it]\n",
            "100% 50/50 [00:04<00:00,  9.08it/s]\u001b[A\n",
            "{'loss': 1.3693, 'grad_norm': 0.7295779585838318, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.4146, 'grad_norm': 0.7126926779747009, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:25<00:33,  8.28s/it][INFO|trainer.py:3788] 2024-07-23 16:39:12,042 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:39:12,042 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:39:12,042 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 20.14it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:03, 13.05it/s]\u001b[A\n",
            " 16% 8/50 [00:00<00:03, 11.15it/s]\u001b[A\n",
            " 20% 10/50 [00:00<00:03, 10.12it/s]\u001b[A\n",
            " 24% 12/50 [00:01<00:03, 10.13it/s]\u001b[A\n",
            " 28% 14/50 [00:01<00:03, 10.18it/s]\u001b[A\n",
            " 32% 16/50 [00:01<00:03, 11.03it/s]\u001b[A\n",
            " 36% 18/50 [00:01<00:02, 11.68it/s]\u001b[A\n",
            " 40% 20/50 [00:01<00:02, 10.56it/s]\u001b[A\n",
            " 44% 22/50 [00:01<00:02, 10.90it/s]\u001b[A\n",
            " 48% 24/50 [00:02<00:02, 11.09it/s]\u001b[A\n",
            " 52% 26/50 [00:02<00:02, 10.38it/s]\u001b[A\n",
            " 56% 28/50 [00:02<00:01, 11.11it/s]\u001b[A\n",
            " 60% 30/50 [00:02<00:01, 11.19it/s]\u001b[A\n",
            " 64% 32/50 [00:02<00:01, 10.01it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.61it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.32it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.16it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.39it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.90it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.21it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.93it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.74it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.78it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.95it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.22318434715271, 'eval_runtime': 4.9587, 'eval_samples_per_second': 20.167, 'eval_steps_per_second': 10.083, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:30<00:33,  8.28s/it]\n",
            "100% 50/50 [00:04<00:00,  8.87it/s]\u001b[A\n",
            "100% 54/54 [08:04<00:00,  9.05s/it][INFO|trainer.py:3478] 2024-07-23 16:39:51,291 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:39:51,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:39:51,896 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:39:52,008 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:39:52,012 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 16:39:52,413 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 485.8371, 'train_samples_per_second': 5.557, 'train_steps_per_second': 0.111, 'train_loss': 1.5848010027850117, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [08:05<00:00,  9.00s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 16:39:52,417 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:39:52,885 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:39:52,886 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:39:52,988 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:39:52,993 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  2649106GF\n",
            "  train_loss               =     1.5848\n",
            "  train_runtime            = 0:08:05.83\n",
            "  train_samples_per_second =      5.557\n",
            "  train_steps_per_second   =      0.111\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04/training_eval_loss.png\n",
            "07/23/2024 16:39:53 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 16:39:53,483 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:39:53,484 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:39:53,484 >>   Batch size = 2\n",
            "100% 50/50 [00:04<00:00, 10.22it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.2232\n",
            "  eval_runtime            = 0:00:05.01\n",
            "  eval_samples_per_second =      19.93\n",
            "  eval_steps_per_second   =      9.965\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 16:39:58,518 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-04_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZbLvVilEmfS",
        "outputId": "b6799dfb-cb6a-431a-b705-1330602b864f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 17:16:42.201825: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 17:16:42.201876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 17:16:42.203365: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 17:16:42.212294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 17:16:43.391892: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 17:16:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:16:49,916 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 17:16:50,150 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 17:16:50 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 17:16:50 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 17:16:50 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[27, 91, 872, 91, 397, 16141, 279, 2701, 5248, 5754, 3405, 25, 362, 26052, 29199, 39852, 33283, 14829, 18967, 374, 26568, 264, 1803, 19308, 25629, 12733, 448, 279, 9292, 21201, 438, 279, 23218, 27279, 13, 11954, 279, 1142, 11, 279, 18967, 69085, 15104, 264, 5763, 269, 87832, 13, 576, 87832, 374, 51734, 2041, 85819, 13, 576, 23218, 10742, 279, 18967, 429, 279, 8720, 686, 653, 6915, 11, 323, 1052, 374, 902, 1184, 311, 1895, 419, 8922, 85819, 429, 686, 537, 11428, 279, 8720, 11, 438, 566, 1558, 537, 1366, 311, 1281, 279, 8720, 10955, 82374, 13, 1260, 10742, 279, 18967, 311, 5274, 419, 85819, 700, 315, 279, 63785, 1895, 13, 15920, 315, 279, 2701, 374, 279, 4396, 1790, 1917, 369, 279, 18967, 311, 1896, 12622, 362, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151646, 198, 27, 91, 77091, 91, 397]\n",
            "inputs:\n",
            "<|user|>\n",
            "Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 17:16:51,135 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 17:16:51,136 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 17:16:51 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 17:16:51,165 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 17:16:51,175 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:16:51,176 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 17:16:52,130 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 17:16:52,130 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 17:16:52,325 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:16:52,325 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 17:16:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 17:16:52 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 17:16:52 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04\n",
            "07/23/2024 17:16:52 - INFO - llamafactory.model.loader - all params: 494,032,768\n",
            "[INFO|trainer.py:3788] 2024-07-23 17:16:52,954 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 17:16:52,954 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 17:16:52,954 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-23 17:17:08,921 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "100% 34/34 [05:41<00:00,  4.44s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.607 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [05:42<00:00, 10.07s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    25.9257\n",
            "  predict_rouge-1            =    33.8807\n",
            "  predict_rouge-2            =    18.6024\n",
            "  predict_rouge-l            =    30.4131\n",
            "  predict_runtime            = 0:05:58.98\n",
            "  predict_samples_per_second =      0.279\n",
            "  predict_steps_per_second   =      0.095\n",
            "07/23/2024 17:22:51 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-04_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-04_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-04"
      ],
      "metadata": {
        "id": "lK5YwInSEmhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-05 lora qwen05"
      ],
      "metadata": {
        "id": "0uWU_msMDraQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpBXtPzHDTQB",
        "outputId": "a52cdfcb-29cf-47eb-f5fa-b4a1e65a7678"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 16:23:06.030703: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 16:23:06.030751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 16:23:06.032056: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 16:23:06.039115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 16:23:07.197915: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 16:23:13 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,784 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,784 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,785 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,785 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,785 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:23:13,785 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 16:23:14,034 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 16:23:14 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 16:23:14 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 16:23:14 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:23:15,707 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:23:15,708 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 16:23:15,737 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 16:23:15,746 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:23:15,748 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 16:23:16,906 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 16:23:16,906 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 16:23:17,107 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:23:17,107 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,v_proj,o_proj,q_proj,down_proj,up_proj\n",
            "07/23/2024 16:23:17 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:642] 2024-07-23 16:23:17,537 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 16:23:17,957 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 16:23:17,957 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 16:23:17,957 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 16:23:17,957 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 16:23:17,957 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 16:23:17,957 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 16:23:17,957 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 16:23:17,960 >>   Number of trainable parameters = 4,399,104\n",
            "{'loss': 2.0739, 'grad_norm': 1.441144585609436, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.9661, 'grad_norm': 1.0188319683074951, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:27<06:25,  8.75s/it][INFO|trainer.py:3788] 2024-07-23 16:24:45,714 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:24:45,714 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:24:45,714 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 20.00it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:03, 12.99it/s]\u001b[A\n",
            " 16% 8/50 [00:00<00:03, 11.18it/s]\u001b[A\n",
            " 20% 10/50 [00:00<00:03, 10.20it/s]\u001b[A\n",
            " 24% 12/50 [00:01<00:03, 10.20it/s]\u001b[A\n",
            " 28% 14/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
            " 32% 16/50 [00:01<00:03, 10.98it/s]\u001b[A\n",
            " 36% 18/50 [00:01<00:02, 11.66it/s]\u001b[A\n",
            " 40% 20/50 [00:01<00:02, 10.58it/s]\u001b[A\n",
            " 44% 22/50 [00:01<00:02, 10.96it/s]\u001b[A\n",
            " 48% 24/50 [00:02<00:02, 11.05it/s]\u001b[A\n",
            " 52% 26/50 [00:02<00:02, 10.23it/s]\u001b[A\n",
            " 56% 28/50 [00:02<00:02, 11.00it/s]\u001b[A\n",
            " 60% 30/50 [00:02<00:01, 11.14it/s]\u001b[A\n",
            " 64% 32/50 [00:02<00:01,  9.98it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.68it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.49it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.35it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.15it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.40it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.86it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.27it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.99it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.76it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.76it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.91it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3116683959960938, 'eval_runtime': 4.961, 'eval_samples_per_second': 20.157, 'eval_steps_per_second': 10.079, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:32<06:25,  8.75s/it]\n",
            "100% 50/50 [00:04<00:00,  8.86it/s]\u001b[A\n",
            "{'loss': 1.8036, 'grad_norm': 0.8869906067848206, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.6268, 'grad_norm': 0.8778506517410278, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [02:56<04:49,  8.53s/it][INFO|trainer.py:3788] 2024-07-23 16:26:14,187 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:26:14,187 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:26:14,188 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.82it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.52it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.33it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04, 10.13it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:03,  9.79it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03, 10.32it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.47it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:02, 11.22it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.82it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.79it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.87it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.56it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.73it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.81it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.36it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.74it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.40it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.28it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.11it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.34it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.83it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.22it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.94it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.74it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.74it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2529176473617554, 'eval_runtime': 4.9747, 'eval_samples_per_second': 20.102, 'eval_steps_per_second': 10.051, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [03:01<04:49,  8.53s/it]\n",
            "100% 50/50 [00:04<00:00,  8.91it/s]\u001b[A\n",
            "{'loss': 1.7289, 'grad_norm': 0.8163855075836182, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.7578, 'grad_norm': 0.7438313961029053, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:24<03:24,  8.50s/it][INFO|trainer.py:3788] 2024-07-23 16:27:42,717 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:27:42,717 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:27:42,717 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 20.08it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:03, 13.04it/s]\u001b[A\n",
            " 16% 8/50 [00:00<00:03, 11.21it/s]\u001b[A\n",
            " 20% 10/50 [00:00<00:03, 10.19it/s]\u001b[A\n",
            " 24% 12/50 [00:01<00:03, 10.13it/s]\u001b[A\n",
            " 28% 14/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
            " 32% 16/50 [00:01<00:03, 11.01it/s]\u001b[A\n",
            " 36% 18/50 [00:01<00:02, 11.68it/s]\u001b[A\n",
            " 40% 20/50 [00:01<00:02, 10.51it/s]\u001b[A\n",
            " 44% 22/50 [00:01<00:02, 10.92it/s]\u001b[A\n",
            " 48% 24/50 [00:02<00:02, 11.15it/s]\u001b[A\n",
            " 52% 26/50 [00:02<00:02, 10.31it/s]\u001b[A\n",
            " 56% 28/50 [00:02<00:01, 11.03it/s]\u001b[A\n",
            " 60% 30/50 [00:02<00:01, 11.09it/s]\u001b[A\n",
            " 64% 32/50 [00:02<00:01,  9.93it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.65it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.44it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.30it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.14it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.39it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.88it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.26it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.98it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.78it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.80it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.92it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2393155097961426, 'eval_runtime': 4.9624, 'eval_samples_per_second': 20.151, 'eval_steps_per_second': 10.076, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:29<03:24,  8.50s/it]\n",
            "100% 50/50 [00:04<00:00,  8.86it/s]\u001b[A\n",
            "{'loss': 1.7606, 'grad_norm': 0.7756956815719604, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.7361, 'grad_norm': 0.7307877540588379, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [05:56<02:00,  8.63s/it][INFO|trainer.py:3788] 2024-07-23 16:29:14,231 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:29:14,231 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:29:14,231 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.88it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.65it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.41it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04, 10.14it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:03,  9.83it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03, 10.33it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.54it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:02, 11.24it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.82it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.83it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.89it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.57it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.81it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.91it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.29it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.66it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.34it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.20it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.10it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.33it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.77it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.17it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.87it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.67it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.74it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.85it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2347214221954346, 'eval_runtime': 4.9819, 'eval_samples_per_second': 20.073, 'eval_steps_per_second': 10.036, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [06:01<02:00,  8.63s/it]\n",
            "100% 50/50 [00:04<00:00,  8.84it/s]\u001b[A\n",
            "{'loss': 1.6559, 'grad_norm': 0.9329028725624084, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.6953, 'grad_norm': 0.7628763914108276, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:25<00:33,  8.28s/it][INFO|trainer.py:3788] 2024-07-23 16:30:43,540 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:30:43,541 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:30:43,541 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 20.22it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:03, 12.94it/s]\u001b[A\n",
            " 16% 8/50 [00:00<00:03, 11.17it/s]\u001b[A\n",
            " 20% 10/50 [00:00<00:03, 10.14it/s]\u001b[A\n",
            " 24% 12/50 [00:01<00:03, 10.14it/s]\u001b[A\n",
            " 28% 14/50 [00:01<00:03, 10.17it/s]\u001b[A\n",
            " 32% 16/50 [00:01<00:03, 10.99it/s]\u001b[A\n",
            " 36% 18/50 [00:01<00:02, 11.48it/s]\u001b[A\n",
            " 40% 20/50 [00:01<00:02, 10.48it/s]\u001b[A\n",
            " 44% 22/50 [00:01<00:02, 10.80it/s]\u001b[A\n",
            " 48% 24/50 [00:02<00:02, 11.11it/s]\u001b[A\n",
            " 52% 26/50 [00:02<00:02, 10.36it/s]\u001b[A\n",
            " 56% 28/50 [00:02<00:01, 11.08it/s]\u001b[A\n",
            " 60% 30/50 [00:02<00:01, 11.23it/s]\u001b[A\n",
            " 64% 32/50 [00:02<00:01, 10.03it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.68it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.47it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.35it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  9.14it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.41it/s]\u001b[A\n",
            " 82% 41/50 [00:03<00:00,  9.89it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00, 10.28it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.97it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.77it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  9.13it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2333474159240723, 'eval_runtime': 4.9559, 'eval_samples_per_second': 20.178, 'eval_steps_per_second': 10.089, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:30<00:33,  8.28s/it]\n",
            "100% 50/50 [00:04<00:00,  9.11it/s]\u001b[A\n",
            "100% 54/54 [08:04<00:00,  9.04s/it][INFO|trainer.py:3478] 2024-07-23 16:31:22,725 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:31:23,179 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:31:23,180 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:31:23,290 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:31:23,294 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 16:31:23,736 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 485.7758, 'train_samples_per_second': 5.558, 'train_steps_per_second': 0.111, 'train_loss': 1.7740475160104257, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [08:05<00:00,  9.00s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 16:31:23,740 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:31:24,152 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:31:24,152 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:31:24,271 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:31:24,276 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  2649106GF\n",
            "  train_loss               =      1.774\n",
            "  train_runtime            = 0:08:05.77\n",
            "  train_samples_per_second =      5.558\n",
            "  train_steps_per_second   =      0.111\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05/training_eval_loss.png\n",
            "07/23/2024 16:31:24 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 16:31:24,759 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:31:24,759 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:31:24,759 >>   Batch size = 2\n",
            "100% 50/50 [00:04<00:00, 10.22it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.2333\n",
            "  eval_runtime            = 0:00:05.01\n",
            "  eval_samples_per_second =      19.93\n",
            "  eval_steps_per_second   =      9.965\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 16:31:29,816 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-05_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05"
      ],
      "metadata": {
        "id": "eZwqjvJFFbcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-05_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ_3nRu2Fbeh",
        "outputId": "278e44f6-a5e7-4fbe-c149-85a01edaf48a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 17:33:53.169934: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 17:33:53.169987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 17:33:53.171328: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 17:33:53.178711: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 17:33:54.330036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 17:34:00 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:34:00,936 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 17:34:01,185 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 17:34:01 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 17:34:01 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 17:34:01 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[27, 91, 872, 91, 397, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 12553, 279, 5055, 72728, 315, 6267, 68934, 24613, 4694, 2115, 68666, 52472, 5267, 98365, 50, 25, 576, 5055, 72728, 315, 6267, 68934, 24613, 320, 14912, 39, 1220, 8, 374, 264, 2697, 11, 52723, 78285, 5452, 429, 10953, 63808, 28187, 13, 4940, 220, 19, 17, 3204, 3501, 11, 220, 22, 3501, 525, 5961, 5435, 311, 18662, 315, 4128, 7707, 448, 1172, 220, 17, 3501, 5435, 311, 38232, 13, 1205, 24109, 279, 42615, 6773, 315, 279, 451, 5245, 50, 259, 12, 8041, 12654, 9091, 311, 1273, 279, 30078, 429, 279, 2790, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 1290, 68666, 52472, 1035, 387, 7046, 1091, 279, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 2115, 68666, 52472, 879, 614, 4428, 83270, 1220, 12205, 13, 576, 8123, 315, 12654, 572, 10838, 553, 6366, 1506, 2168, 6358, 315, 18572, 12351, 323, 18572, 5335, 9768, 389, 6366, 16971, 323, 28293, 53758, 13, 42592, 39214, 17991, 315, 92117, 8123, 572, 10660, 369, 1817, 18572, 13, 4058, 10155, 92117, 8123, 572, 29139, 304, 264, 71710, 30549, 1614, 311, 7023, 8123, 315, 12654, 553, 83270, 1220, 5456, 369, 1817, 68666, 13, 77437, 1515, 7077, 25588, 572, 1483, 311, 8253, 279, 12687, 1948, 279, 83270, 1220, 5456, 323, 92117, 8123, 13, 576, 8123, 369, 1290, 68666, 12654, 572, 46852, 7046, 1091, 279, 8123, 369, 2115, 68666, 52472, 11, 42368, 369, 279, 25869, 83270, 1220, 320, 47, 27, 15, 13, 220, 15, 15, 16, 568, 1752, 1817, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 5456, 27, 17, 15, 11, 279, 22553, 8123, 315, 1290, 68666, 52472, 572, 13187, 1990, 279, 22553, 8123, 315, 2115, 68666, 52472, 13, 1752, 3110, 11, 369, 6835, 448, 264, 2115, 68666, 12654, 323, 264, 220, 17, 19, 21231, 83270, 1220, 5456, 315, 220, 16, 21, 311, 220, 17, 15, 11, 279, 22553, 8123, 315, 59645, 4132, 277, 407, 572, 220, 19, 23, 64070, 320, 2245, 446, 471, 457, 2088, 220, 16, 19, 311, 220, 16, 16, 16, 64070, 8, 438, 7707, 448, 220, 16, 18, 18, 64070, 320, 2245, 446, 471, 457, 2088, 220, 23, 16, 311, 220, 17, 15, 23, 64070, 8, 369, 6835, 448, 264, 1290, 68666, 12654, 320, 47, 27, 15, 13, 15, 15, 16, 568, 576, 22553, 8123, 315, 264, 1290, 68666, 12654, 572, 17267, 6144, 311, 279, 22553, 8123, 315, 264, 2115, 68666, 12654, 304, 279, 1790, 8426, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 13, 576, 77437, 1515, 7077, 25588, 1948, 279, 220, 17, 19, 21231, 83270, 1220, 5456, 323, 220, 18, 22289, 92117, 8123, 572, 220, 15, 13, 22, 17, 369, 6835, 448, 2115, 68666, 12654, 323, 220, 15, 13, 22, 16, 369, 6835, 448, 1290, 68666, 12654, 13, 151646, 198, 27, 91, 77091, 91, 397]\n",
            "inputs:\n",
            "<|user|>\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 17:34:02,149 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 17:34:02,150 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 17:34:02 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 17:34:02,180 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 17:34:02,190 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:34:02,192 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 17:34:03,119 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 17:34:03,119 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 17:34:03,314 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:34:03,314 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 17:34:03 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 17:34:03 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 17:34:03 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank8_5e-05\n",
            "07/23/2024 17:34:03 - INFO - llamafactory.model.loader - all params: 494,032,768\n",
            "[INFO|trainer.py:3788] 2024-07-23 17:34:03,943 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 17:34:03,943 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 17:34:03,943 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-23 17:34:26,492 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "100% 34/34 [08:18<00:00, 13.38s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.636 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [08:20<00:00, 14.71s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    24.4288\n",
            "  predict_rouge-1            =     32.845\n",
            "  predict_rouge-2            =    10.1459\n",
            "  predict_rouge-l            =    18.7106\n",
            "  predict_runtime            = 0:08:43.43\n",
            "  predict_samples_per_second =      0.191\n",
            "  predict_steps_per_second   =      0.065\n",
            "07/23/2024 17:42:47 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank8_5e-05_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-04 lora qwen05"
      ],
      "metadata": {
        "id": "bcu9FnPsD0rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QJUckF7DPkG",
        "outputId": "003b940f-cb5e-4e13-88e5-90f030515784",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-24 01:56:08.170235: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-24 01:56:08.170286: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-24 01:56:08.171531: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-24 01:56:08.178429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-24 01:56:09.326333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/24/2024 01:56:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 01:56:15,934 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-24 01:56:16,191 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/24/2024 01:56:16 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/24/2024 01:56:16 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:00, 1649.54 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2150.13 examples/s]\n",
            "07/24/2024 01:56:19 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 10731.73 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2365.26 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:02<00:00, 396.19 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "config.json: 100% 659/659 [00:00<00:00, 5.18MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-24 01:56:27,818 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-24 01:56:27,821 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 988M/988M [00:33<00:00, 29.3MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-24 01:57:02,249 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-24 01:57:02,267 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 01:57:02,269 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-24 01:57:03,507 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-24 01:57:03,508 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 1.46MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-24 01:57:03,899 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 01:57:03,900 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/24/2024 01:57:03 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/24/2024 01:57:03 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/24/2024 01:57:03 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/24/2024 01:57:03 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/24/2024 01:57:03 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,down_proj,o_proj,v_proj,up_proj,q_proj\n",
            "07/24/2024 01:57:04 - INFO - llamafactory.model.loader - trainable params: 8,798,208 || all params: 502,830,976 || trainable%: 1.7497\n",
            "[INFO|trainer.py:642] 2024-07-24 01:57:04,665 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-24 01:57:05,153 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-24 01:57:05,153 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-24 01:57:05,153 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-24 01:57:05,153 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-24 01:57:05,153 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-24 01:57:05,153 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-24 01:57:05,153 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-24 01:57:05,157 >>   Number of trainable parameters = 8,798,208\n",
            "{'loss': 1.9374, 'grad_norm': 0.7540633082389832, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.8309, 'grad_norm': 0.5300021767616272, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:29<06:45,  9.22s/it][INFO|trainer.py:3788] 2024-07-24 01:58:35,113 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 01:58:35,113 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 01:58:35,113 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 18.71it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 11.94it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 11.76it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04,  9.63it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:04,  9.33it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03,  9.82it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.10it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:03, 10.83it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.46it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.36it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.48it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.16it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.33it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:02, 10.45it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.07it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.37it/s]\u001b[A\n",
            " 68% 34/50 [00:03<00:01,  9.17it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.00it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  8.92it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  8.81it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.06it/s]\u001b[A\n",
            " 82% 41/50 [00:04<00:00,  9.51it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00,  9.90it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.63it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.43it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.42it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.61it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2294776439666748, 'eval_runtime': 5.1652, 'eval_samples_per_second': 19.36, 'eval_steps_per_second': 9.68, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:35<06:45,  9.22s/it]\n",
            "100% 50/50 [00:05<00:00,  8.62it/s]\u001b[A\n",
            "{'loss': 1.7244, 'grad_norm': 0.41890379786491394, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.5585, 'grad_norm': 0.37734803557395935, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [03:01<05:01,  8.85s/it][INFO|trainer.py:3788] 2024-07-24 02:00:06,546 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:00:06,546 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:00:06,547 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.25it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.19it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 11.93it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04,  9.80it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:04,  9.37it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03,  9.94it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:03, 10.96it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.54it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.46it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.56it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.30it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.49it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.60it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.25it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.51it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.10it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  8.97it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  8.83it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.08it/s]\u001b[A\n",
            " 82% 41/50 [00:04<00:00,  9.50it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00,  9.88it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.64it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.45it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.49it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.70it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2174640893936157, 'eval_runtime': 5.1183, 'eval_samples_per_second': 19.538, 'eval_steps_per_second': 9.769, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [03:06<05:01,  8.85s/it]\n",
            "100% 50/50 [00:05<00:00,  8.70it/s]\u001b[A\n",
            "{'loss': 1.5584, 'grad_norm': 0.4201507866382599, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.5677, 'grad_norm': 0.3926990032196045, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:32<03:31,  8.80s/it][INFO|trainer.py:3788] 2024-07-24 02:01:38,037 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:01:38,037 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:01:38,037 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.51it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.23it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.00it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04,  9.87it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:04,  9.36it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03,  9.92it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:03, 10.98it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.63it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.58it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.60it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.24it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.48it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.60it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.23it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.48it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.11it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  8.98it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  8.85it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.07it/s]\u001b[A\n",
            " 82% 41/50 [00:04<00:00,  9.48it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00,  9.87it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.62it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.43it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.48it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.67it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.21890389919281, 'eval_runtime': 5.116, 'eval_samples_per_second': 19.546, 'eval_steps_per_second': 9.773, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [04:37<03:31,  8.80s/it]\n",
            "100% 50/50 [00:05<00:00,  8.68it/s]\u001b[A\n",
            "{'loss': 1.5469, 'grad_norm': 0.46128517389297485, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.4984, 'grad_norm': 0.4528440535068512, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [06:07<02:04,  8.93s/it][INFO|trainer.py:3788] 2024-07-24 02:03:12,740 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:03:12,740 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:03:12,740 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.26it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.27it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.08it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04,  9.74it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:04,  9.42it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03,  9.93it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.16it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:03, 10.94it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.56it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.53it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.57it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.30it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.52it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.58it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.18it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.49it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.15it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.04it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  8.85it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.05it/s]\u001b[A\n",
            " 82% 41/50 [00:04<00:00,  9.51it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00,  9.88it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.64it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.41it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.47it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.65it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.220996379852295, 'eval_runtime': 5.1216, 'eval_samples_per_second': 19.525, 'eval_steps_per_second': 9.763, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [06:12<02:04,  8.93s/it]\n",
            "100% 50/50 [00:05<00:00,  8.59it/s]\u001b[A\n",
            "{'loss': 1.3742, 'grad_norm': 0.5042566061019897, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.4207, 'grad_norm': 0.49404221773147583, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:39<00:34,  8.56s/it][INFO|trainer.py:3788] 2024-07-24 02:04:45,071 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:04:45,071 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:04:45,071 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:02, 19.72it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:03, 12.43it/s]\u001b[A\n",
            " 14% 7/50 [00:00<00:03, 12.18it/s]\u001b[A\n",
            " 18% 9/50 [00:00<00:04,  9.88it/s]\u001b[A\n",
            " 22% 11/50 [00:01<00:04,  9.42it/s]\u001b[A\n",
            " 26% 13/50 [00:01<00:03,  9.95it/s]\u001b[A\n",
            " 30% 15/50 [00:01<00:03, 10.18it/s]\u001b[A\n",
            " 34% 17/50 [00:01<00:03, 11.00it/s]\u001b[A\n",
            " 38% 19/50 [00:01<00:02, 10.55it/s]\u001b[A\n",
            " 42% 21/50 [00:01<00:02, 10.49it/s]\u001b[A\n",
            " 46% 23/50 [00:02<00:02, 10.54it/s]\u001b[A\n",
            " 50% 25/50 [00:02<00:02, 10.33it/s]\u001b[A\n",
            " 54% 27/50 [00:02<00:02, 10.62it/s]\u001b[A\n",
            " 58% 29/50 [00:02<00:01, 10.70it/s]\u001b[A\n",
            " 62% 31/50 [00:02<00:01, 11.31it/s]\u001b[A\n",
            " 66% 33/50 [00:03<00:01,  9.51it/s]\u001b[A\n",
            " 70% 35/50 [00:03<00:01,  9.13it/s]\u001b[A\n",
            " 72% 36/50 [00:03<00:01,  9.04it/s]\u001b[A\n",
            " 74% 37/50 [00:03<00:01,  8.93it/s]\u001b[A\n",
            " 78% 39/50 [00:03<00:01,  9.14it/s]\u001b[A\n",
            " 82% 41/50 [00:04<00:00,  9.58it/s]\u001b[A\n",
            " 86% 43/50 [00:04<00:00,  9.93it/s]\u001b[A\n",
            " 90% 45/50 [00:04<00:00,  9.65it/s]\u001b[A\n",
            " 92% 46/50 [00:04<00:00,  9.41it/s]\u001b[A\n",
            " 94% 47/50 [00:04<00:00,  9.42it/s]\u001b[A\n",
            " 96% 48/50 [00:04<00:00,  8.65it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2225416898727417, 'eval_runtime': 5.1015, 'eval_samples_per_second': 19.602, 'eval_steps_per_second': 9.801, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [07:45<00:34,  8.56s/it]\n",
            "100% 50/50 [00:05<00:00,  8.61it/s]\u001b[A\n",
            "100% 54/54 [08:20<00:00,  9.38s/it][INFO|trainer.py:3478] 2024-07-24 02:05:25,732 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-24 02:05:26,182 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-24 02:05:26,183 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-24 02:05:26,323 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-24 02:05:26,327 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-24 02:05:26,828 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 501.6705, 'train_samples_per_second': 5.382, 'train_steps_per_second': 0.108, 'train_loss': 1.5878477802983038, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [08:21<00:00,  9.29s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-24 02:05:26,832 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-24 02:05:27,270 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-24 02:05:27,271 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-24 02:05:27,410 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-24 02:05:27,415 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  2681272GF\n",
            "  train_loss               =     1.5878\n",
            "  train_runtime            = 0:08:21.67\n",
            "  train_samples_per_second =      5.382\n",
            "  train_steps_per_second   =      0.108\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04/training_eval_loss.png\n",
            "07/24/2024 02:05:27 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-24 02:05:27,953 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:05:27,953 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:05:27,953 >>   Batch size = 2\n",
            "100% 50/50 [00:05<00:00,  9.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.2226\n",
            "  eval_runtime            = 0:00:05.17\n",
            "  eval_samples_per_second =     19.311\n",
            "  eval_steps_per_second   =      9.656\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-24 02:05:33,148 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank16_MedQA_test_US_5e-04 \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04"
      ],
      "metadata": {
        "id": "-QYQ9PGqFqMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de99e58e-3fdf-43f6-a35a-318cf186de6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-24 02:07:00.047219: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-24 02:07:00.047273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-24 02:07:00.048670: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-24 02:07:00.055963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-24 02:07:01.191086: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/24/2024 02:07:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:07:07,776 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-24 02:07:08,022 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/24/2024 02:07:08 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/24/2024 02:07:08 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/24/2024 02:07:08 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "Generating train split: 1273 examples [00:00, 2522.12 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 442.09 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:02<00:00, 42.50 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[27, 91, 872, 91, 397, 16141, 279, 2701, 5248, 5754, 3405, 25, 362, 26052, 29199, 39852, 33283, 14829, 18967, 374, 26568, 264, 1803, 19308, 25629, 12733, 448, 279, 9292, 21201, 438, 279, 23218, 27279, 13, 11954, 279, 1142, 11, 279, 18967, 69085, 15104, 264, 5763, 269, 87832, 13, 576, 87832, 374, 51734, 2041, 85819, 13, 576, 23218, 10742, 279, 18967, 429, 279, 8720, 686, 653, 6915, 11, 323, 1052, 374, 902, 1184, 311, 1895, 419, 8922, 85819, 429, 686, 537, 11428, 279, 8720, 11, 438, 566, 1558, 537, 1366, 311, 1281, 279, 8720, 10955, 82374, 13, 1260, 10742, 279, 18967, 311, 5274, 419, 85819, 700, 315, 279, 63785, 1895, 13, 15920, 315, 279, 2701, 374, 279, 4396, 1790, 1917, 369, 279, 18967, 311, 1896, 12622, 362, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151646, 198, 27, 91, 77091, 91, 397]\n",
            "inputs:\n",
            "<|user|>\n",
            "Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-07-24 02:07:14,306 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-24 02:07:14,308 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/24/2024 02:07:14 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-24 02:07:14,340 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-24 02:07:14,350 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 02:07:14,351 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-24 02:07:15,388 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-24 02:07:15,388 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-24 02:07:15,616 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 02:07:15,616 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/24/2024 02:07:15 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/24/2024 02:07:16 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/24/2024 02:07:16 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16_5e-04\n",
            "07/24/2024 02:07:16 - INFO - llamafactory.model.loader - all params: 494,032,768\n",
            "[INFO|trainer.py:3788] 2024-07-24 02:07:16,336 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:07:16,336 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:07:16,336 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-24 02:07:38,605 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "100% 34/34 [06:14<00:00,  6.13s/it]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.675 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [06:15<00:00, 11.05s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    21.5864\n",
            "  predict_rouge-1            =    31.8232\n",
            "  predict_rouge-2            =    17.7647\n",
            "  predict_rouge-l            =    26.8691\n",
            "  predict_runtime            = 0:06:38.55\n",
            "  predict_samples_per_second =      0.251\n",
            "  predict_steps_per_second   =      0.085\n",
            "07/24/2024 02:13:54 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank16_MedQA_test_US_5e-04/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/eval_LoRA_Rank16_PubMedQA_val_5e-04 \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16"
      ],
      "metadata": {
        "id": "6r8XVUGYFqOI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3563395e-0c70-48cd-b2bd-eea1f454f978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-24 02:13:59.814742: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-24 02:13:59.814793: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-24 02:13:59.816299: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-24 02:13:59.823992: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-24 02:14:00.999139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/24/2024 02:14:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-24 02:14:07,523 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-24 02:14:07,781 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/24/2024 02:14:07 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/24/2024 02:14:07 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/24/2024 02:14:07 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "Generating train split: 100 examples [00:00, 6193.51 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 437.26 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:02<00:00, 40.14 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[27, 91, 872, 91, 397, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 12553, 279, 5055, 72728, 315, 6267, 68934, 24613, 4694, 2115, 68666, 52472, 5267, 98365, 50, 25, 576, 5055, 72728, 315, 6267, 68934, 24613, 320, 14912, 39, 1220, 8, 374, 264, 2697, 11, 52723, 78285, 5452, 429, 10953, 63808, 28187, 13, 4940, 220, 19, 17, 3204, 3501, 11, 220, 22, 3501, 525, 5961, 5435, 311, 18662, 315, 4128, 7707, 448, 1172, 220, 17, 3501, 5435, 311, 38232, 13, 1205, 24109, 279, 42615, 6773, 315, 279, 451, 5245, 50, 259, 12, 8041, 12654, 9091, 311, 1273, 279, 30078, 429, 279, 2790, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 1290, 68666, 52472, 1035, 387, 7046, 1091, 279, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 2115, 68666, 52472, 879, 614, 4428, 83270, 1220, 12205, 13, 576, 8123, 315, 12654, 572, 10838, 553, 6366, 1506, 2168, 6358, 315, 18572, 12351, 323, 18572, 5335, 9768, 389, 6366, 16971, 323, 28293, 53758, 13, 42592, 39214, 17991, 315, 92117, 8123, 572, 10660, 369, 1817, 18572, 13, 4058, 10155, 92117, 8123, 572, 29139, 304, 264, 71710, 30549, 1614, 311, 7023, 8123, 315, 12654, 553, 83270, 1220, 5456, 369, 1817, 68666, 13, 77437, 1515, 7077, 25588, 572, 1483, 311, 8253, 279, 12687, 1948, 279, 83270, 1220, 5456, 323, 92117, 8123, 13, 576, 8123, 369, 1290, 68666, 12654, 572, 46852, 7046, 1091, 279, 8123, 369, 2115, 68666, 52472, 11, 42368, 369, 279, 25869, 83270, 1220, 320, 47, 27, 15, 13, 220, 15, 15, 16, 568, 1752, 1817, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 5456, 27, 17, 15, 11, 279, 22553, 8123, 315, 1290, 68666, 52472, 572, 13187, 1990, 279, 22553, 8123, 315, 2115, 68666, 52472, 13, 1752, 3110, 11, 369, 6835, 448, 264, 2115, 68666, 12654, 323, 264, 220, 17, 19, 21231, 83270, 1220, 5456, 315, 220, 16, 21, 311, 220, 17, 15, 11, 279, 22553, 8123, 315, 59645, 4132, 277, 407, 572, 220, 19, 23, 64070, 320, 2245, 446, 471, 457, 2088, 220, 16, 19, 311, 220, 16, 16, 16, 64070, 8, 438, 7707, 448, 220, 16, 18, 18, 64070, 320, 2245, 446, 471, 457, 2088, 220, 23, 16, 311, 220, 17, 15, 23, 64070, 8, 369, 6835, 448, 264, 1290, 68666, 12654, 320, 47, 27, 15, 13, 15, 15, 16, 568, 576, 22553, 8123, 315, 264, 1290, 68666, 12654, 572, 17267, 6144, 311, 279, 22553, 8123, 315, 264, 2115, 68666, 12654, 304, 279, 1790, 8426, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 13, 576, 77437, 1515, 7077, 25588, 1948, 279, 220, 17, 19, 21231, 83270, 1220, 5456, 323, 220, 18, 22289, 92117, 8123, 572, 220, 15, 13, 22, 17, 369, 6835, 448, 2115, 68666, 12654, 323, 220, 15, 13, 22, 16, 369, 6835, 448, 1290, 68666, 12654, 13, 151646, 198, 27, 91, 77091, 91, 397]\n",
            "inputs:\n",
            "<|user|>\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-07-24 02:14:12,577 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-24 02:14:12,579 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/24/2024 02:14:12 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-24 02:14:12,612 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-24 02:14:12,622 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 02:14:12,624 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-24 02:14:13,698 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-24 02:14:13,699 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-24 02:14:13,891 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-24 02:14:13,892 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/24/2024 02:14:13 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/24/2024 02:14:16 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/24/2024 02:14:16 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/Qwen05_LoRA_Rank/train_Rank16\n",
            "07/24/2024 02:14:16 - INFO - llamafactory.model.loader - all params: 494,032,768\n",
            "[INFO|trainer.py:3788] 2024-07-24 02:14:16,337 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-24 02:14:16,337 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-24 02:14:16,337 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-24 02:14:39,500 >> We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            " 15% 5/34 [01:14<06:50, 14.14s/it]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 115, in run_sft\n",
            "    predict_results = trainer.predict(dataset_module[\"eval_dataset\"], metric_key_prefix=\"predict\", **gen_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 244, in predict\n",
            "    return super().predict(test_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3717, in predict\n",
            "    output = eval_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3826, in evaluation_loop\n",
            "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/trainer.py\", line 99, in prediction_step\n",
            "    loss, generated_tokens, _ = super().prediction_step(  # ignore the returned labels (may be truncated)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer_seq2seq.py\", line 310, in prediction_step\n",
            "    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 1914, in generate\n",
            "    result = self._sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2693, in _sample\n",
            "    next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
            "KeyboardInterrupt\n",
            " 15% 5/34 [01:37<09:27, 19.57s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-05 lora qwen15"
      ],
      "metadata": {
        "id": "mDGYy0nMD43r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank16 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ag_Y_ZzDIJ4",
        "outputId": "a7b9d770-e74c-4622-b9ca-80495cb88838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 16:10:28.579548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 16:10:28.579593: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 16:10:28.581021: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 16:10:28.588794: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 16:10:29.732454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 16:10:36 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,564 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,564 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,565 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,565 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,565 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:10:36,565 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 16:10:36,800 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 16:10:36 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 16:10:36 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 16:10:37 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:10:38,475 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:10:38,476 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 16:10:38,506 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 16:10:38,517 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:10:38,519 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 16:10:41,062 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 16:10:41,062 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 16:10:41,259 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:10:41,260 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,k_proj,o_proj,q_proj,down_proj,v_proj\n",
            "07/23/2024 16:10:41 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
            "[INFO|trainer.py:642] 2024-07-23 16:10:41,929 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 16:10:42,360 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 16:10:42,360 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 16:10:42,361 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 16:10:42,361 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 16:10:42,361 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 16:10:42,361 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 16:10:42,361 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 16:10:42,365 >>   Number of trainable parameters = 18,464,768\n",
            "{'loss': 2.0088, 'grad_norm': 0.7621185183525085, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.869, 'grad_norm': 0.4546549916267395, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:11<14:06, 19.24s/it][INFO|trainer.py:3788] 2024-07-23 16:13:54,213 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:13:54,213 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:13:54,213 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.67it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.00it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.69it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.14it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.59it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.75it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.28it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.26it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.19it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.66it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.93it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.33it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:06,  5.49it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  5.98it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.93it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.66it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.87it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  4.97it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.87it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.21it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.54it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.87it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.23it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.88it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.59it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.47it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.09it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.11it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.05it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.01it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.00it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  3.98it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.43it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.11it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.47it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.68it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.37it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.30it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.34it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.88it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.302522897720337, 'eval_runtime': 10.9341, 'eval_samples_per_second': 9.146, 'eval_steps_per_second': 4.573, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:22<14:06, 19.24s/it]\n",
            "100% 50/50 [00:10<00:00,  4.10it/s]\u001b[A\n",
            " 20% 11/54 [03:41<15:59, 22.32s/it]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 94, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1932, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2268, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3307, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3338, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1430, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 179, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/qwen2/modeling_qwen2.py\", line 1249, in forward\n",
            "    loss = loss_fct(shift_logits, shift_labels)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\", line 1185, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3086, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.71 GiB. GPU \n",
            " 20% 11/54 [03:49<14:56, 20.85s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/eval_LoRA_Rank16_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank16"
      ],
      "metadata": {
        "id": "j6xXZJA2GLjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/eval_LoRA_Rank16_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T_0OEI-GLlR",
        "outputId": "deff4b70-bd9a-4b87-b9db-e53fb8f8a450"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-23 17:57:00.925347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 17:57:00.925408: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 17:57:00.926663: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 17:57:00.933319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 17:57:02.090734: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 17:57:08 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 17:57:08,838 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 17:57:09,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 17:57:09 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 17:57:09 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 17:57:09 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[27, 91, 872, 91, 397, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 12553, 279, 5055, 72728, 315, 6267, 68934, 24613, 4694, 2115, 68666, 52472, 5267, 98365, 50, 25, 576, 5055, 72728, 315, 6267, 68934, 24613, 320, 14912, 39, 1220, 8, 374, 264, 2697, 11, 52723, 78285, 5452, 429, 10953, 63808, 28187, 13, 4940, 220, 19, 17, 3204, 3501, 11, 220, 22, 3501, 525, 5961, 5435, 311, 18662, 315, 4128, 7707, 448, 1172, 220, 17, 3501, 5435, 311, 38232, 13, 1205, 24109, 279, 42615, 6773, 315, 279, 451, 5245, 50, 259, 12, 8041, 12654, 9091, 311, 1273, 279, 30078, 429, 279, 2790, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 1290, 68666, 52472, 1035, 387, 7046, 1091, 279, 8123, 315, 59645, 4132, 277, 407, 304, 6835, 448, 2115, 68666, 52472, 879, 614, 4428, 83270, 1220, 12205, 13, 576, 8123, 315, 12654, 572, 10838, 553, 6366, 1506, 2168, 6358, 315, 18572, 12351, 323, 18572, 5335, 9768, 389, 6366, 16971, 323, 28293, 53758, 13, 42592, 39214, 17991, 315, 92117, 8123, 572, 10660, 369, 1817, 18572, 13, 4058, 10155, 92117, 8123, 572, 29139, 304, 264, 71710, 30549, 1614, 311, 7023, 8123, 315, 12654, 553, 83270, 1220, 5456, 369, 1817, 68666, 13, 77437, 1515, 7077, 25588, 572, 1483, 311, 8253, 279, 12687, 1948, 279, 83270, 1220, 5456, 323, 92117, 8123, 13, 576, 8123, 369, 1290, 68666, 12654, 572, 46852, 7046, 1091, 279, 8123, 369, 2115, 68666, 52472, 11, 42368, 369, 279, 25869, 83270, 1220, 320, 47, 27, 15, 13, 220, 15, 15, 16, 568, 1752, 1817, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 5456, 27, 17, 15, 11, 279, 22553, 8123, 315, 1290, 68666, 52472, 572, 13187, 1990, 279, 22553, 8123, 315, 2115, 68666, 52472, 13, 1752, 3110, 11, 369, 6835, 448, 264, 2115, 68666, 12654, 323, 264, 220, 17, 19, 21231, 83270, 1220, 5456, 315, 220, 16, 21, 311, 220, 17, 15, 11, 279, 22553, 8123, 315, 59645, 4132, 277, 407, 572, 220, 19, 23, 64070, 320, 2245, 446, 471, 457, 2088, 220, 16, 19, 311, 220, 16, 16, 16, 64070, 8, 438, 7707, 448, 220, 16, 18, 18, 64070, 320, 2245, 446, 471, 457, 2088, 220, 23, 16, 311, 220, 17, 15, 23, 64070, 8, 369, 6835, 448, 264, 1290, 68666, 12654, 320, 47, 27, 15, 13, 15, 15, 16, 568, 576, 22553, 8123, 315, 264, 1290, 68666, 12654, 572, 17267, 6144, 311, 279, 22553, 8123, 315, 264, 2115, 68666, 12654, 304, 279, 1790, 8426, 220, 20, 16574, 5582, 315, 279, 83270, 1220, 13, 576, 77437, 1515, 7077, 25588, 1948, 279, 220, 17, 19, 21231, 83270, 1220, 5456, 323, 220, 18, 22289, 92117, 8123, 572, 220, 15, 13, 22, 17, 369, 6835, 448, 2115, 68666, 12654, 323, 220, 15, 13, 22, 16, 369, 6835, 448, 1290, 68666, 12654, 13, 151646, 198, 27, 91, 77091, 91, 397]\n",
            "inputs:\n",
            "<|user|>\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|>\n",
            "<|assistant|>\n",
            "\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 17:57:10,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 17:57:10,095 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 17:57:10 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 17:57:10,125 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 17:57:10,135 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:57:10,137 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 17:57:11,078 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 17:57:11,078 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 17:57:11,306 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 17:57:11,306 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 17:57:11 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/config.py\", line 197, in _get_peft_type\n",
            "    config_file = hf_hub_download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
            "    validate_repo_id(arg_value)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 154, in validate_repo_id\n",
            "    raise HFValidationError(\n",
            "huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank16'. Use `repo_type` argument if needed.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 111, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
            "    model = load_model(tokenizer, model_args, finetuning_args, training_args.do_train)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/loader.py\", line 162, in load_model\n",
            "    model = init_adapter(config, model, model_args, finetuning_args, is_trainable)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/adapter.py\", line 310, in init_adapter\n",
            "    model = _setup_lora_tuning(\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/model/adapter.py\", line 191, in _setup_lora_tuning\n",
            "    model: \"LoraModel\" = PeftModel.from_pretrained(model, adapter, **init_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 372, in from_pretrained\n",
            "    PeftConfig._get_peft_type(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/config.py\", line 203, in _get_peft_type\n",
            "    raise ValueError(f\"Can't find '{CONFIG_NAME}' at '{model_id}'\")\n",
            "ValueError: Can't find 'adapter_config.json' at '/content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank16'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-04 lora qwen15"
      ],
      "metadata": {
        "id": "QmJL6iH7EECc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qndKKQ1fC6tK",
        "outputId": "ef92874d-d224-4245-a498-50c853e88556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 16:40:03.287806: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 16:40:03.287853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 16:40:03.289167: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 16:40:03.296199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 16:40:04.439951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 16:40:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 16:40:11,117 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 16:40:11,355 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 16:40:11 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 16:40:11 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 16:40:11 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:40:12,997 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:40:12,998 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 16:40:13,028 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 16:40:13,039 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:40:13,041 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 16:40:15,429 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 16:40:15,429 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 16:40:15,664 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 16:40:15,665 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 16:40:15 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 16:40:15 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 16:40:15 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 16:40:15 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 16:40:15 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,gate_proj,q_proj,v_proj,down_proj,o_proj\n",
            "07/23/2024 16:40:16 - INFO - llamafactory.model.loader - trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n",
            "[INFO|trainer.py:642] 2024-07-23 16:40:16,240 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 16:40:16,676 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 16:40:16,677 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 16:40:16,677 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 16:40:16,677 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 16:40:16,677 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 16:40:16,677 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 16:40:16,677 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 16:40:16,681 >>   Number of trainable parameters = 9,232,384\n",
            "{'loss': 1.7766, 'grad_norm': 0.5148631930351257, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.6187, 'grad_norm': 0.37401697039604187, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:11<14:04, 19.19s/it][INFO|trainer.py:3788] 2024-07-23 16:43:28,104 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:43:28,104 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:43:28,104 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.59it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.04it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.72it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.19it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.65it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.78it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.30it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.29it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.23it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.67it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.95it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.36it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:05,  5.51it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  6.00it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.91it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.70it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.93it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  5.01it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.89it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.23it/s]\u001b[A\n",
            " 50% 25/50 [00:04<00:05,  4.55it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.88it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.23it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.86it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.57it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.49it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.09it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.12it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.05it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.01it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  3.97it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.41it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.10it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.51it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.45it/s]\u001b[A\n",
            " 86% 43/50 [00:08<00:01,  4.67it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.66it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.38it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.31it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.33it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.87it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.92it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0946135520935059, 'eval_runtime': 10.907, 'eval_samples_per_second': 9.168, 'eval_steps_per_second': 4.584, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:22<14:04, 19.19s/it]\n",
            "100% 50/50 [00:10<00:00,  4.10it/s]\u001b[A\n",
            "{'loss': 1.543, 'grad_norm': 0.40800777077674866, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.3971, 'grad_norm': 0.30211973190307617, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [06:26<10:42, 18.90s/it][INFO|trainer.py:3788] 2024-07-23 16:46:43,260 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:46:43,260 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:46:43,260 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.34it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.02it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.71it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.20it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.62it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.76it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.30it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.30it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.23it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.67it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.94it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.35it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:06,  5.49it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  5.96it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.90it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.69it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.91it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  5.01it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.89it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.22it/s]\u001b[A\n",
            " 50% 25/50 [00:04<00:05,  4.54it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.89it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.25it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.88it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.59it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.49it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.09it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.12it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.06it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.03it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  3.98it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.44it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.14it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.53it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.47it/s]\u001b[A\n",
            " 86% 43/50 [00:08<00:01,  4.68it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.66it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.37it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.28it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.33it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.89it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0847820043563843, 'eval_runtime': 10.9042, 'eval_samples_per_second': 9.171, 'eval_steps_per_second': 4.585, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [06:37<10:42, 18.90s/it]\n",
            "100% 50/50 [00:10<00:00,  4.10it/s]\u001b[A\n",
            "{'loss': 1.3918, 'grad_norm': 0.36440509557724, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.4145, 'grad_norm': 0.36013439297676086, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [09:42<07:31, 18.79s/it][INFO|trainer.py:3788] 2024-07-23 16:49:59,051 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:49:59,051 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:49:59,052 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.32it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.11it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.71it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.16it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.55it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.74it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.30it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.27it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.20it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.68it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.52it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.93it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.33it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:06,  5.49it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  6.02it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.95it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.68it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.89it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  4.98it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.89it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.25it/s]\u001b[A\n",
            " 50% 25/50 [00:04<00:05,  4.54it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.89it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.25it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.87it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.58it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.49it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.10it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.14it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.08it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.04it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.48it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.15it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.54it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.51it/s]\u001b[A\n",
            " 86% 43/50 [00:08<00:01,  4.72it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.68it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.38it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.30it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.34it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.87it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.91it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0853278636932373, 'eval_runtime': 10.8963, 'eval_samples_per_second': 9.177, 'eval_steps_per_second': 4.589, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [09:53<07:31, 18.79s/it]\n",
            "100% 50/50 [00:10<00:00,  4.08it/s]\u001b[A\n",
            "{'loss': 1.3883, 'grad_norm': 0.4223986268043518, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.3282, 'grad_norm': 0.4206138253211975, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [13:04<04:26, 19.03s/it][INFO|trainer.py:3788] 2024-07-23 16:53:20,904 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:53:20,904 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:53:20,904 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.75it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.04it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.68it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.17it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.57it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.75it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.28it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.27it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.19it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.68it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.50it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.93it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.33it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:06,  5.49it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  5.99it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.94it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.70it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.89it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  4.99it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.87it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.23it/s]\u001b[A\n",
            " 50% 25/50 [00:04<00:05,  4.59it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.90it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.26it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.91it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.62it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.52it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.12it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.15it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.09it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.04it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.05it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  4.03it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.15it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.54it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.43it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.49it/s]\u001b[A\n",
            " 86% 43/50 [00:08<00:01,  4.70it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.67it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.39it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.31it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.34it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.88it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.92it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0940686464309692, 'eval_runtime': 10.8822, 'eval_samples_per_second': 9.189, 'eval_steps_per_second': 4.595, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [13:15<04:26, 19.03s/it]\n",
            "100% 50/50 [00:10<00:00,  4.10it/s]\u001b[A\n",
            "{'loss': 1.2176, 'grad_norm': 0.4770277142524719, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.2523, 'grad_norm': 0.5108922719955444, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [16:21<01:13, 18.29s/it][INFO|trainer.py:3788] 2024-07-23 16:56:38,008 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:56:38,008 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:56:38,008 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:02, 16.57it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:07,  6.11it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:07,  5.70it/s]\u001b[A\n",
            " 12% 6/50 [00:00<00:07,  6.18it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:07,  5.60it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.76it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.28it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:09,  4.27it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:09,  4.21it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.68it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.50it/s]\u001b[A\n",
            " 30% 15/50 [00:02<00:07,  4.94it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:06,  5.36it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:05,  5.51it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:05,  6.03it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.95it/s]\u001b[A\n",
            " 40% 20/50 [00:03<00:06,  4.68it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:05,  4.90it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  5.01it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.88it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:04,  5.24it/s]\u001b[A\n",
            " 50% 25/50 [00:04<00:05,  4.56it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.54it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.89it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  5.23it/s]\u001b[A\n",
            " 58% 29/50 [00:05<00:04,  4.87it/s]\u001b[A\n",
            " 60% 30/50 [00:05<00:03,  5.57it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:03,  5.51it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.09it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:04,  4.12it/s]\u001b[A\n",
            " 68% 34/50 [00:06<00:03,  4.07it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.02it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:03,  4.00it/s]\u001b[A\n",
            " 76% 38/50 [00:07<00:02,  4.46it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.12it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.53it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.49it/s]\u001b[A\n",
            " 86% 43/50 [00:08<00:01,  4.70it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.69it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.41it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.31it/s]\u001b[A\n",
            " 94% 47/50 [00:09<00:00,  4.36it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  3.88it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  3.93it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0992857217788696, 'eval_runtime': 10.8899, 'eval_samples_per_second': 9.183, 'eval_steps_per_second': 4.591, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [16:32<01:13, 18.29s/it]\n",
            "100% 50/50 [00:10<00:00,  4.10it/s]\u001b[A\n",
            "100% 54/54 [17:47<00:00, 19.96s/it][INFO|trainer.py:3478] 2024-07-23 16:58:04,547 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:58:04,981 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:58:04,981 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:58:05,127 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:58:05,131 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 16:58:05,644 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1068.9636, 'train_samples_per_second': 2.526, 'train_steps_per_second': 0.051, 'train_loss': 1.4200330487004034, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [17:48<00:00, 19.80s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 16:58:05,649 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:58:06,079 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:58:06,080 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:58:06,230 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:58:06,235 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  9648678GF\n",
            "  train_loss               =       1.42\n",
            "  train_runtime            = 0:17:48.96\n",
            "  train_samples_per_second =      2.526\n",
            "  train_steps_per_second   =      0.051\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-04/training_eval_loss.png\n",
            "07/23/2024 16:58:06 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 16:58:06,724 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:58:06,724 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:58:06,724 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.0994\n",
            "  eval_runtime            = 0:00:10.94\n",
            "  eval_samples_per_second =      9.133\n",
            "  eval_steps_per_second   =      4.567\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 16:58:17,686 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-04 lora qwen15"
      ],
      "metadata": {
        "id": "38vqgBGiEMRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_LoRA_Rank/train_Rank8_5e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "id": "M72F89Z0DAYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-04 NF8 qlora qwen15"
      ],
      "metadata": {
        "id": "y1STRTswESZY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoF7d188CfP5",
        "outputId": "52f32484-4fa1-41d9-f2b5-6aa9f0bb7819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 15:47:23.439483: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 15:47:23.439540: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 15:47:23.440875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 15:47:23.448192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 15:47:24.584174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 15:47:30 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/23/2024 15:47:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:47:31,200 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 15:47:31,435 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 15:47:31 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 15:47:31 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 15:47:32 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:47:33,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:47:33,062 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 15:47:33 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 15:47:33,093 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 15:47:33,103 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:47:33,105 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 15:47:35,849 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 15:47:35,849 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 15:47:36,045 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:47:36,046 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,down_proj,up_proj,q_proj,gate_proj,v_proj,k_proj\n",
            "07/23/2024 15:47:36 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
            "[INFO|trainer.py:642] 2024-07-23 15:47:36,653 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 15:47:37,081 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 15:47:37,081 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 15:47:37,081 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 15:47:37,081 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 15:47:37,081 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 15:47:37,081 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 15:47:37,081 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 15:47:37,085 >>   Number of trainable parameters = 18,464,768\n",
            "{'loss': 1.7803, 'grad_norm': 0.39569443464279175, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.6224, 'grad_norm': 0.26806172728538513, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:54<17:12, 23.47s/it][INFO|trainer.py:3788] 2024-07-23 15:51:31,470 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:51:31,471 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:51:31,471 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.25it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:11,  4.14it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:14,  3.13it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.97it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:14,  2.98it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.84it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.61it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.47it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.49it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:15,  2.47it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:14,  2.55it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.60it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.56it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.66it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.73it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.72it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.79it/s]\u001b[A\n",
            " 38% 19/50 [00:06<00:12,  2.58it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:11,  2.51it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.57it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.58it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
            " 48% 24/50 [00:08<00:09,  2.62it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.50it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.50it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:08,  2.58it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.66it/s]\u001b[A\n",
            " 58% 29/50 [00:10<00:08,  2.59it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.71it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.70it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 68% 34/50 [00:12<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.33it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.34it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.37it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.41it/s]\u001b[A\n",
            " 82% 41/50 [00:15<00:03,  2.42it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.47it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.52it/s]\u001b[A\n",
            " 88% 44/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.46it/s]\u001b[A\n",
            " 92% 46/50 [00:17<00:01,  2.41it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.31it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.34it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0978825092315674, 'eval_runtime': 19.8904, 'eval_samples_per_second': 5.028, 'eval_steps_per_second': 2.514, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [04:14<17:12, 23.47s/it]\n",
            "100% 50/50 [00:19<00:00,  2.41it/s]\u001b[A\n",
            "{'loss': 1.5449, 'grad_norm': 0.2863648235797882, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.3985, 'grad_norm': 0.22114509344100952, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [08:00<13:11, 23.27s/it][INFO|trainer.py:3788] 2024-07-23 15:55:37,859 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:55:37,859 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:55:37,859 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.65it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.82it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.96it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.85it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.87it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.74it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.55it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.50it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.51it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.47it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.55it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.63it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.60it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.69it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.51it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.47it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.53it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.54it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.48it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.49it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.61it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.54it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.63it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.62it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.29it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.33it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.32it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.33it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.45it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.48it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.48it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.51it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.56it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.58it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.48it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.44it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.32it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.34it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0860637426376343, 'eval_runtime': 20.1812, 'eval_samples_per_second': 4.955, 'eval_steps_per_second': 2.478, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [08:20<13:11, 23.27s/it]\n",
            "100% 50/50 [00:19<00:00,  2.40it/s]\u001b[A\n",
            "{'loss': 1.3984, 'grad_norm': 0.2578218877315521, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.4207, 'grad_norm': 0.25872549414634705, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:07<09:15, 23.15s/it][INFO|trainer.py:3788] 2024-07-23 15:59:44,747 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:59:44,747 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:59:44,747 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.86it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.88it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.99it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.84it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.84it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.54it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.49it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.52it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.47it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.56it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.62it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.64it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.72it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.51it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.47it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.52it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.51it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.48it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.54it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.44it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.46it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.54it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.56it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.66it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.36it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.36it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.36it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.45it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.47it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.47it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.49it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.46it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.42it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.30it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.30it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.08504319190979, 'eval_runtime': 20.2203, 'eval_samples_per_second': 4.946, 'eval_steps_per_second': 2.473, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:27<09:15, 23.15s/it]\n",
            "100% 50/50 [00:19<00:00,  2.35it/s]\u001b[A\n",
            "{'loss': 1.3931, 'grad_norm': 0.30225107073783875, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.334, 'grad_norm': 0.3010861575603485, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:21<05:27, 23.41s/it][INFO|trainer.py:3788] 2024-07-23 16:03:58,585 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:03:58,585 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:03:58,585 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.94it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.88it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.98it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.74it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.55it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.42it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.50it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.51it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.47it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.56it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.63it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.66it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.74it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.55it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:11,  2.51it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.57it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.57it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:09,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.48it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.48it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.54it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.61it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.54it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.36it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.36it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.36it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.46it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.41it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.49it/s]\u001b[A\n",
            " 82% 41/50 [00:15<00:03,  2.49it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.52it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.57it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.58it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.48it/s]\u001b[A\n",
            " 92% 46/50 [00:17<00:01,  2.42it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.30it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.33it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0955898761749268, 'eval_runtime': 20.0766, 'eval_samples_per_second': 4.981, 'eval_steps_per_second': 2.49, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:41<05:27, 23.41s/it]\n",
            "100% 50/50 [00:19<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 1.2256, 'grad_norm': 0.3414445221424103, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.2637, 'grad_norm': 0.35775741934776306, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:30<01:30, 22.65s/it][INFO|trainer.py:3788] 2024-07-23 16:08:07,598 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:08:07,598 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:08:07,598 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.90it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.87it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.97it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.85it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.85it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.54it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.38it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.38it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.42it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:15,  2.44it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.41it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.51it/s]\u001b[A\n",
            " 32% 16/50 [00:06<00:13,  2.59it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.62it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.70it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.52it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.48it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.54it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.53it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.48it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.48it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:08,  2.56it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.64it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.58it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.68it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.66it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.39it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.46it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.45it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.47it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.52it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.53it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.44it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.40it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.40it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.28it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.30it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0978139638900757, 'eval_runtime': 20.2582, 'eval_samples_per_second': 4.936, 'eval_steps_per_second': 2.468, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:50<01:30, 22.65s/it]\n",
            "100% 50/50 [00:19<00:00,  2.36it/s]\u001b[A\n",
            "100% 54/54 [22:23<00:00, 25.27s/it][INFO|trainer.py:3478] 2024-07-23 16:10:00,807 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:10:01,256 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:10:01,257 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:10:01,504 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:10:01,508 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 16:10:02,246 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1345.1613, 'train_samples_per_second': 2.007, 'train_steps_per_second': 0.04, 'train_loss': 1.4255138503180609, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [22:25<00:00, 24.91s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 16:10:02,251 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 16:10:02,703 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 16:10:02,703 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 16:10:02,937 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 16:10:02,943 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  9716185GF\n",
            "  train_loss               =     1.4255\n",
            "  train_runtime            = 0:22:25.16\n",
            "  train_samples_per_second =      2.007\n",
            "  train_steps_per_second   =       0.04\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-04/training_eval_loss.png\n",
            "07/23/2024 16:10:03 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 16:10:03,445 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 16:10:03,445 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 16:10:03,445 >>   Batch size = 2\n",
            "100% 50/50 [00:19<00:00,  2.53it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =      1.099\n",
            "  eval_runtime            = 0:00:20.20\n",
            "  eval_samples_per_second =      4.948\n",
            "  eval_steps_per_second   =      2.474\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 16:10:23,668 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-05 NF8 qlora qwen15"
      ],
      "metadata": {
        "id": "7IGR_nEUIIaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "w6qUxx87OH5I",
        "outputId": "09d9141e-eee5-485f-96d4-b544db0c7755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 15:24:17.838904: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 15:24:17.838951: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 15:24:17.840344: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 15:24:17.847715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 15:24:18.997688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 15:24:25 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/23/2024 15:24:25 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:24:25,565 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 15:24:25,799 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 15:24:25 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 15:24:25 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 15:24:26 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:24:27,486 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:24:27,487 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 15:24:27 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 15:24:27,518 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 15:24:27,529 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:24:27,530 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 15:24:30,296 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 15:24:30,296 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 15:24:30,521 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:24:30,522 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 15:24:30 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 15:24:30 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 15:24:30 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 15:24:30 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 15:24:30 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,up_proj,v_proj,down_proj,gate_proj,o_proj\n",
            "07/23/2024 15:24:31 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
            "[INFO|trainer.py:642] 2024-07-23 15:24:31,129 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 15:24:31,549 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 15:24:31,549 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 15:24:31,550 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 15:24:31,550 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 15:24:31,550 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 15:24:31,550 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 15:24:31,550 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 15:24:31,554 >>   Number of trainable parameters = 18,464,768\n",
            "{'loss': 2.0062, 'grad_norm': 0.754160463809967, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.8725, 'grad_norm': 0.4516548216342926, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:54<17:13, 23.49s/it][INFO|trainer.py:3788] 2024-07-23 15:28:25,961 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:28:25,962 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:28:25,962 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.19it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:11,  4.08it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:14,  3.10it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.95it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:14,  2.97it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.84it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:15,  2.64it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.52it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:15,  2.53it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:15,  2.49it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.58it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.53it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.64it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.72it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.71it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.78it/s]\u001b[A\n",
            " 38% 19/50 [00:06<00:12,  2.57it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:11,  2.51it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.56it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.57it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            " 48% 24/50 [00:08<00:09,  2.61it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.49it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.49it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
            " 58% 29/50 [00:10<00:08,  2.55it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.64it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.35it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 68% 34/50 [00:12<00:06,  2.33it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.35it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.45it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.38it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.45it/s]\u001b[A\n",
            " 82% 41/50 [00:15<00:03,  2.45it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.48it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.43it/s]\u001b[A\n",
            " 92% 46/50 [00:17<00:01,  2.40it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.42it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.29it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.31it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3047665357589722, 'eval_runtime': 19.9837, 'eval_samples_per_second': 5.004, 'eval_steps_per_second': 2.502, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [04:14<17:13, 23.49s/it]\n",
            "100% 50/50 [00:19<00:00,  2.37it/s]\u001b[A\n",
            "{'loss': 1.6987, 'grad_norm': 0.4580146074295044, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.5076, 'grad_norm': 0.3374615013599396, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [08:00<13:10, 23.26s/it][INFO|trainer.py:3788] 2024-07-23 15:32:32,387 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:32:32,387 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:32:32,387 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.85it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.88it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.96it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.84it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.84it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.54it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.49it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.51it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.46it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.54it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:13,  2.61it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.63it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.72it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.52it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.47it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.53it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.53it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.51it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.55it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.44it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.46it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.54it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:06,  2.33it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.33it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.43it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.39it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.46it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.46it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.49it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.46it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.41it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.30it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.31it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1792004108428955, 'eval_runtime': 20.2378, 'eval_samples_per_second': 4.941, 'eval_steps_per_second': 2.471, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [08:21<13:10, 23.26s/it]\n",
            "100% 50/50 [00:19<00:00,  2.36it/s]\u001b[A\n",
            "{'loss': 1.5721, 'grad_norm': 0.332785427570343, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.6025, 'grad_norm': 0.26914942264556885, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:07<09:15, 23.15s/it][INFO|trainer.py:3788] 2024-07-23 15:36:39,559 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:36:39,559 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:36:39,559 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.77it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.81it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.94it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.83it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.84it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:16,  2.68it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.52it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:17,  2.40it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.40it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.38it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.46it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.48it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.43it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.51it/s]\u001b[A\n",
            " 32% 16/50 [00:06<00:13,  2.58it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.61it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.68it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.49it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.46it/s]\u001b[A\n",
            " 42% 21/50 [00:08<00:11,  2.51it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.53it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.52it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.55it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.45it/s]\u001b[A\n",
            " 52% 26/50 [00:10<00:09,  2.45it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.53it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.61it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.56it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.67it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.66it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.38it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.45it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.45it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.48it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.52it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.52it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.42it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.39it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.41it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.28it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.30it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.136906623840332, 'eval_runtime': 20.3216, 'eval_samples_per_second': 4.921, 'eval_steps_per_second': 2.46, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:28<09:15, 23.15s/it]\n",
            "100% 50/50 [00:19<00:00,  2.37it/s]\u001b[A\n",
            "{'loss': 1.5937, 'grad_norm': 0.2839964032173157, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.5653, 'grad_norm': 0.26731541752815247, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:21<05:27, 23.39s/it][INFO|trainer.py:3788] 2024-07-23 15:40:53,408 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:40:53,408 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:40:53,408 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.75it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.81it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.94it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.81it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.81it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.52it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:17,  2.39it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.41it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.39it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.48it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.49it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.44it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.52it/s]\u001b[A\n",
            " 32% 16/50 [00:06<00:13,  2.57it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.59it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.67it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.47it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.41it/s]\u001b[A\n",
            " 42% 21/50 [00:08<00:11,  2.48it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.51it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.50it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.57it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.46it/s]\u001b[A\n",
            " 52% 26/50 [00:10<00:09,  2.46it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.53it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.61it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.55it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.64it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.35it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.33it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.39it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.47it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.47it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.50it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.55it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.43it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.38it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.39it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.26it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.28it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1259208917617798, 'eval_runtime': 20.3632, 'eval_samples_per_second': 4.911, 'eval_steps_per_second': 2.455, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:42<05:27, 23.39s/it]\n",
            "100% 50/50 [00:19<00:00,  2.35it/s]\u001b[A\n",
            "{'loss': 1.4874, 'grad_norm': 0.2993106245994568, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.5424, 'grad_norm': 0.31654787063598633, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:30<01:30, 22.63s/it][INFO|trainer.py:3788] 2024-07-23 15:45:02,516 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:45:02,516 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:45:02,516 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.72it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.81it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.94it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:16,  2.80it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.76it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:16,  2.65it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.48it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:17,  2.38it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.38it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.37it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.44it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.47it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.43it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.52it/s]\u001b[A\n",
            " 32% 16/50 [00:06<00:13,  2.61it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.63it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.71it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.51it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.48it/s]\u001b[A\n",
            " 42% 21/50 [00:08<00:11,  2.53it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.54it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.52it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.57it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.47it/s]\u001b[A\n",
            " 52% 26/50 [00:10<00:09,  2.47it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.53it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.56it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.64it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.35it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.33it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.29it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:06,  2.30it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.31it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.42it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.37it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.45it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.46it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.49it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.54it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.45it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.41it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.42it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.30it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.32it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.122622013092041, 'eval_runtime': 20.3475, 'eval_samples_per_second': 4.915, 'eval_steps_per_second': 2.457, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:51<01:30, 22.63s/it]\n",
            "100% 50/50 [00:19<00:00,  2.38it/s]\u001b[A\n",
            "100% 54/54 [22:24<00:00, 25.27s/it][INFO|trainer.py:3478] 2024-07-23 15:46:55,804 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:46:56,245 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:46:56,246 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:46:56,486 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:46:56,490 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 15:46:57,231 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1345.678, 'train_samples_per_second': 2.006, 'train_steps_per_second': 0.04, 'train_loss': 1.6374295640874792, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [22:25<00:00, 24.92s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 15:46:57,236 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:46:57,689 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:46:57,690 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:46:57,938 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:46:57,942 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  9716185GF\n",
            "  train_loss               =     1.6374\n",
            "  train_runtime            = 0:22:25.67\n",
            "  train_samples_per_second =      2.006\n",
            "  train_steps_per_second   =       0.04\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank8_NF8_5e-05/training_eval_loss.png\n",
            "07/23/2024 15:46:58 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 15:46:58,425 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:46:58,425 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:46:58,425 >>   Batch size = 2\n",
            "100% 50/50 [00:19<00:00,  2.53it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.1223\n",
            "  eval_runtime            = 0:00:20.14\n",
            "  eval_samples_per_second =      4.964\n",
            "  eval_steps_per_second   =      2.482\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 15:47:18,590 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-05 NF8 qlora qwen15"
      ],
      "metadata": {
        "id": "0bAd0vQiIMn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "R8iUJ9--Oex8",
        "outputId": "979b3cd8-3773-414b-dd88-8b250b54189f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 14:36:22.832865: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 14:36:22.832914: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 14:36:22.834410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 14:36:22.841859: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 14:36:23.994277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 14:36:30 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/23/2024 14:36:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,489 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,489 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,489 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,489 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,490 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:36:30,490 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 14:36:30,738 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 14:36:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 14:36:30 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 14:36:31 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 14:36:32,738 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 14:36:32,739 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-1.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 14:36:32 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 14:36:32,768 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 14:36:32,779 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 14:36:32,781 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 14:36:35,604 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 14:36:35,604 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 14:36:35,801 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 14:36:35,802 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 14:36:35 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 14:36:35 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 14:36:35 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 14:36:35 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 14:36:35 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,q_proj,up_proj,down_proj,gate_proj,k_proj,v_proj\n",
            "07/23/2024 14:36:36 - INFO - llamafactory.model.loader - trainable params: 18,464,768 || all params: 1,562,179,072 || trainable%: 1.1820\n",
            "[INFO|trainer.py:642] 2024-07-23 14:36:36,399 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 14:36:36,818 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 14:36:36,818 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 14:36:36,818 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 14:36:36,818 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 14:36:36,818 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 14:36:36,818 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 14:36:36,818 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 14:36:36,822 >>   Number of trainable parameters = 18,464,768\n",
            "{'loss': 2.0075, 'grad_norm': 0.7597557902336121, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.8724, 'grad_norm': 0.45342373847961426, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [03:52<17:10, 23.43s/it][INFO|trainer.py:3788] 2024-07-23 14:40:29,292 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:40:29,292 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:40:29,292 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.24it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:11,  4.11it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:14,  3.12it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.99it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:14,  3.02it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:14,  2.88it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:15,  2.65it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.50it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.49it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:15,  2.46it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.51it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.54it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.52it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.62it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.70it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.70it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.76it/s]\u001b[A\n",
            " 38% 19/50 [00:06<00:12,  2.52it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.48it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.54it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.54it/s]\u001b[A\n",
            " 48% 24/50 [00:08<00:10,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.50it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.48it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:08,  2.56it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.66it/s]\u001b[A\n",
            " 58% 29/50 [00:10<00:08,  2.59it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.69it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.67it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.39it/s]\u001b[A\n",
            " 68% 34/50 [00:12<00:06,  2.36it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.36it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.37it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.37it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.47it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.46it/s]\u001b[A\n",
            " 82% 41/50 [00:15<00:03,  2.44it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.46it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.50it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.52it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.43it/s]\u001b[A\n",
            " 92% 46/50 [00:17<00:01,  2.40it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.41it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.28it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.32it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.304753065109253, 'eval_runtime': 19.9596, 'eval_samples_per_second': 5.01, 'eval_steps_per_second': 2.505, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [04:12<17:10, 23.43s/it]\n",
            "100% 50/50 [00:19<00:00,  2.38it/s]\u001b[A\n",
            "{'loss': 1.6994, 'grad_norm': 0.46547985076904297, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.5073, 'grad_norm': 0.33727267384529114, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [07:58<13:11, 23.28s/it][INFO|trainer.py:3788] 2024-07-23 14:44:35,804 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:44:35,804 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:44:35,804 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.44it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.78it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.93it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:16,  2.77it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.80it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.53it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.48it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.50it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.46it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.53it/s]\u001b[A\n",
            " 32% 16/50 [00:06<00:13,  2.61it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.62it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.71it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.53it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.49it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.55it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.54it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:09,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.49it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.47it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.64it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.57it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.67it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.66it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.39it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.32it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.33it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.34it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.46it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.46it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.48it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.53it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.54it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.46it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.42it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.42it/s]\u001b[A\n",
            " 96% 48/50 [00:19<00:00,  2.29it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.32it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1796413660049438, 'eval_runtime': 20.1974, 'eval_samples_per_second': 4.951, 'eval_steps_per_second': 2.476, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [08:19<13:11, 23.28s/it]\n",
            "100% 50/50 [00:19<00:00,  2.40it/s]\u001b[A\n",
            "{'loss': 1.5741, 'grad_norm': 0.3364830017089844, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.6026, 'grad_norm': 0.27475258708000183, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:06<09:16, 23.17s/it][INFO|trainer.py:3788] 2024-07-23 14:48:43,015 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:48:43,016 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:48:43,016 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.82it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.87it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.98it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.83it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.85it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.74it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.54it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.48it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.50it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.46it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.55it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.63it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.65it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.72it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.54it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.49it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.55it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:11,  2.54it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.53it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:10,  2.58it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.46it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.45it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.64it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.58it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.68it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.68it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.39it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.37it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 72% 36/50 [00:14<00:05,  2.36it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.36it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.46it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.47it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.46it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.49it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.54it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.47it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.44it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.32it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.33it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.135266900062561, 'eval_runtime': 20.129, 'eval_samples_per_second': 4.968, 'eval_steps_per_second': 2.484, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [12:26<09:16, 23.17s/it]\n",
            "100% 50/50 [00:19<00:00,  2.39it/s]\u001b[A\n",
            "{'loss': 1.5924, 'grad_norm': 0.2893880009651184, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.5652, 'grad_norm': 0.2674891948699951, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:19<05:27, 23.38s/it][INFO|trainer.py:3788] 2024-07-23 14:52:56,680 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:52:56,680 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:52:56,681 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.92it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.88it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.98it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.75it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.55it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.42it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.51it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.53it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.49it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.57it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:12,  2.64it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.67it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.74it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.55it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:11,  2.51it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.57it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.58it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:09,  2.62it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.50it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.48it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:08,  2.56it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.62it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.55it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.64it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.64it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.35it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.34it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.36it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.45it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.40it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.49it/s]\u001b[A\n",
            " 82% 41/50 [00:15<00:03,  2.48it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.52it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.57it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.58it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.48it/s]\u001b[A\n",
            " 92% 46/50 [00:17<00:01,  2.43it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.43it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.30it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.32it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1260124444961548, 'eval_runtime': 20.076, 'eval_samples_per_second': 4.981, 'eval_steps_per_second': 2.491, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [16:39<05:27, 23.38s/it]\n",
            "100% 50/50 [00:19<00:00,  2.38it/s]\u001b[A\n",
            "{'loss': 1.4887, 'grad_norm': 0.30496805906295776, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.5411, 'grad_norm': 0.31508082151412964, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:28<01:30, 22.61s/it][INFO|trainer.py:3788] 2024-07-23 14:57:05,387 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:57:05,387 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:57:05,387 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.83it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.87it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:15,  2.97it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:15,  2.74it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:16,  2.55it/s]\u001b[A\n",
            " 18% 9/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
            " 20% 10/50 [00:03<00:16,  2.44it/s]\u001b[A\n",
            " 22% 11/50 [00:04<00:16,  2.42it/s]\u001b[A\n",
            " 24% 12/50 [00:04<00:15,  2.49it/s]\u001b[A\n",
            " 26% 13/50 [00:04<00:14,  2.50it/s]\u001b[A\n",
            " 28% 14/50 [00:05<00:14,  2.47it/s]\u001b[A\n",
            " 30% 15/50 [00:05<00:13,  2.52it/s]\u001b[A\n",
            " 32% 16/50 [00:05<00:13,  2.59it/s]\u001b[A\n",
            " 34% 17/50 [00:06<00:12,  2.63it/s]\u001b[A\n",
            " 36% 18/50 [00:06<00:11,  2.72it/s]\u001b[A\n",
            " 38% 19/50 [00:07<00:12,  2.54it/s]\u001b[A\n",
            " 40% 20/50 [00:07<00:12,  2.50it/s]\u001b[A\n",
            " 42% 21/50 [00:07<00:11,  2.56it/s]\u001b[A\n",
            " 44% 22/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
            " 46% 23/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            " 48% 24/50 [00:09<00:09,  2.60it/s]\u001b[A\n",
            " 50% 25/50 [00:09<00:10,  2.49it/s]\u001b[A\n",
            " 52% 26/50 [00:09<00:09,  2.49it/s]\u001b[A\n",
            " 54% 27/50 [00:10<00:08,  2.56it/s]\u001b[A\n",
            " 56% 28/50 [00:10<00:08,  2.64it/s]\u001b[A\n",
            " 58% 29/50 [00:11<00:08,  2.57it/s]\u001b[A\n",
            " 60% 30/50 [00:11<00:07,  2.67it/s]\u001b[A\n",
            " 62% 31/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            " 64% 32/50 [00:12<00:07,  2.36it/s]\u001b[A\n",
            " 66% 33/50 [00:12<00:07,  2.37it/s]\u001b[A\n",
            " 68% 34/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 70% 35/50 [00:13<00:06,  2.35it/s]\u001b[A\n",
            " 72% 36/50 [00:13<00:05,  2.36it/s]\u001b[A\n",
            " 74% 37/50 [00:14<00:05,  2.35it/s]\u001b[A\n",
            " 76% 38/50 [00:14<00:04,  2.44it/s]\u001b[A\n",
            " 78% 39/50 [00:15<00:04,  2.39it/s]\u001b[A\n",
            " 80% 40/50 [00:15<00:04,  2.47it/s]\u001b[A\n",
            " 82% 41/50 [00:16<00:03,  2.47it/s]\u001b[A\n",
            " 84% 42/50 [00:16<00:03,  2.49it/s]\u001b[A\n",
            " 86% 43/50 [00:16<00:02,  2.55it/s]\u001b[A\n",
            " 88% 44/50 [00:17<00:02,  2.55it/s]\u001b[A\n",
            " 90% 45/50 [00:17<00:02,  2.45it/s]\u001b[A\n",
            " 92% 46/50 [00:18<00:01,  2.39it/s]\u001b[A\n",
            " 94% 47/50 [00:18<00:01,  2.38it/s]\u001b[A\n",
            " 96% 48/50 [00:18<00:00,  2.27it/s]\u001b[A\n",
            " 98% 49/50 [00:19<00:00,  2.29it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1240512132644653, 'eval_runtime': 20.1785, 'eval_samples_per_second': 4.956, 'eval_steps_per_second': 2.478, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [20:48<01:30, 22.61s/it]\n",
            "100% 50/50 [00:19<00:00,  2.36it/s]\u001b[A\n",
            "100% 54/54 [22:21<00:00, 25.24s/it][INFO|trainer.py:3478] 2024-07-23 14:58:58,441 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 14:58:58,895 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 14:58:58,896 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 14:58:59,150 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 14:58:59,154 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 14:58:59,898 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1343.0759, 'train_samples_per_second': 2.01, 'train_steps_per_second': 0.04, 'train_loss': 1.637726448200367, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [22:23<00:00, 24.87s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 14:58:59,902 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 14:59:00,318 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-1.5B-Instruct/snapshots/ba1cf1846d7df0a0591d6c00649f57e798519da8/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 14:59:00,319 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 14:59:00,559 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 14:59:00,563 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  9716185GF\n",
            "  train_loss               =     1.6377\n",
            "  train_runtime            = 0:22:23.07\n",
            "  train_samples_per_second =       2.01\n",
            "  train_steps_per_second   =       0.04\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen15_QLoRA_Rank/train_Rank16_NF8/training_eval_loss.png\n",
            "07/23/2024 14:59:01 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 14:59:01,154 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 14:59:01,154 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 14:59:01,154 >>   Batch size = 2\n",
            "100% 50/50 [00:19<00:00,  2.53it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.1254\n",
            "  eval_runtime            = 0:00:20.18\n",
            "  eval_samples_per_second =      4.953\n",
            "  eval_steps_per_second   =      2.477\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 14:59:21,358 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-05 NF4 qlora qwen15"
      ],
      "metadata": {
        "id": "dS5QoIWFJw79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-05 NF4 qlora qwen15"
      ],
      "metadata": {
        "id": "L-6Zd_0HJ0w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-04 NF8 qlora qwen05"
      ],
      "metadata": {
        "id": "zjFWkFPrIRVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mh_iiFikOe2X",
        "outputId": "51dfbd88-89df-46e0-fed9-6771091e841c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 14:59:28.048222: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 14:59:28.048272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 14:59:28.049701: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 14:59:28.057397: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 14:59:29.192790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 14:59:35 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/23/2024 14:59:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 14:59:35,839 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 14:59:36,083 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 14:59:36 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 14:59:36 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 14:59:36 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 14:59:37,732 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 14:59:37,734 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 14:59:37 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 14:59:37,767 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 14:59:37,776 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 14:59:37,778 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 14:59:39,331 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 14:59:39,331 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 14:59:39,537 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 14:59:39,538 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,k_proj,up_proj,o_proj,gate_proj,q_proj,v_proj\n",
            "07/23/2024 14:59:39 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:642] 2024-07-23 14:59:39,896 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 14:59:40,303 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 14:59:40,303 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 14:59:40,304 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 14:59:40,304 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 14:59:40,304 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 14:59:40,304 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 14:59:40,304 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 14:59:40,307 >>   Number of trainable parameters = 4,399,104\n",
            "{'loss': 1.9314, 'grad_norm': 0.893233060836792, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.8282, 'grad_norm': 0.7264893054962158, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:59<08:47, 11.98s/it][INFO|trainer.py:3788] 2024-07-23 15:01:40,212 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:01:40,212 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:01:40,212 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.73it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:08,  5.27it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:10,  4.24it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  4.02it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:11,  3.94it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:11,  3.80it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:11,  3.61it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:11,  3.48it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:11,  3.49it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:11,  3.46it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:10,  3.52it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:10,  3.56it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.52it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:09,  3.60it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:09,  3.64it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.67it/s]\u001b[A\n",
            " 36% 18/50 [00:04<00:08,  3.67it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:08,  3.54it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:08,  3.52it/s]\u001b[A\n",
            " 42% 21/50 [00:05<00:08,  3.58it/s]\u001b[A\n",
            " 44% 22/50 [00:05<00:07,  3.61it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:07,  3.59it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.63it/s]\u001b[A\n",
            " 50% 25/50 [00:06<00:07,  3.52it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:06,  3.53it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.52it/s]\u001b[A\n",
            " 56% 28/50 [00:07<00:06,  3.55it/s]\u001b[A\n",
            " 58% 29/50 [00:07<00:06,  3.46it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.49it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.48it/s]\u001b[A\n",
            " 64% 32/50 [00:08<00:05,  3.26it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.28it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.21it/s]\u001b[A\n",
            " 70% 35/50 [00:09<00:04,  3.20it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.22it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:04,  3.21it/s]\u001b[A\n",
            " 76% 38/50 [00:10<00:03,  3.27it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.23it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.24it/s]\u001b[A\n",
            " 82% 41/50 [00:11<00:02,  3.27it/s]\u001b[A\n",
            " 84% 42/50 [00:11<00:02,  3.32it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.42it/s]\u001b[A\n",
            " 90% 45/50 [00:12<00:01,  3.32it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.27it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.27it/s]\u001b[A\n",
            " 96% 48/50 [00:13<00:00,  3.17it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.21it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2317595481872559, 'eval_runtime': 14.6235, 'eval_samples_per_second': 6.838, 'eval_steps_per_second': 3.419, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [02:14<08:47, 11.98s/it]\n",
            "100% 50/50 [00:14<00:00,  3.27it/s]\u001b[A\n",
            "{'loss': 1.7263, 'grad_norm': 0.6297321319580078, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.5586, 'grad_norm': 0.5541884899139404, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [04:10<06:45, 11.94s/it][INFO|trainer.py:3788] 2024-07-23 15:03:51,001 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:03:51,001 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:03:51,001 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.96it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.81it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.93it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.78it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:12,  3.65it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:12,  3.58it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.37it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.31it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.28it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:11,  3.27it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.34it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.34it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.33it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.38it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.39it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.40it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.46it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.33it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.28it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.35it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.35it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.33it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.36it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.29it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.29it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.37it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.44it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.39it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.44it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.46it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.24it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.28it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.24it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.23it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.25it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:03,  3.25it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.30it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.27it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.33it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.34it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.38it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.30it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.25it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.19it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.06it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.14it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2196452617645264, 'eval_runtime': 15.142, 'eval_samples_per_second': 6.604, 'eval_steps_per_second': 3.302, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [04:25<06:45, 11.94s/it]\n",
            "100% 50/50 [00:14<00:00,  3.23it/s]\u001b[A\n",
            "{'loss': 1.5553, 'grad_norm': 0.5881783366203308, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.563, 'grad_norm': 0.5647853016853333, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [06:22<04:45, 11.90s/it][INFO|trainer.py:3788] 2024-07-23 15:06:02,400 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:06:02,400 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:06:02,400 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.96it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.84it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.94it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.76it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:12,  3.61it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:12,  3.53it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.35it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.26it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.25it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:12,  3.23it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.30it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.26it/s]\u001b[A\n",
            " 28% 14/50 [00:04<00:11,  3.22it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.24it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.29it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.33it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.42it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.28it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.26it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.32it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.32it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.30it/s]\u001b[A\n",
            " 48% 24/50 [00:07<00:07,  3.32it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.24it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.18it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:07,  3.28it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.36it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.34it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.38it/s]\u001b[A\n",
            " 62% 31/50 [00:09<00:05,  3.40it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.20it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.23it/s]\u001b[A\n",
            " 68% 34/50 [00:10<00:04,  3.21it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.23it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.24it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:03,  3.25it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.32it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.29it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.33it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.35it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.38it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.43it/s]\u001b[A\n",
            " 88% 44/50 [00:13<00:01,  3.45it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.35it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.30it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.29it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2209728956222534, 'eval_runtime': 15.2282, 'eval_samples_per_second': 6.567, 'eval_steps_per_second': 3.283, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [06:37<04:45, 11.90s/it]\n",
            "100% 50/50 [00:14<00:00,  3.23it/s]\u001b[A\n",
            "{'loss': 1.5407, 'grad_norm': 0.6438606977462769, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.4952, 'grad_norm': 0.6532832980155945, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [08:36<02:48, 12.00s/it][INFO|trainer.py:3788] 2024-07-23 15:08:16,883 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:08:16,884 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:08:16,884 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.17it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.90it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.99it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.82it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:11,  3.70it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:11,  3.58it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.39it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.30it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.25it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:12,  3.25it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.31it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.30it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.29it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.34it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.36it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.38it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.48it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.33it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.26it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.33it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.34it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.33it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.36it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.30it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.31it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.38it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.44it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.40it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.44it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.45it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.22it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.24it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:05,  3.20it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.21it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.23it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:04,  3.24it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.30it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.27it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.33it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.33it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.35it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.40it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.39it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.29it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.26it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.26it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.10it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.13it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2221978902816772, 'eval_runtime': 15.1483, 'eval_samples_per_second': 6.601, 'eval_steps_per_second': 3.301, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [08:51<02:48, 12.00s/it]\n",
            "100% 50/50 [00:14<00:00,  3.18it/s]\u001b[A\n",
            "{'loss': 1.3697, 'grad_norm': 0.7309451103210449, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.4115, 'grad_norm': 0.7102791666984558, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [10:48<00:46, 11.66s/it][INFO|trainer.py:3788] 2024-07-23 15:10:29,059 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:10:29,060 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:10:29,060 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.04it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.86it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.94it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.79it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:12,  3.67it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:12,  3.57it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.39it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.31it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.28it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:11,  3.29it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.35it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.32it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.32it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.35it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.35it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.33it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.40it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.20it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.19it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.30it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.30it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.30it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.35it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.29it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.28it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.35it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.44it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.41it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.46it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.48it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.24it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.26it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.20it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.21it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.21it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:04,  3.23it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.30it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.28it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.34it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.36it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.40it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.40it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.25it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.22it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.23it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.13it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2238740921020508, 'eval_runtime': 15.1734, 'eval_samples_per_second': 6.591, 'eval_steps_per_second': 3.295, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [11:03<00:46, 11.66s/it]\n",
            "100% 50/50 [00:14<00:00,  3.24it/s]\u001b[A\n",
            "100% 54/54 [11:51<00:00, 13.46s/it][INFO|trainer.py:3478] 2024-07-23 15:11:31,910 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:11:32,433 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:11:32,434 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:11:32,539 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:11:32,543 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 15:11:32,991 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 712.6837, 'train_samples_per_second': 3.788, 'train_steps_per_second': 0.076, 'train_loss': 1.583768191160979, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [11:52<00:00, 13.20s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 15:11:32,997 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:11:33,561 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:11:33,562 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:11:33,668 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:11:33,672 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  2649106GF\n",
            "  train_loss               =     1.5838\n",
            "  train_runtime            = 0:11:52.68\n",
            "  train_samples_per_second =      3.788\n",
            "  train_steps_per_second   =      0.076\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8_5e-04/training_eval_loss.png\n",
            "07/23/2024 15:11:34 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 15:11:34,192 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:11:34,192 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:11:34,192 >>   Batch size = 2\n",
            "100% 50/50 [00:14<00:00,  3.42it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.2233\n",
            "  eval_runtime            = 0:00:14.95\n",
            "  eval_samples_per_second =      6.688\n",
            "  eval_steps_per_second   =      3.344\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 15:11:49,162 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 8 learning_rate 5e-05 NF8 qlora qwen05"
      ],
      "metadata": {
        "id": "NFLrduEOIYZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template qwen \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwyv1UiLCNfA",
        "outputId": "19a490cb-8a16-4695-c8d3-fefa3a1bc5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 15:11:53.890446: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 15:11:53.890496: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 15:11:53.891864: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 15:11:53.898872: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 15:11:55.042412: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 15:12:01 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/23/2024 15:12:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 15:12:01,639 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 15:12:01,870 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 15:12:01 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
            "07/23/2024 15:12:01 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/23/2024 15:12:02 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 93512, 3412, 3405, 2661, 279, 3405, 323, 37597, 198, 52428, 25, 2160, 2619, 458, 23208, 5162, 315, 12091, 17564, 266, 535, 14768, 1032, 15349, 3719, 1313, 220, 17, 64, 66235, 2267, 25339, 51655, 58713, 311, 17564, 266, 535, 14768, 1032, 15349, 2463, 396, 92070, 547, 417, 726, 304, 32710, 287, 479, 22975, 1950, 453, 276, 837, 774, 31259, 408, 77638, 350, 68261, 5949, 5267, 98365, 50, 25, 1084, 374, 9788, 3425, 15394, 9819, 1757, 266, 535, 14768, 34168, 52482, 220, 17, 64, 320, 50900, 17, 64, 8, 32019, 2267, 25339, 51655, 320, 40, 22455, 8, 702, 5107, 897, 7707, 311, 1757, 266, 535, 14768, 34168, 1136, 396, 92070, 320, 50, 11451, 8, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 2033, 311, 71149, 34168, 8887, 45655, 1667, 220, 16, 22, 22, 49202, 16022, 302, 265, 20223, 320, 6480, 5350, 8, 304, 6835, 448, 58249, 1950, 453, 276, 837, 774, 17773, 408, 77638, 54971, 320, 38, 9197, 11250, 1348, 82, 568, 576, 21538, 315, 419, 3920, 1033, 25, 320, 16, 8, 311, 5695, 279, 11414, 315, 274, 267, 17, 64, 32019, 453, 436, 18055, 304, 479, 9197, 11250, 1348, 10469, 315, 8575, 5350, 87086, 6835, 11, 320, 17, 8, 311, 8253, 279, 5025, 1948, 1850, 479, 9197, 11250, 1348, 2033, 1667, 74136, 3846, 220, 16, 13, 15, 12890, 220, 16, 1042, 1283, 8575, 5350, 323, 15394, 9819, 274, 267, 17, 64, 358, 22455, 11, 323, 320, 18, 8, 311, 9429, 17452, 315, 6835, 448, 274, 267, 17, 64, 358, 22455, 60935, 323, 481, 30487, 54971, 13, 2009, 220, 22, 18, 23921, 6835, 1033, 4091, 369, 8575, 5350, 3118, 389, 264, 6785, 328, 11451, 13, 34869, 5729, 2033, 572, 16548, 4092, 311, 74136, 3846, 220, 16, 13, 15, 12890, 13, 274, 267, 17, 64, 2639, 572, 16507, 389, 35154, 10469, 553, 358, 22455, 13, 758, 2790, 11, 220, 24, 18, 4, 315, 479, 9197, 11250, 1348, 10469, 8542, 274, 267, 17, 64, 358, 22455, 96942, 13, 2308, 46852, 5089, 5025, 572, 13166, 1948, 304, 53904, 274, 267, 17, 64, 7493, 323, 304, 40194, 1850, 479, 9197, 11250, 1348, 2033, 220, 16, 1042, 1283, 8575, 5350, 320, 79, 284, 220, 15, 13, 19, 22, 568, 6695, 11, 6028, 35154, 2747, 11, 8457, 6430, 11, 5190, 43167, 30923, 44, 23850, 11, 29458, 12, 21, 22, 1922, 11, 8426, 40429, 21372, 538, 6576, 258, 6691, 2188, 11, 323, 8426, 48284, 18906, 662, 337, 519, 2188, 1033, 537, 11941, 2155, 1948, 6835, 448, 8225, 323, 6785, 274, 267, 17, 64, 15394, 9819, 358, 22455, 448, 279, 4683, 315, 4231, 518, 22982, 320, 79, 284, 220, 15, 13, 15, 15, 22, 568, 151645, 198, 151644, 77091, 198, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a helpful assistant.<|im_end|>\n",
            "<|im_start|>user\n",
            "Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|im_end|>\n",
            "<|im_start|>assistant\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2152, 13, 274, 267, 17, 64, 358, 22455, 315, 35154, 10469, 702, 902, 5107, 897, 7707, 311, 328, 11451, 68475, 1667, 4915, 29583, 26570, 11909, 304, 51897, 35154, 2033, 1283, 8575, 5350, 13, 151645]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|im_end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:12:03,527 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:12:03,528 >> Model config Qwen2Config {\n",
            "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "07/23/2024 15:12:03 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 15:12:03,559 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 15:12:03,568 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:12:03,570 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 15:12:05,050 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 15:12:05,051 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 15:12:05,281 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 15:12:05,282 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,o_proj,k_proj,v_proj,q_proj,down_proj\n",
            "07/23/2024 15:12:05 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\n",
            "[INFO|trainer.py:642] 2024-07-23 15:12:05,622 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-23 15:12:06,031 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-23 15:12:06,031 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-23 15:12:06,031 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-23 15:12:06,031 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-23 15:12:06,031 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-23 15:12:06,031 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-23 15:12:06,031 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-23 15:12:06,035 >>   Number of trainable parameters = 4,399,104\n",
            "{'loss': 2.0764, 'grad_norm': 1.4118915796279907, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 121776}\n",
            "{'loss': 1.9712, 'grad_norm': 1.0194246768951416, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [01:59<08:46, 11.97s/it][INFO|trainer.py:3788] 2024-07-23 15:14:05,787 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:14:05,787 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:14:05,787 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.72it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:08,  5.30it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:10,  4.26it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  4.02it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:11,  3.95it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:11,  3.82it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:11,  3.63it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:11,  3.52it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:11,  3.48it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:11,  3.46it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:10,  3.52it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:10,  3.55it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.54it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:09,  3.63it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:09,  3.67it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:08,  3.71it/s]\u001b[A\n",
            " 36% 18/50 [00:04<00:08,  3.77it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:08,  3.59it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:08,  3.55it/s]\u001b[A\n",
            " 42% 21/50 [00:05<00:08,  3.59it/s]\u001b[A\n",
            " 44% 22/50 [00:05<00:07,  3.62it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:07,  3.56it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.59it/s]\u001b[A\n",
            " 50% 25/50 [00:06<00:07,  3.43it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:06,  3.44it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.51it/s]\u001b[A\n",
            " 56% 28/50 [00:07<00:06,  3.55it/s]\u001b[A\n",
            " 58% 29/50 [00:07<00:06,  3.46it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.36it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.40it/s]\u001b[A\n",
            " 64% 32/50 [00:08<00:05,  3.21it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.26it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.23it/s]\u001b[A\n",
            " 70% 35/50 [00:09<00:04,  3.25it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.25it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:03,  3.27it/s]\u001b[A\n",
            " 76% 38/50 [00:10<00:03,  3.32it/s]\u001b[A\n",
            " 78% 39/50 [00:10<00:03,  3.29it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.33it/s]\u001b[A\n",
            " 82% 41/50 [00:11<00:02,  3.34it/s]\u001b[A\n",
            " 84% 42/50 [00:11<00:02,  3.38it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.43it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.44it/s]\u001b[A\n",
            " 90% 45/50 [00:12<00:01,  3.35it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.31it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.30it/s]\u001b[A\n",
            " 96% 48/50 [00:13<00:00,  3.19it/s]\u001b[A\n",
            " 98% 49/50 [00:13<00:00,  3.23it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3128665685653687, 'eval_runtime': 14.5625, 'eval_samples_per_second': 6.867, 'eval_steps_per_second': 3.433, 'epoch': 0.53, 'num_input_tokens_seen': 245952}\n",
            " 19% 10/54 [02:14<08:46, 11.97s/it]\n",
            "100% 50/50 [00:14<00:00,  3.29it/s]\u001b[A\n",
            "{'loss': 1.8065, 'grad_norm': 0.9060378074645996, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 364128}\n",
            "{'loss': 1.6293, 'grad_norm': 0.9219687581062317, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [04:10<06:45, 11.94s/it][INFO|trainer.py:3788] 2024-07-23 15:16:16,511 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:16:16,511 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:16:16,511 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.10it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.89it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.98it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.83it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:11,  3.71it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:11,  3.59it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.41it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.32it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.29it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:11,  3.28it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.35it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.34it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.32it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.37it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:09,  3.41it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.42it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.47it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.34it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.30it/s]\u001b[A\n",
            " 42% 21/50 [00:05<00:08,  3.38it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.39it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.36it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.38it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.31it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.32it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.37it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.45it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.41it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.43it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.43it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.22it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.22it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:05,  3.16it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.17it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.19it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:04,  3.19it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.24it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.21it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.27it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.29it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.33it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.42it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.31it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.28it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.28it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.20it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2518153190612793, 'eval_runtime': 15.1034, 'eval_samples_per_second': 6.621, 'eval_steps_per_second': 3.311, 'epoch': 1.07, 'num_input_tokens_seen': 484224}\n",
            " 37% 20/54 [04:25<06:45, 11.94s/it]\n",
            "100% 50/50 [00:14<00:00,  3.27it/s]\u001b[A\n",
            "{'loss': 1.7319, 'grad_norm': 0.8409714102745056, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 600720}\n",
            "{'loss': 1.7609, 'grad_norm': 0.7850850820541382, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [06:21<04:44, 11.87s/it][INFO|trainer.py:3788] 2024-07-23 15:18:27,697 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:18:27,697 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:18:27,697 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.92it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.80it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.92it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.76it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:12,  3.63it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:12,  3.53it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.30it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.24it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.24it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:12,  3.23it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.32it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.34it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.34it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.37it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.39it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.43it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.51it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.36it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:08,  3.35it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.43it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.42it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:07,  3.39it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.42it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.33it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.31it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.39it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.46it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.41it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.44it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.46it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.23it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.27it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.26it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.27it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.28it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:03,  3.28it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.33it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.30it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:02,  3.34it/s]\u001b[A\n",
            " 82% 41/50 [00:11<00:02,  3.35it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.26it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.34it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.37it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.26it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.22it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.22it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.10it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.14it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2414259910583496, 'eval_runtime': 15.118, 'eval_samples_per_second': 6.615, 'eval_steps_per_second': 3.307, 'epoch': 1.6, 'num_input_tokens_seen': 723072}\n",
            " 56% 30/54 [06:36<04:44, 11.87s/it]\n",
            "100% 50/50 [00:14<00:00,  3.21it/s]\u001b[A\n",
            "{'loss': 1.7623, 'grad_norm': 0.805554211139679, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 848544}\n",
            "{'loss': 1.7367, 'grad_norm': 0.736824095249176, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [08:35<02:47, 11.97s/it][INFO|trainer.py:3788] 2024-07-23 15:20:41,791 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:20:41,792 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:20:41,792 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.86it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.76it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.84it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:12,  3.72it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:12,  3.60it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:12,  3.52it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.35it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.29it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.27it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:11,  3.27it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.35it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.35it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:10,  3.34it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.39it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:09,  3.42it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.44it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.49it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.35it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.31it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.40it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.42it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:07,  3.44it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.46it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.39it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.37it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.42it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.48it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.41it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.45it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.47it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.25it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.28it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.25it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.26it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.27it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:04,  3.25it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.28it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
            " 82% 41/50 [00:11<00:02,  3.33it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.33it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.39it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.38it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.28it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.26it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.26it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.14it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.17it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2362672090530396, 'eval_runtime': 15.0601, 'eval_samples_per_second': 6.64, 'eval_steps_per_second': 3.32, 'epoch': 2.13, 'num_input_tokens_seen': 969696}\n",
            " 74% 40/54 [08:50<02:47, 11.97s/it]\n",
            "100% 50/50 [00:14<00:00,  3.23it/s]\u001b[A\n",
            "{'loss': 1.658, 'grad_norm': 0.9146463871002197, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1094352}\n",
            "{'loss': 1.6978, 'grad_norm': 0.7647510170936584, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [10:47<00:46, 11.62s/it][INFO|trainer.py:3788] 2024-07-23 15:22:53,511 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:22:53,511 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:22:53,511 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.07it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:09,  4.88it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:11,  3.98it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:11,  3.79it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:11,  3.69it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:11,  3.60it/s]\u001b[A\n",
            " 16% 8/50 [00:02<00:12,  3.40it/s]\u001b[A\n",
            " 18% 9/50 [00:02<00:12,  3.30it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:12,  3.25it/s]\u001b[A\n",
            " 22% 11/50 [00:03<00:12,  3.24it/s]\u001b[A\n",
            " 24% 12/50 [00:03<00:11,  3.30it/s]\u001b[A\n",
            " 26% 13/50 [00:03<00:11,  3.27it/s]\u001b[A\n",
            " 28% 14/50 [00:03<00:11,  3.27it/s]\u001b[A\n",
            " 30% 15/50 [00:04<00:10,  3.29it/s]\u001b[A\n",
            " 32% 16/50 [00:04<00:10,  3.33it/s]\u001b[A\n",
            " 34% 17/50 [00:04<00:09,  3.36it/s]\u001b[A\n",
            " 36% 18/50 [00:05<00:09,  3.44it/s]\u001b[A\n",
            " 38% 19/50 [00:05<00:09,  3.30it/s]\u001b[A\n",
            " 40% 20/50 [00:05<00:09,  3.27it/s]\u001b[A\n",
            " 42% 21/50 [00:06<00:08,  3.35it/s]\u001b[A\n",
            " 44% 22/50 [00:06<00:08,  3.33it/s]\u001b[A\n",
            " 46% 23/50 [00:06<00:08,  3.34it/s]\u001b[A\n",
            " 48% 24/50 [00:06<00:07,  3.36it/s]\u001b[A\n",
            " 50% 25/50 [00:07<00:07,  3.29it/s]\u001b[A\n",
            " 52% 26/50 [00:07<00:07,  3.30it/s]\u001b[A\n",
            " 54% 27/50 [00:07<00:06,  3.36it/s]\u001b[A\n",
            " 56% 28/50 [00:08<00:06,  3.45it/s]\u001b[A\n",
            " 58% 29/50 [00:08<00:06,  3.40it/s]\u001b[A\n",
            " 60% 30/50 [00:08<00:05,  3.45it/s]\u001b[A\n",
            " 62% 31/50 [00:08<00:05,  3.46it/s]\u001b[A\n",
            " 64% 32/50 [00:09<00:05,  3.22it/s]\u001b[A\n",
            " 66% 33/50 [00:09<00:05,  3.27it/s]\u001b[A\n",
            " 68% 34/50 [00:09<00:04,  3.23it/s]\u001b[A\n",
            " 70% 35/50 [00:10<00:04,  3.23it/s]\u001b[A\n",
            " 72% 36/50 [00:10<00:04,  3.24it/s]\u001b[A\n",
            " 74% 37/50 [00:10<00:03,  3.26it/s]\u001b[A\n",
            " 76% 38/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
            " 78% 39/50 [00:11<00:03,  3.29it/s]\u001b[A\n",
            " 80% 40/50 [00:11<00:03,  3.32it/s]\u001b[A\n",
            " 82% 41/50 [00:12<00:02,  3.35it/s]\u001b[A\n",
            " 84% 42/50 [00:12<00:02,  3.37it/s]\u001b[A\n",
            " 86% 43/50 [00:12<00:02,  3.44it/s]\u001b[A\n",
            " 88% 44/50 [00:12<00:01,  3.46it/s]\u001b[A\n",
            " 90% 45/50 [00:13<00:01,  3.32it/s]\u001b[A\n",
            " 92% 46/50 [00:13<00:01,  3.27it/s]\u001b[A\n",
            " 94% 47/50 [00:13<00:00,  3.29it/s]\u001b[A\n",
            " 96% 48/50 [00:14<00:00,  3.19it/s]\u001b[A\n",
            " 98% 49/50 [00:14<00:00,  3.21it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2342450618743896, 'eval_runtime': 15.117, 'eval_samples_per_second': 6.615, 'eval_steps_per_second': 3.308, 'epoch': 2.67, 'num_input_tokens_seen': 1210752}\n",
            " 93% 50/54 [11:02<00:46, 11.62s/it]\n",
            "100% 50/50 [00:14<00:00,  3.26it/s]\u001b[A\n",
            "100% 54/54 [11:50<00:00, 13.41s/it][INFO|trainer.py:3478] 2024-07-23 15:23:56,141 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:23:56,652 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:23:56,653 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:23:56,760 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:23:56,764 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-23 15:23:57,198 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 711.1633, 'train_samples_per_second': 3.797, 'train_steps_per_second': 0.076, 'train_loss': 1.776593181822035, 'epoch': 2.88, 'num_input_tokens_seen': 1308528}\n",
            "100% 54/54 [11:51<00:00, 13.17s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-23 15:23:57,202 >> Saving model checkpoint to /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 15:23:57,690 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 15:23:57,692 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 896,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 4864,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 24,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 14,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-23 15:23:57,797 >> tokenizer config file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-23 15:23:57,801 >> Special tokens file saved in /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1308528\n",
            "  total_flos               =  2649106GF\n",
            "  train_loss               =     1.7766\n",
            "  train_runtime            = 0:11:51.16\n",
            "  train_samples_per_second =      3.797\n",
            "  train_steps_per_second   =      0.076\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/Qwen05_QLoRA_Rank/train_Rank8_NF8/training_eval_loss.png\n",
            "07/23/2024 15:23:58 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-23 15:23:58,309 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 15:23:58,309 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 15:23:58,309 >>   Batch size = 2\n",
            "100% 50/50 [00:14<00:00,  3.47it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     1.2331\n",
            "  eval_runtime            = 0:00:14.71\n",
            "  eval_samples_per_second =      6.795\n",
            "  eval_steps_per_second   =      3.397\n",
            "  num_input_tokens_seen   =    1308528\n",
            "[INFO|modelcard.py:449] 2024-07-23 15:24:13,041 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-05 NF4 qlora qwen05"
      ],
      "metadata": {
        "id": "8emwhU1lJ-zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lora_rank 16 learning_rate 5e-05 NF8 qlora qwen05"
      ],
      "metadata": {
        "id": "-DPwb59pJh1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phi-3"
      ],
      "metadata": {
        "id": "R4ENU1A6I0wY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/phi3/QloRA/train_bz2 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2fQR3c1Yufu",
        "outputId": "21f03446-d883-4bad-bac8-243f5fa79e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-25 14:54:59.075274: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-25 14:54:59.075330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-25 14:54:59.076654: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-25 14:54:59.084082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-25 14:55:00.226683: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/25/2024 14:55:07 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/25/2024 14:55:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 28.7MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 2.80MB/s]\n",
            "tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 2.23MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.70MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 5.75MB/s]\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:55:10,334 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:55:10,334 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:55:10,334 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:55:10,334 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:55:10,334 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-25 14:55:10,407 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/25/2024 14:55:10 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/25/2024 14:55:10 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/25/2024 14:55:10 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:01, 761.37 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2150.02 examples/s]\n",
            "07/25/2024 14:55:15 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 14140.30 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2482.35 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1097.92 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 8.59MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 14:55:21,005 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 60.6MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 14:55:22,100 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-25 14:55:22,101 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/25/2024 14:55:22 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 383kB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 70.4MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-25 14:55:24,215 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<03:51, 21.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<01:34, 52.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<01:18, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:00<01:08, 72.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:00<01:02, 79.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.97G [00:01<00:57, 85.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:01<00:54, 90.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:01<00:51, 94.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:01<00:50, 96.8MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:01<00:49, 98.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:01<00:48, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:01<00:47, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:01<00:47, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:01<00:47, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.97G [00:02<00:45, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:02<00:46, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:02<00:45, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:03<00:46, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:03<00:45, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:03<00:45, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:03<00:45, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:03<00:45, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:03<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:03<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:03<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:03<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:03<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:04<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:04<00:44, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:04<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:04<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:04<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:04<00:44, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:04<00:43, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:04<00:43, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:04<00:43, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.97G [00:04<00:43, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:05<00:43, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:05<00:44, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:05<00:44, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:05<00:43, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:05<00:43, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:05<00:44, 99.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:05<00:44, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.97G [00:05<00:43, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:05<00:44, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:06<00:43, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:06<00:43, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:06<00:43, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:06<00:42, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:06<00:42, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:06<00:42, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:06<00:42, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:06<00:42, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:06<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:06<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:07<00:41, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.97G [00:07<00:42, 99.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:07<00:41, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:07<00:41, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:08<00:41, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:08<00:41, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:08<00:40, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.97G [00:08<00:41, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:08<00:41, 98.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:08<00:41, 98.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:08<00:41, 98.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.97G [00:08<00:41, 98.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:08<00:41, 99.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:09<00:41, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:09<00:41, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:09<00:40, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:09<00:40, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:09<00:40, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:09<00:40, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:09<00:39, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:09<00:39, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:09<00:39, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:09<00:39, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:10<00:39, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:10<00:39, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:10<00:39, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.97G [00:10<00:38, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:10<00:38, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:10<00:38, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:10<00:38, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:10<00:38, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:10<00:37, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:10<00:38, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:11<00:38, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.97G [00:11<00:38, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:11<00:37, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:11<00:37, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:11<00:37, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:11<00:37, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:11<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:11<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:11<00:36, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:11<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:12<00:36, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:12<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:12<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:12<00:36, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:12<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:12<00:36, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:12<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:12<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:12<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:13<00:35, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:13<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:13<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:13<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:14<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:14<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:14<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:14<00:34, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:14<00:34, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:14<00:34, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:14<00:34, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:14<00:34, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:14<00:34, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:14<00:34, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [00:15<00:33, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:15<00:34, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:15<00:34, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:15<00:33, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:15<00:33, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:15<00:34, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:15<00:34, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:15<00:33, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:15<00:33, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:15<00:33, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:16<00:32, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:16<00:32, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:16<00:32, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:16<00:33, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:16<00:32, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:16<00:32, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:16<00:32, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:16<00:32, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:16<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:17<00:31, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.97G [00:17<00:31, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:18<00:54, 58.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:18<00:47, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [00:18<00:41, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:18<00:38, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:18<00:35, 87.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:18<00:34, 91.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:18<00:32, 95.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:18<00:31, 97.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:18<00:31, 99.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [00:19<00:30, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:19<00:30, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [00:19<00:30, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:19<00:29, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:19<00:29, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [00:19<00:29, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:19<00:29, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:19<00:29, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [00:19<00:29, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:20<00:29, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:20<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:21<00:28, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [00:21<00:28, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [00:21<00:28, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:21<00:28, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.97G [00:21<00:28, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:21<00:28, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:21<00:28, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [00:21<00:28, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:21<00:28, 98.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:21<00:28, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [00:22<00:27, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [00:22<00:27, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:22<00:27, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:22<00:27, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [00:22<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:22<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.97G [00:22<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [00:22<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:22<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.97G [00:23<00:26, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [00:23<00:26, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:23<00:25, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.97G [00:23<00:25, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [00:23<00:26, 99.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:23<00:26, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:23<00:26, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [00:23<00:25, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:23<00:25, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:23<00:25, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.97G [00:24<00:25, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [00:24<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:25<00:24, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:25<00:23, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [00:25<00:23, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [00:25<00:23, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:25<00:23, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [00:26<00:23, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:26<00:23, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:26<00:23, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [00:26<00:23, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:26<00:22, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:26<00:22, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:26<00:22, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:26<00:22, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:26<00:22, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:27<00:22, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [00:27<00:21, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:27<00:21, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:28<00:20, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:28<00:20, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:28<00:20, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:28<00:20, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:28<00:20, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:28<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:28<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:28<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:28<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.97G [00:28<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:29<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:29<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:29<00:20, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:29<00:20, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [00:29<00:19, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.97G [00:29<00:19, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [00:29<00:19, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:29<00:19, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.97G [00:29<00:19, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:29<00:19, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:30<00:19, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:30<00:19, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:30<00:19, 98.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:30<00:19, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:30<00:18, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [00:30<00:18, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:30<00:18, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [00:30<00:18, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:30<00:18, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:31<00:18, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [00:31<00:18, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:31<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:32<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.97G [00:32<00:17, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:32<00:17, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:32<00:29, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.97G [00:32<00:25, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:32<00:22, 74.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:32<00:20, 81.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [00:32<00:19, 87.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:33<00:18, 91.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:33<00:17, 94.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:33<00:17, 96.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:33<00:16, 98.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:33<00:16, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [00:33<00:16, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [00:33<00:15, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:33<00:15, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.97G [00:33<00:15, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:34<00:15, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:34<00:15, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.97G [00:34<00:15, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:34<00:15, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:34<00:15, 97.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [00:34<00:15, 97.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [00:34<00:15, 98.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:34<00:15, 98.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [00:34<00:15, 98.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [00:34<00:14, 99.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:35<00:14, 98.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.97G [00:35<00:14, 99.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [00:35<00:14, 98.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:35<00:14, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.97G [00:35<00:14, 99.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:35<00:14, 98.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [00:35<00:14, 99.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [00:35<00:14, 98.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:35<00:14, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [00:36<00:14, 98.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:36<00:13, 99.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [00:36<00:13, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [00:36<00:13, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:36<00:13, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [00:36<00:13, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [00:36<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.97G [00:36<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [00:36<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:36<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [00:37<00:12, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [00:37<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:37<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.97G [00:37<00:12, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [00:37<00:12, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:37<00:12, 99.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [00:37<00:12, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [00:37<00:11, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [00:37<00:11, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:37<00:11, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [00:38<00:11, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [00:38<00:11, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [00:38<00:11, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [00:38<00:11, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:38<00:11, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [00:38<00:11, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [00:38<00:10, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [00:38<00:10, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.97G [00:38<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [00:39<00:10, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [00:39<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [00:39<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:39<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [00:39<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [00:40<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [00:40<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [00:40<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [00:40<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:40<00:09, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.97G [00:40<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:40<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [00:40<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:40<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [00:40<00:08, 104MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [00:41<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.97G [00:41<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [00:41<00:08, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:41<00:08, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [00:41<00:08, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [00:41<00:08, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:41<00:07, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.97G [00:41<00:07, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:41<00:07, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [00:41<00:07, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [00:42<00:07, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [00:42<00:07, 98.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:42<00:07, 99.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [00:42<00:07, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [00:42<00:07, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [00:42<00:07, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [00:42<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [00:42<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [00:42<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.97G [00:43<00:06, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [00:43<00:06, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [00:43<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [00:43<00:06, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [00:43<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [00:43<00:06, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:43<00:05, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [00:43<00:05, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [00:43<00:05, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [00:43<00:05, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [00:44<00:05, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [00:44<00:05, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [00:44<00:05, 101MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [00:44<00:05, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [00:44<00:05, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.97G [00:44<00:05, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:44<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.97G [00:44<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [00:44<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [00:44<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [00:45<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.97G [00:45<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [00:45<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [00:45<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [00:45<00:04, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:45<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [00:45<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [00:45<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [00:45<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [00:45<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [00:46<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [00:46<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [00:46<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [00:46<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [00:46<00:03, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.97G [00:46<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [00:46<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [00:46<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [00:46<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [00:47<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [00:47<00:02, 103MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [00:47<00:02, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [00:47<00:02, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [00:47<00:02, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [00:47<00:02, 102MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [00:47<00:02, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.97G [00:47<00:01, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [00:47<00:01, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [00:47<00:01, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [00:48<00:01, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [00:48<00:01, 99.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [00:48<00:01, 99.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [00:48<00:01, 99.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [00:48<00:01, 100MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [00:48<00:01, 99.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [00:48<00:00, 99.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [00:48<00:00, 99.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [00:48<00:00, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [00:48<00:00, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [00:49<00:00, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.97G [00:49<00:00, 100MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [00:49<00:00, 99.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.97G [00:49<00:00, 99.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.97G [00:49<00:00, 101MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:49<00:00, 100MB/s]\n",
            "Downloading shards:  50% 1/2 [00:50<00:50, 50.14s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/2.67G [00:00<00:26, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:00<00:25, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:00<00:25, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:25, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.67G [00:00<00:25, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:00<00:25, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:00<00:46, 55.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:01<00:39, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:01<00:35, 73.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 105M/2.67G [00:01<00:31, 80.6MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 115M/2.67G [00:01<00:29, 86.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:01<00:28, 89.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:01<00:27, 92.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:01<00:26, 94.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:01<00:25, 97.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 168M/2.67G [00:01<00:25, 99.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:02<00:24, 101MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:02<00:24, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:02<00:24, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 210M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 220M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 273M/2.67G [00:02<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:03<00:23, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 315M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 377M/2.67G [00:03<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:04<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:04<00:22, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 430M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:04<00:21, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 472M/2.67G [00:04<00:21, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:04<00:21, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:05<00:21, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:05<00:21, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:05<00:21, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:05<00:21, 99.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 535M/2.67G [00:05<00:21, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:05<00:21, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:05<00:21, 99.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:05<00:21, 99.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 577M/2.67G [00:05<00:20, 99.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:06<00:21, 98.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:06<00:20, 99.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:06<00:20, 98.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:06<00:20, 99.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 629M/2.67G [00:06<00:20, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 640M/2.67G [00:06<00:20, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:06<00:19, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:06<00:19, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:06<00:19, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 682M/2.67G [00:06<00:19, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:07<00:19, 99.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:07<00:19, 101MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:07<00:19, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:07<00:19, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:07<00:19, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 744M/2.67G [00:07<00:19, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:07<00:18, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:07<00:18, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:07<00:18, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 786M/2.67G [00:07<00:18, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:08<00:18, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:08<00:18, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:08<00:17, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:08<00:17, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 839M/2.67G [00:08<00:17, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 849M/2.67G [00:08<00:17, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:08<00:17, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:08<00:17, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:08<00:17, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 891M/2.67G [00:08<00:17, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:09<00:17, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:09<00:16, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:09<00:16, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:09<00:16, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:09<00:16, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 954M/2.67G [00:09<00:16, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:09<00:16, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:09<00:16, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:09<00:16, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 996M/2.67G [00:10<00:16, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:10<00:16, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:10<00:15, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:10<00:15, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:10<00:15, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:10<00:15, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.06G/2.67G [00:10<00:15, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:10<00:17, 90.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:10<00:16, 94.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:10<00:16, 96.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:11<00:15, 98.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:11<00:15, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:11<00:15, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:11<00:15, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.14G/2.67G [00:11<00:14, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:11<00:14, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:11<00:14, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:11<00:14, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:11<00:14, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:11<00:14, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:12<00:14, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.22G/2.67G [00:12<00:14, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:12<00:14, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:12<00:25, 57.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.25G/2.67G [00:12<00:21, 65.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:12<00:19, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:13<00:17, 78.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:13<00:16, 83.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:13<00:15, 87.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.30G/2.67G [00:13<00:15, 90.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:13<00:14, 93.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:13<00:14, 94.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:13<00:13, 96.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:13<00:13, 96.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:13<00:13, 97.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.36G/2.67G [00:13<00:13, 97.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:14<00:13, 98.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:14<00:13, 98.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:14<00:12, 98.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:14<00:12, 98.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:14<00:12, 99.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:14<00:12, 99.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:14<00:12, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:14<00:12, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:14<00:12, 99.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.47G/2.67G [00:15<00:12, 98.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:15<00:11, 99.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:15<00:11, 101MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:15<00:11, 99.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:15<00:11, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:15<00:11, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.53G/2.67G [00:15<00:11, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:15<00:11, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:15<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.56G/2.67G [00:15<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:16<00:10, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.63G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:16<00:10, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:16<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:16<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:16<00:09, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:17<00:09, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.69G/2.67G [00:17<00:09, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:17<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:17<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.72G/2.67G [00:17<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:17<00:09, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:17<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:17<00:08, 104MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:17<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:17<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:18<00:08, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.85G/2.67G [00:18<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:18<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:18<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.88G/2.67G [00:18<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.89G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.94G/2.67G [00:19<00:07, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.95G/2.67G [00:19<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:19<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [00:19<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.98G/2.67G [00:20<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:20<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.00G/2.67G [00:20<00:06, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [00:20<00:06, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:20<00:06, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.03G/2.67G [00:20<00:06, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.04G/2.67G [00:20<00:06, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:20<00:06, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:20<00:06, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [00:20<00:05, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:21<00:05, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.10G/2.67G [00:21<00:05, 100MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.11G/2.67G [00:21<00:05, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:21<00:05, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [00:21<00:05, 99.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:21<00:05, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [00:21<00:05, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [00:21<00:05, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:21<00:04, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:21<00:04, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [00:22<00:04, 99.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:22<00:04, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.21G/2.67G [00:22<00:04, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:22<00:04, 96.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:22<00:04, 98.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [00:22<00:04, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.25G/2.67G [00:22<00:04, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:22<00:03, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:22<00:03, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:23<00:03, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [00:23<00:03, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.31G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:23<00:03, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.37G/2.67G [00:23<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:23<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:24<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [00:24<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [00:24<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [00:24<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:24<00:02, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [00:24<00:02, 99.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:24<00:02, 100MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [00:24<00:02, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [00:24<00:01, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:24<00:01, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.51G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.53G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.56G/2.67G [00:25<00:01, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:25<00:00, 101MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.58G/2.67G [00:25<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:25<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [00:26<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [00:26<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [00:26<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [00:26<00:00, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.64G/2.67G [00:26<00:00, 103MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.65G/2.67G [00:26<00:00, 102MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:26<00:00, 99.7MB/s]\n",
            "Downloading shards: 100% 2/2 [01:17<00:00, 38.67s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-25 14:56:41,547 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-25 14:56:41,549 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.58s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-25 14:56:47,310 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-25 14:56:47,310 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.72MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-25 14:56:47,699 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-25 14:56:47,699 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/25/2024 14:56:47 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/25/2024 14:56:47 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/25/2024 14:56:47 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/25/2024 14:56:47 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/25/2024 14:56:47 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,down_proj,o_proj,gate_up_proj\n",
            "07/25/2024 14:56:48 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:642] 2024-07-25 14:56:48,041 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-25 14:56:48,447 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-25 14:56:48,447 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-25 14:56:48,447 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-25 14:56:48,447 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2134] 2024-07-25 14:56:48,447 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2135] 2024-07-25 14:56:48,447 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-25 14:56:48,447 >>   Total optimization steps = 168\n",
            "[INFO|trainer.py:2137] 2024-07-25 14:56:48,450 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/168 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.5097, 'grad_norm': 1.1717318296432495, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 37296}\n",
            "{'loss': 1.4538, 'grad_norm': 0.6610831618309021, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 76256}\n",
            "  6% 10/168 [02:49<44:29, 16.90s/it][INFO|trainer.py:3788] 2024-07-25 14:59:38,003 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:59:38,004 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:59:38,004 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.96it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.62it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.28it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.56it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.30it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.88it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.77it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.71it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.84it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.93it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.85it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:16,  2.07it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.22it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.31it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.49it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:15,  2.04it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.99it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.09it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.12it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.06it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.23it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.95it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.08it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.08it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.64it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.90it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:04,  1.94it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  2.00it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.84it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.80it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.60it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.038487195968628, 'eval_runtime': 26.0548, 'eval_samples_per_second': 3.838, 'eval_steps_per_second': 1.919, 'epoch': 0.18, 'num_input_tokens_seen': 76256}\n",
            "  6% 10/168 [03:15<44:29, 16.90s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 1.1787, 'grad_norm': 0.512626051902771, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.27, 'num_input_tokens_seen': 114240}\n",
            "{'loss': 1.0951, 'grad_norm': 0.4535239636898041, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 150128}\n",
            " 12% 20/168 [06:02<40:29, 16.42s/it][INFO|trainer.py:3788] 2024-07-25 15:02:50,993 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:02:50,993 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:02:50,993 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.93it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.63it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.29it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.56it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.31it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.88it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.73it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.78it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.71it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.85it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.94it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.85it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:16,  2.07it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.22it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.31it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.49it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:15,  2.04it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.99it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.09it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.06it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.24it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.95it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.08it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.09it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.40it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.34it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.80it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.79it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.68it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.65it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.90it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.90it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:04,  1.94it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  2.00it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:03,  1.98it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.59it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8940544128417969, 'eval_runtime': 26.0075, 'eval_samples_per_second': 3.845, 'eval_steps_per_second': 1.923, 'epoch': 0.36, 'num_input_tokens_seen': 150128}\n",
            " 12% 20/168 [06:28<40:29, 16.42s/it]\n",
            "100% 50/50 [00:25<00:00,  1.69it/s]\u001b[A\n",
            "{'loss': 1.1076, 'grad_norm': 0.4911997616291046, 'learning_rate': 4.731745523109029e-05, 'epoch': 0.44, 'num_input_tokens_seen': 188192}\n",
            "{'loss': 0.8921, 'grad_norm': 0.4631842076778412, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 224480}\n",
            " 18% 30/168 [09:16<38:52, 16.91s/it][INFO|trainer.py:3788] 2024-07-25 15:06:05,362 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:06:05,362 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:06:05,362 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.92it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.61it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.29it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.56it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.30it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.88it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.73it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.77it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.71it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.85it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.94it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.85it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:16,  2.07it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.22it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.31it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.48it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:15,  2.04it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  2.00it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.09it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.06it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.23it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.95it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.08it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.22it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.09it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.39it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.34it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.79it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.79it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.65it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.83it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.90it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.90it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:04,  1.95it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  2.00it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.84it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.80it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.60it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.59it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8571299910545349, 'eval_runtime': 25.9926, 'eval_samples_per_second': 3.847, 'eval_steps_per_second': 1.924, 'epoch': 0.53, 'num_input_tokens_seen': 224480}\n",
            " 18% 30/168 [09:42<38:52, 16.91s/it]\n",
            "100% 50/50 [00:25<00:00,  1.69it/s]\u001b[A\n",
            "{'loss': 0.9994, 'grad_norm': 0.35515710711479187, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 261840}\n",
            "{'loss': 1.0682, 'grad_norm': 0.33543142676353455, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 297488}\n",
            " 24% 40/168 [12:29<35:52, 16.82s/it][INFO|trainer.py:3788] 2024-07-25 15:09:17,704 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:09:17,705 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:09:17,705 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.58it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.54it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.31it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.26it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.53it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.29it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.87it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.03it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.18it/s]\u001b[A\n",
            " 34% 17/50 [00:08<00:14,  2.28it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
            " 50% 25/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:10,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.36it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.81it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.60it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.59it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8396031260490417, 'eval_runtime': 26.1901, 'eval_samples_per_second': 3.818, 'eval_steps_per_second': 1.909, 'epoch': 0.71, 'num_input_tokens_seen': 297488}\n",
            " 24% 40/168 [12:55<35:52, 16.82s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 1.0003, 'grad_norm': 0.560179591178894, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.8, 'num_input_tokens_seen': 334672}\n",
            "{'loss': 0.9384, 'grad_norm': 0.3438631296157837, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 370336}\n",
            " 30% 50/168 [15:41<33:16, 16.92s/it][INFO|trainer.py:3788] 2024-07-25 15:12:30,201 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:12:30,202 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:12:30,202 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.77it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.59it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.32it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.28it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.52it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.28it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.06it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.07it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.93it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.36it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.92it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.98it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.96it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8263943195343018, 'eval_runtime': 26.2034, 'eval_samples_per_second': 3.816, 'eval_steps_per_second': 1.908, 'epoch': 0.89, 'num_input_tokens_seen': 370336}\n",
            " 30% 50/168 [16:07<33:16, 16.92s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 0.7479, 'grad_norm': 0.4801520109176636, 'learning_rate': 3.790265684518767e-05, 'epoch': 0.98, 'num_input_tokens_seen': 404736}\n",
            "{'loss': 0.9572, 'grad_norm': 0.48357006907463074, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 442192}\n",
            " 36% 60/168 [18:51<30:19, 16.84s/it][INFO|trainer.py:3788] 2024-07-25 15:15:40,353 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:15:40,353 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:15:40,353 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.72it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.55it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.30it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.51it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.77it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.11it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.08it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8201797008514404, 'eval_runtime': 26.1705, 'eval_samples_per_second': 3.821, 'eval_steps_per_second': 1.911, 'epoch': 1.07, 'num_input_tokens_seen': 442192}\n",
            " 36% 60/168 [19:18<30:19, 16.84s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 1.0174, 'grad_norm': 0.34302377700805664, 'learning_rate': 3.369677161463068e-05, 'epoch': 1.16, 'num_input_tokens_seen': 479296}\n",
            "{'loss': 1.0616, 'grad_norm': 0.338623583316803, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 514080}\n",
            " 42% 70/168 [22:03<26:47, 16.40s/it][INFO|trainer.py:3788] 2024-07-25 15:18:51,512 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:18:51,512 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:18:51,512 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.68it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.56it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.32it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.28it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.53it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.28it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.77it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.71it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.45it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.07it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.93it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.64it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.81it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.98it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8237302899360657, 'eval_runtime': 26.187, 'eval_samples_per_second': 3.819, 'eval_steps_per_second': 1.909, 'epoch': 1.24, 'num_input_tokens_seen': 514080}\n",
            " 42% 70/168 [22:29<26:47, 16.40s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 0.8665, 'grad_norm': 0.30889368057250977, 'learning_rate': 2.918765558261841e-05, 'epoch': 1.33, 'num_input_tokens_seen': 548512}\n",
            "{'loss': 1.0293, 'grad_norm': 0.38323119282722473, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 587376}\n",
            " 48% 80/168 [25:15<26:03, 17.77s/it][INFO|trainer.py:3788] 2024-07-25 15:22:04,276 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:22:04,277 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:22:04,277 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.70it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.56it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.31it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.51it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:24,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.82it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.03it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.17it/s]\u001b[A\n",
            " 34% 17/50 [00:08<00:14,  2.26it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.41it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.00it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.96it/s]\u001b[A\n",
            " 42% 21/50 [00:10<00:14,  2.06it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.08it/s]\u001b[A\n",
            " 46% 23/50 [00:11<00:13,  2.04it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.20it/s]\u001b[A\n",
            " 50% 25/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.91it/s]\u001b[A\n",
            " 54% 27/50 [00:13<00:11,  2.05it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:10,  2.19it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.35it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.30it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.77it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.77it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:18<00:08,  1.64it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:19<00:06,  1.81it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.98it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8191865682601929, 'eval_runtime': 26.293, 'eval_samples_per_second': 3.803, 'eval_steps_per_second': 1.902, 'epoch': 1.42, 'num_input_tokens_seen': 587376}\n",
            " 48% 80/168 [25:42<26:03, 17.77s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 1.1452, 'grad_norm': 0.37895238399505615, 'learning_rate': 2.4532528339227452e-05, 'epoch': 1.51, 'num_input_tokens_seen': 626688}\n",
            "{'loss': 0.8673, 'grad_norm': 0.4394339323043823, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 660976}\n",
            " 54% 90/168 [28:29<21:34, 16.60s/it][INFO|trainer.py:3788] 2024-07-25 15:25:18,155 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:25:18,155 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:25:18,155 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.81it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.53it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.31it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.54it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.29it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.87it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.06it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.45it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.07it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.93it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.32it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.98it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.82it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.57it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8152318596839905, 'eval_runtime': 26.1852, 'eval_samples_per_second': 3.819, 'eval_steps_per_second': 1.909, 'epoch': 1.6, 'num_input_tokens_seen': 660976}\n",
            " 54% 90/168 [28:55<21:34, 16.60s/it]\n",
            "100% 50/50 [00:25<00:00,  1.67it/s]\u001b[A\n",
            "{'loss': 1.0185, 'grad_norm': 0.34992364048957825, 'learning_rate': 1.9893700455257996e-05, 'epoch': 1.69, 'num_input_tokens_seen': 699872}\n",
            "{'loss': 0.9927, 'grad_norm': 0.3612867295742035, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 738096}\n",
            " 60% 100/168 [31:49<19:55, 17.58s/it][INFO|trainer.py:3788] 2024-07-25 15:28:38,244 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:28:38,244 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:28:38,244 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.78it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.55it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.30it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.52it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.93it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.32it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.64it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.98it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.80it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.60it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8152772784233093, 'eval_runtime': 26.1686, 'eval_samples_per_second': 3.821, 'eval_steps_per_second': 1.911, 'epoch': 1.78, 'num_input_tokens_seen': 738096}\n",
            " 60% 100/168 [32:15<19:55, 17.58s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 1.0201, 'grad_norm': 0.3431640565395355, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.87, 'num_input_tokens_seen': 778080}\n",
            "{'loss': 0.8988, 'grad_norm': 0.37448611855506897, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 814672}\n",
            " 65% 110/168 [35:08<16:35, 17.17s/it][INFO|trainer.py:3788] 2024-07-25 15:31:57,102 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:31:57,102 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:31:57,102 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.55it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.51it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:20,  2.29it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.25it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.51it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.06it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.30it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.47it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.04it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.99it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:10,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.36it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.94it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8155746459960938, 'eval_runtime': 26.1852, 'eval_samples_per_second': 3.819, 'eval_steps_per_second': 1.909, 'epoch': 1.96, 'num_input_tokens_seen': 814672}\n",
            " 65% 110/168 [35:34<16:35, 17.17s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 0.9014, 'grad_norm': 0.3244176208972931, 'learning_rate': 1.130570401955322e-05, 'epoch': 2.04, 'num_input_tokens_seen': 850688}\n",
            "{'loss': 1.0911, 'grad_norm': 0.31825241446495056, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 887792}\n",
            " 71% 120/168 [38:21<13:47, 17.23s/it][INFO|trainer.py:3788] 2024-07-25 15:35:09,618 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:35:09,618 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:35:09,618 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.84it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.60it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.31it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.26it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.52it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.28it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.93it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.22it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.94it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.07it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.08it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.59it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8149203658103943, 'eval_runtime': 26.1419, 'eval_samples_per_second': 3.825, 'eval_steps_per_second': 1.913, 'epoch': 2.13, 'num_input_tokens_seen': 887792}\n",
            " 71% 120/168 [38:47<13:47, 17.23s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 0.9634, 'grad_norm': 0.3717946708202362, 'learning_rate': 7.65597359928646e-06, 'epoch': 2.22, 'num_input_tokens_seen': 926800}\n",
            "{'loss': 0.9852, 'grad_norm': 0.3132462799549103, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 963840}\n",
            " 77% 130/168 [41:38<11:10, 17.65s/it][INFO|trainer.py:3788] 2024-07-25 15:38:27,010 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:38:27,010 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:38:27,010 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.43it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.50it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:20,  2.29it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.26it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.51it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.27it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.85it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.04it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.18it/s]\u001b[A\n",
            " 34% 17/50 [00:08<00:14,  2.27it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:13,  2.45it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:10<00:13,  2.07it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.20it/s]\u001b[A\n",
            " 50% 25/50 [00:12<00:12,  1.93it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.05it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:10,  2.19it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.35it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:19<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.93it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.84it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.78it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.59it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8149449825286865, 'eval_runtime': 26.2236, 'eval_samples_per_second': 3.813, 'eval_steps_per_second': 1.907, 'epoch': 2.31, 'num_input_tokens_seen': 963840}\n",
            " 77% 130/168 [42:04<11:10, 17.65s/it]\n",
            "100% 50/50 [00:25<00:00,  1.69it/s]\u001b[A\n",
            "{'loss': 0.9433, 'grad_norm': 0.31733438372612, 'learning_rate': 4.610978276018496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1001424}\n",
            "{'loss': 1.1972, 'grad_norm': 0.4991397559642792, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1039792}\n",
            " 83% 140/168 [44:55<08:17, 17.76s/it][INFO|trainer.py:3788] 2024-07-25 15:41:44,371 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:41:44,371 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:41:44,371 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.81it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.58it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.32it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.53it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.29it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.87it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.77it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.93it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.83it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.47it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.02it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.07it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.93it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:10,  2.19it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.06it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.35it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.30it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.77it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.81it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.88it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.88it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.92it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.97it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.96it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.82it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.78it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.58it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.814422607421875, 'eval_runtime': 26.2086, 'eval_samples_per_second': 3.816, 'eval_steps_per_second': 1.908, 'epoch': 2.49, 'num_input_tokens_seen': 1039792}\n",
            " 83% 140/168 [45:22<08:17, 17.76s/it]\n",
            "100% 50/50 [00:25<00:00,  1.68it/s]\u001b[A\n",
            "{'loss': 0.9577, 'grad_norm': 0.38822904229164124, 'learning_rate': 2.2768880646947268e-06, 'epoch': 2.58, 'num_input_tokens_seen': 1075216}\n",
            "{'loss': 0.8638, 'grad_norm': 0.3908481001853943, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1111008}\n",
            " 89% 150/168 [48:04<05:00, 16.70s/it][INFO|trainer.py:3788] 2024-07-25 15:44:52,687 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:44:52,687 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:44:52,687 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.72it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.57it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.30it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.26it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.52it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.28it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.71it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.70it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.83it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.92it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.19it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.46it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.05it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.94it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.92it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.20it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.36it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.65it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.71it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.94it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  1.99it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.97it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.79it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.815035879611969, 'eval_runtime': 26.1936, 'eval_samples_per_second': 3.818, 'eval_steps_per_second': 1.909, 'epoch': 2.67, 'num_input_tokens_seen': 1111008}\n",
            " 89% 150/168 [48:30<05:00, 16.70s/it]\n",
            "100% 50/50 [00:25<00:00,  1.67it/s]\u001b[A\n",
            "{'loss': 0.842, 'grad_norm': 0.4511862099170685, 'learning_rate': 7.350858136652261e-07, 'epoch': 2.76, 'num_input_tokens_seen': 1148816}\n",
            "{'loss': 1.1676, 'grad_norm': 0.28919464349746704, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1187824}\n",
            " 95% 160/168 [51:24<02:25, 18.16s/it][INFO|trainer.py:3788] 2024-07-25 15:48:12,871 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:48:12,871 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:48:12,871 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:07,  6.55it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:13,  3.54it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.31it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.27it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:17,  2.53it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.28it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:22,  1.86it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.72it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:22,  1.76it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.71it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.84it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:19,  1.93it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:19,  1.84it/s]\u001b[A\n",
            " 30% 15/50 [00:07<00:17,  2.05it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:15,  2.20it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.47it/s]\u001b[A\n",
            " 38% 19/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:15,  1.98it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.08it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:13,  2.10it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:13,  2.04it/s]\u001b[A\n",
            " 48% 24/50 [00:11<00:11,  2.21it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  1.93it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.93it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:11,  2.06it/s]\u001b[A\n",
            " 56% 28/50 [00:13<00:09,  2.21it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:10,  2.07it/s]\u001b[A\n",
            " 60% 30/50 [00:14<00:08,  2.37it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:08,  2.31it/s]\u001b[A\n",
            " 64% 32/50 [00:15<00:10,  1.78it/s]\u001b[A\n",
            " 66% 33/50 [00:16<00:09,  1.78it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.67it/s]\u001b[A\n",
            " 70% 35/50 [00:17<00:09,  1.66it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.65it/s]\u001b[A\n",
            " 74% 37/50 [00:18<00:07,  1.64it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.82it/s]\u001b[A\n",
            " 78% 39/50 [00:19<00:06,  1.72it/s]\u001b[A\n",
            " 80% 40/50 [00:20<00:05,  1.89it/s]\u001b[A\n",
            " 82% 41/50 [00:20<00:04,  1.89it/s]\u001b[A\n",
            " 84% 42/50 [00:21<00:04,  1.94it/s]\u001b[A\n",
            " 86% 43/50 [00:21<00:03,  2.00it/s]\u001b[A\n",
            " 88% 44/50 [00:22<00:03,  1.98it/s]\u001b[A\n",
            " 90% 45/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 92% 46/50 [00:23<00:02,  1.77it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.78it/s]\u001b[A\n",
            " 96% 48/50 [00:24<00:01,  1.59it/s]\u001b[A\n",
            " 98% 49/50 [00:25<00:00,  1.58it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.8142320513725281, 'eval_runtime': 26.1783, 'eval_samples_per_second': 3.82, 'eval_steps_per_second': 1.91, 'epoch': 2.84, 'num_input_tokens_seen': 1187824}\n",
            " 95% 160/168 [51:50<02:25, 18.16s/it]\n",
            "100% 50/50 [00:25<00:00,  1.67it/s]\u001b[A\n",
            "{'loss': 1.0488, 'grad_norm': 0.3260374963283539, 'learning_rate': 3.9329624554584884e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1226848}\n",
            "100% 168/168 [54:06<00:00, 17.14s/it][INFO|trainer.py:3478] 2024-07-25 15:50:55,020 >> Saving model checkpoint to /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/checkpoint-168\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 15:50:55,911 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-25 15:50:55,912 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-25 15:50:56,104 >> tokenizer config file saved in /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/checkpoint-168/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-25 15:50:56,109 >> Special tokens file saved in /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/checkpoint-168/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-25 15:50:56,508 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3248.0579, 'train_samples_per_second': 0.831, 'train_steps_per_second': 0.052, 'train_loss': 1.0195864765417009, 'epoch': 2.99, 'num_input_tokens_seen': 1247792}\n",
            "100% 168/168 [54:08<00:00, 19.33s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-25 15:50:56,513 >> Saving model checkpoint to /content/drive/MyDrive/9900/phi3/QloRA/train_bz2\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 15:50:57,251 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-25 15:50:57,252 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-25 15:50:57,442 >> tokenizer config file saved in /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-25 15:50:57,447 >> Special tokens file saved in /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9867\n",
            "  num_input_tokens_seen    =    1247792\n",
            "  total_flos               = 26043719GF\n",
            "  train_loss               =     1.0196\n",
            "  train_runtime            = 0:54:08.05\n",
            "  train_samples_per_second =      0.831\n",
            "  train_steps_per_second   =      0.052\n",
            "Figure saved at: /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/phi3/QloRA/train_bz2/training_eval_loss.png\n",
            "07/25/2024 15:50:57 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-25 15:50:57,869 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 15:50:57,869 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 15:50:57,869 >>   Batch size = 2\n",
            "100% 50/50 [00:25<00:00,  1.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9867\n",
            "  eval_loss               =     0.8137\n",
            "  eval_runtime            = 0:00:26.38\n",
            "  eval_samples_per_second =      3.791\n",
            "  eval_steps_per_second   =      1.895\n",
            "  num_input_tokens_seen   =    1247792\n",
            "[INFO|modelcard.py:449] 2024-07-25 15:51:24,265 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlWpYnzljOkw",
        "outputId": "0a207e71-dfd5-40fc-d0f7-7ecd5d3172c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-21 13:47:41.419099: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-21 13:47:41.419153: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-21 13:47:41.420554: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-21 13:47:41.428105: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-21 13:47:42.674187: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/21/2024 13:47:50 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/21/2024 13:47:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 20.2MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 21.4MB/s]\n",
            "tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 2.21MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.13MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 4.22MB/s]\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 13:47:53,615 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 13:47:53,615 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 13:47:53,615 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 13:47:53,615 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 13:47:53,615 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-21 13:47:53,701 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/21/2024 13:47:53 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/21/2024 13:47:53 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/21/2024 13:47:53 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:00, 1492.65 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 1762.93 examples/s]\n",
            "07/21/2024 13:47:57 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 14267.68 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2228.38 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:01<00:00, 911.06 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 5.25MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 13:48:03,063 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 49.0MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 13:48:03,668 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 13:48:03,670 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/21/2024 13:48:03 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 4 bit with bitsandbytes.\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 434kB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 62.3MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-21 13:48:05,165 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:01<11:03, 7.48MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:01<06:06, 13.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:02<04:32, 18.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:02<03:48, 21.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:02<03:25, 23.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:03<03:10, 25.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.97G [00:03<03:00, 27.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:03<03:06, 26.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:04<03:14, 25.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:04<03:04, 26.3MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:05<03:00, 26.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:05<02:54, 27.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:05<02:50, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:06<02:57, 27.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:06<02:51, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:06<02:47, 28.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:07<02:45, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:07<02:42, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:07<02:40, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:08<02:39, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:08<02:38, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:08<02:38, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.97G [00:09<02:38, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:09<02:39, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:10<02:36, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:10<02:37, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:10<02:36, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:11<02:35, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:11<02:35, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:11<02:51, 27.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:12<02:45, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:12<02:41, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:12<02:39, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:13<02:37, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:13<02:35, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:13<02:34, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:14<02:32, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:14<02:32, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:15<02:33, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:15<02:30, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:15<02:30, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:16<02:30, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:16<02:29, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:16<02:30, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.97G [00:17<02:28, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:17<02:28, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:17<02:30, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:18<02:29, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:18<02:26, 30.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:18<02:27, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:19<02:38, 28.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:19<02:34, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.97G [00:19<02:32, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:20<02:29, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:20<02:34, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:21<02:27, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:21<02:26, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:21<02:25, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:22<02:24, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:22<02:24, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:22<02:24, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:23<02:25, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:23<02:24, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:23<02:23, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:24<02:46, 25.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:24<02:17, 31.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:24<02:16, 31.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:25<02:20, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:25<02:22, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:25<02:23, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:26<02:22, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.97G [00:26<02:21, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:27<02:20, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:27<02:20, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:27<02:19, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:28<02:19, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:28<02:19, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.97G [00:28<02:18, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:29<02:18, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:29<02:18, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:29<02:17, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.97G [00:30<02:17, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:30<02:16, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:30<02:16, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:31<02:15, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:31<02:14, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:31<02:14, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:32<02:13, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:32<02:13, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:32<02:13, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:33<02:13, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:33<02:12, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:34<02:12, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:34<02:12, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:34<02:12, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:35<02:12, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.97G [00:35<02:12, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:35<02:12, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:36<02:12, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:36<02:11, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:36<02:11, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:37<02:11, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:37<02:11, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:37<02:14, 28.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.97G [00:38<02:15, 28.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:38<02:13, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:39<02:11, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:39<02:09, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:39<02:09, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:40<02:07, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:40<02:08, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:40<02:05, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:41<02:05, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:41<02:04, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:41<02:04, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:42<02:04, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:42<02:08, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:42<02:07, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:43<02:05, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:43<02:04, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:43<02:03, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:44<02:02, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.97G [00:44<02:02, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:44<02:01, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:45<02:01, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:45<02:00, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:45<02:00, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:46<02:00, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:46<02:00, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:47<02:00, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:47<02:01, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:47<01:59, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:48<02:01, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:48<01:59, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:48<02:01, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:49<02:01, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:49<02:01, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:49<02:00, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:50<01:59, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:50<01:58, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:50<01:57, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:51<01:56, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [00:51<01:55, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:51<01:55, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:52<01:55, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:52<01:54, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:53<01:54, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:53<01:54, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:53<01:54, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:54<01:53, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:54<01:52, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:54<01:52, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:55<01:51, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:55<01:51, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:55<01:51, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:56<01:50, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:56<01:50, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:56<01:49, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:57<01:49, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:57<01:48, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:57<01:48, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:58<01:48, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:58<01:50, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:58<01:48, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:59<01:47, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:59<01:47, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [01:00<01:49, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [01:00<01:48, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [01:00<01:49, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.97G [01:01<01:48, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [01:01<01:47, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [01:01<01:46, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [01:02<01:46, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [01:02<01:45, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [01:02<01:44, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [01:03<01:44, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [01:03<01:44, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [01:03<01:43, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [01:04<01:43, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [01:04<01:45, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [01:05<01:47, 28.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [01:05<01:42, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [01:05<01:41, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [01:06<01:40, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [01:06<01:41, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [01:06<01:40, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [01:07<01:40, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [01:07<01:39, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [01:07<01:39, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [01:08<01:39, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [01:08<01:38, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [01:08<01:39, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [01:09<01:41, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [01:09<01:36, 30.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [01:09<01:36, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [01:10<01:36, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [01:10<01:36, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [01:10<01:36, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [01:11<01:36, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [01:11<01:36, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [01:11<01:36, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [01:12<01:36, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.97G [01:12<01:35, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [01:13<01:34, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [01:13<01:36, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [01:13<01:36, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [01:14<01:35, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [01:14<01:34, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [01:14<01:33, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [01:15<01:33, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [01:15<01:33, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [01:15<01:32, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [01:16<01:32, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [01:16<01:31, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.97G [01:16<01:31, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [01:17<01:30, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [01:17<01:44, 25.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.97G [01:18<01:42, 26.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [01:18<01:43, 25.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [01:18<01:37, 27.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.97G [01:19<01:35, 27.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [01:19<01:29, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [01:19<01:28, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [01:20<01:27, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [01:20<01:26, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [01:21<01:26, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [01:21<01:26, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.97G [01:21<01:25, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [01:22<01:25, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [01:22<01:25, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.97G [01:22<01:24, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [01:23<01:26, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [01:23<01:26, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [01:23<01:25, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [01:24<01:59, 20.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [01:25<01:47, 23.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [01:25<01:43, 24.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [01:25<01:33, 26.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [01:26<01:29, 27.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.97G [01:26<01:26, 28.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [01:26<01:24, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [01:27<01:23, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [01:27<01:22, 29.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [01:27<01:20, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [01:28<01:20, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [01:28<01:20, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [01:28<01:19, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [01:29<01:21, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [01:29<01:20, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [01:29<01:19, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [01:30<01:18, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [01:30<01:19, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [01:30<01:18, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [01:31<01:17, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [01:31<01:16, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [01:32<01:17, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [01:32<01:16, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [01:32<01:15, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [01:33<01:15, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [01:33<01:15, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [01:33<01:14, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [01:34<01:14, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [01:34<01:13, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [01:34<01:13, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [01:35<01:13, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [01:35<01:12, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [01:35<01:12, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [01:36<01:12, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [01:36<01:12, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [01:36<01:11, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [01:37<01:11, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [01:37<01:10, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [01:37<01:10, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [01:38<01:11, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [01:38<01:09, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.97G [01:39<01:09, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [01:39<01:08, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [01:39<01:08, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [01:40<01:07, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [01:40<01:07, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [01:40<01:06, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.97G [01:41<01:06, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [01:41<01:06, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [01:41<01:05, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.97G [01:42<01:05, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [01:42<01:05, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [01:42<01:05, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [01:43<01:04, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [01:43<01:07, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [01:43<01:06, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [01:44<01:06, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [01:44<01:06, 28.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [01:45<01:04, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [01:45<01:04, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [01:45<01:03, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [01:46<01:02, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [01:46<01:02, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [01:46<01:01, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [01:47<01:01, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [01:47<01:00, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [01:47<01:00, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [01:48<00:59, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [01:48<00:59, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.97G [01:48<00:59, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [01:49<00:58, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [01:49<00:58, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.97G [01:49<00:58, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [01:50<00:57, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [01:50<00:57, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.97G [01:50<00:57, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [01:51<00:56, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [01:51<00:56, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [01:51<00:56, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [01:52<00:55, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [01:52<00:55, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [01:53<00:55, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [01:53<00:54, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [01:53<00:54, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [01:54<00:54, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [01:54<00:53, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [01:54<00:53, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.97G [01:55<00:52, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [01:55<00:52, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.97G [01:55<00:52, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.97G [01:56<00:52, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [01:56<00:51, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [01:56<00:52, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [01:57<00:52, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [01:57<00:52, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [01:57<00:51, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [01:58<00:50, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [01:58<00:50, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [01:59<00:49, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.97G [01:59<00:49, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [01:59<00:48, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [02:00<00:48, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.97G [02:00<00:48, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [02:00<00:47, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [02:01<00:47, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [02:01<00:46, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [02:01<00:46, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [02:02<00:46, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [02:02<00:45, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [02:02<00:45, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [02:03<00:45, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [02:03<00:44, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [02:03<00:44, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [02:04<00:44, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.97G [02:04<00:43, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [02:04<00:43, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [02:05<00:42, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [02:05<00:42, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [02:06<00:42, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [02:06<00:41, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.97G [02:06<00:41, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [02:07<00:41, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [02:07<00:40, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [02:07<00:40, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [02:08<00:40, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [02:08<00:40, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [02:08<00:42, 28.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [02:09<00:38, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [02:09<00:38, 30.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [02:09<00:38, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [02:10<00:38, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [02:10<00:37, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [02:10<00:37, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [02:11<00:37, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [02:11<00:36, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.97G [02:11<00:36, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [02:12<00:36, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [02:12<00:36, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [02:13<00:35, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [02:13<00:35, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [02:13<00:35, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [02:14<00:34, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [02:14<00:34, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [02:14<00:33, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [02:15<00:33, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [02:15<00:32, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [02:15<00:32, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [02:16<00:32, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [02:16<00:31, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [02:16<00:31, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [02:17<00:31, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.97G [02:17<00:30, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [02:17<00:30, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [02:18<00:30, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [02:18<00:29, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [02:19<00:29, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [02:19<00:29, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.97G [02:19<00:28, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [02:20<00:28, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [02:20<00:28, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [02:20<00:27, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [02:21<00:27, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [02:21<00:26, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.97G [02:21<00:26, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [02:22<00:26, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [02:22<00:25, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [02:22<00:25, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [02:23<00:25, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [02:23<00:24, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [02:23<00:24, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [02:24<00:24, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [02:24<00:24, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [02:24<00:23, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [02:25<00:23, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [02:25<00:22, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.97G [02:26<00:23, 29.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [02:26<00:21, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [02:26<00:21, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [02:27<00:22, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [02:27<00:21, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [02:27<00:20, 30.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [02:28<00:20, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [02:28<00:20, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [02:28<00:19, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [02:29<00:19, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [02:29<00:19, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [02:29<00:18, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [02:30<00:18, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [02:30<00:17, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [02:30<00:17, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.97G [02:31<00:17, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [02:31<00:16, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.97G [02:31<00:16, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [02:32<00:16, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [02:32<00:15, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [02:33<00:15, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.97G [02:33<00:15, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [02:33<00:15, 29.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [02:34<00:14, 29.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [02:34<00:14, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [02:34<00:13, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [02:35<00:13, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [02:35<00:13, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [02:35<00:12, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [02:36<00:12, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [02:36<00:11, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [02:36<00:11, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [02:37<00:11, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [02:37<00:10, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [02:37<00:10, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.97G [02:38<00:10, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [02:38<00:09, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [02:38<00:09, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [02:39<00:09, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [02:39<00:08, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [02:40<00:08, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [02:40<00:08, 29.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [02:40<00:07, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [02:41<00:07, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [02:41<00:07, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [02:41<00:06, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.97G [02:42<00:06, 29.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [02:42<00:06, 29.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [02:42<00:05, 29.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [02:43<00:05, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [02:43<00:04, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [02:43<00:04, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [02:44<00:04, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [02:44<00:03, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [02:44<00:03, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [02:45<00:03, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [02:45<00:02, 29.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [02:46<00:02, 30.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [02:46<00:02, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [02:46<00:01, 30.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.97G [02:47<00:01, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [02:47<00:01, 29.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.97G [02:47<00:00, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.97G [02:48<00:00, 30.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [02:48<00:00, 29.5MB/s]\n",
            "Downloading shards:  50% 1/2 [02:48<02:48, 168.95s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/2.67G [00:00<01:20, 33.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:00<00:57, 45.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:00<00:52, 50.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:46, 56.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.67G [00:01<00:47, 55.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:01<00:45, 57.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:01<00:43, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:01<00:41, 62.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:01<00:45, 56.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 105M/2.67G [00:01<00:44, 58.3MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 115M/2.67G [00:02<00:43, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:02<00:42, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:02<00:40, 62.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:02<00:38, 66.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:02<00:37, 66.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 168M/2.67G [00:02<00:36, 67.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:03<00:39, 63.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:03<00:37, 65.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:03<00:38, 63.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 210M/2.67G [00:03<00:39, 62.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 220M/2.67G [00:03<00:39, 62.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:03<00:37, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:03<00:37, 64.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:04<00:38, 63.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:04<00:37, 63.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 273M/2.67G [00:04<00:38, 61.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:04<00:38, 61.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:04<00:40, 58.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:05<00:39, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 315M/2.67G [00:05<00:40, 57.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:05<00:40, 57.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:05<00:39, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:05<00:38, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:05<00:39, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:06<00:37, 61.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 377M/2.67G [00:06<00:36, 62.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:06<00:36, 63.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:06<00:35, 64.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:06<00:34, 66.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:06<00:35, 62.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 430M/2.67G [00:07<00:37, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:07<00:35, 63.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:07<00:41, 52.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:07<00:38, 56.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 472M/2.67G [00:07<00:38, 57.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:08<00:39, 55.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:08<00:39, 55.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:08<00:37, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:08<00:38, 56.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:08<00:39, 54.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 535M/2.67G [00:09<00:39, 54.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:09<00:36, 58.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:09<00:35, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:09<00:34, 60.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 577M/2.67G [00:09<00:32, 64.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:09<00:32, 63.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:10<00:35, 57.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:10<00:34, 60.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:10<00:35, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 629M/2.67G [00:10<00:38, 53.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 640M/2.67G [00:10<00:39, 51.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:11<00:40, 50.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:11<00:36, 54.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:11<00:35, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 682M/2.67G [00:11<00:36, 53.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:11<00:35, 55.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:11<00:35, 55.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:12<00:38, 50.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:12<00:36, 53.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:12<00:35, 54.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 744M/2.67G [00:12<00:39, 48.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:12<00:37, 51.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:13<00:36, 51.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:13<00:34, 55.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 786M/2.67G [00:13<00:33, 55.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:13<00:35, 53.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:13<00:34, 54.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:14<00:33, 56.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:14<00:32, 57.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 839M/2.67G [00:14<00:33, 54.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 849M/2.67G [00:14<00:32, 55.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:14<00:32, 55.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:15<00:31, 57.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:15<00:30, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 891M/2.67G [00:15<00:30, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:15<00:29, 60.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:15<00:30, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:15<00:31, 55.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:16<00:31, 54.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:16<00:37, 46.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:16<00:28, 59.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:16<00:30, 55.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:17<00:31, 54.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 996M/2.67G [00:17<00:33, 50.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:17<00:33, 49.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:17<00:39, 41.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:18<00:35, 46.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:18<00:34, 46.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:18<00:31, 51.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.06G/2.67G [00:18<00:33, 48.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:18<00:30, 52.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:19<00:28, 56.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:19<00:26, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:19<00:26, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:19<00:25, 61.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:19<00:25, 61.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:19<00:25, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.14G/2.67G [00:20<00:29, 51.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:20<00:27, 55.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:20<00:25, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:20<00:25, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:20<00:26, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:21<00:27, 54.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:21<00:28, 51.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.22G/2.67G [00:21<00:30, 48.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:21<00:29, 49.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:21<00:30, 47.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.25G/2.67G [00:22<00:32, 43.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:22<00:29, 47.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:22<00:26, 52.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:22<00:27, 51.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:22<00:25, 54.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.30G/2.67G [00:23<00:22, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:23<00:22, 59.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:23<00:23, 58.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:23<00:21, 61.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:23<00:20, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:23<00:21, 62.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.36G/2.67G [00:24<00:22, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:24<00:21, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:24<00:20, 62.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:24<00:20, 60.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:24<00:21, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:25<00:20, 60.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:25<00:21, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:25<00:20, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:25<00:19, 63.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:25<00:19, 62.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.47G/2.67G [00:25<00:20, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:26<00:20, 59.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:26<00:19, 62.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:26<00:19, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:26<00:19, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:26<00:21, 54.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.53G/2.67G [00:26<00:19, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:27<00:18, 60.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:27<00:17, 62.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.56G/2.67G [00:27<00:16, 65.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:27<00:17, 61.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:27<00:17, 61.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:27<00:17, 61.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:28<00:17, 60.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:28<00:16, 62.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.63G/2.67G [00:28<00:15, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:28<00:15, 66.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:28<00:19, 51.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:29<00:15, 63.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:29<00:15, 64.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.69G/2.67G [00:29<00:15, 64.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:29<00:15, 62.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:29<00:16, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.72G/2.67G [00:30<00:16, 56.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:30<00:16, 56.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:30<00:15, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:30<00:16, 57.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:30<00:16, 55.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:31<00:19, 45.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [00:31<00:18, 48.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:31<00:16, 52.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [00:31<00:15, 54.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:31<00:16, 52.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [00:32<00:15, 54.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:32<00:14, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.85G/2.67G [00:32<00:13, 60.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:32<00:13, 60.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:32<00:12, 62.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.88G/2.67G [00:32<00:13, 60.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.89G/2.67G [00:33<00:13, 58.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:33<00:12, 60.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [00:33<00:12, 61.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [00:33<00:11, 63.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:33<00:11, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.94G/2.67G [00:33<00:11, 61.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.95G/2.67G [00:34<00:13, 55.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:34<00:12, 56.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [00:34<00:11, 59.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.98G/2.67G [00:34<00:11, 61.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:34<00:10, 62.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.00G/2.67G [00:34<00:11, 60.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [00:35<00:11, 54.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:35<00:11, 56.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.03G/2.67G [00:35<00:11, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.04G/2.67G [00:35<00:10, 58.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:35<00:10, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:36<00:10, 59.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [00:36<00:10, 58.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:36<00:09, 59.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.10G/2.67G [00:36<00:09, 61.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.11G/2.67G [00:36<00:09, 59.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:36<00:09, 57.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [00:37<00:09, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:37<00:09, 56.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [00:37<00:08, 59.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [00:37<00:08, 61.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:37<00:07, 64.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:37<00:07, 65.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [00:38<00:07, 66.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:38<00:07, 63.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.21G/2.67G [00:38<00:06, 65.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:38<00:06, 64.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:38<00:06, 67.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [00:38<00:06, 68.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.25G/2.67G [00:39<00:06, 68.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:39<00:06, 64.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:39<00:06, 62.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:39<00:06, 60.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [00:39<00:06, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.31G/2.67G [00:39<00:06, 60.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:40<00:06, 58.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:40<00:05, 58.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [00:40<00:05, 62.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [00:40<00:05, 62.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:40<00:05, 62.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.37G/2.67G [00:40<00:04, 63.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:41<00:04, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:41<00:04, 66.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [00:41<00:04, 65.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [00:41<00:03, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [00:41<00:04, 61.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:41<00:03, 61.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [00:42<00:03, 62.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:42<00:03, 61.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [00:42<00:03, 64.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [00:42<00:02, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:42<00:02, 66.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:42<00:02, 66.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.51G/2.67G [00:43<00:02, 66.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:43<00:02, 68.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.53G/2.67G [00:43<00:02, 66.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:43<00:01, 68.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:43<00:01, 64.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.56G/2.67G [00:43<00:01, 65.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:43<00:01, 67.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.58G/2.67G [00:44<00:01, 64.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:44<00:01, 65.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [00:44<00:01, 52.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [00:44<00:00, 65.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [00:44<00:00, 64.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.64G/2.67G [00:45<00:00, 63.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.65G/2.67G [00:45<00:00, 63.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:45<00:00, 58.5MB/s]\n",
            "Downloading shards: 100% 2/2 [03:34<00:00, 107.47s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-21 13:51:40,108 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 13:51:40,109 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.93s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-21 13:51:46,821 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-21 13:51:46,821 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.21MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-21 13:51:47,210 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 13:51:47,211 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,down_proj,gate_up_proj,qkv_proj\n",
            "07/21/2024 13:51:47 - INFO - llamafactory.model.loader - trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6543\n",
            "[INFO|trainer.py:642] 2024-07-21 13:51:47,820 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-21 13:51:48,292 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-21 13:51:48,292 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-21 13:51:48,292 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-21 13:51:48,292 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-21 13:51:48,292 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-21 13:51:48,292 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-21 13:51:48,292 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-21 13:51:48,296 >>   Number of trainable parameters = 25,165,824\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.7219, 'grad_norm': 0.513229250907898, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.5583, 'grad_norm': 0.46987658739089966, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [08:11<36:26, 49.69s/it][INFO|trainer.py:3788] 2024-07-21 13:59:59,425 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 13:59:59,425 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 13:59:59,425 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.81it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:15,  3.03it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:22,  2.03it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:22,  1.96it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:20,  2.17it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:21,  1.98it/s]\u001b[A\n",
            " 16% 8/50 [00:04<00:25,  1.64it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:27,  1.52it/s]\u001b[A\n",
            " 20% 10/50 [00:05<00:25,  1.55it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:25,  1.50it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:23,  1.59it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:22,  1.65it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:22,  1.59it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:20,  1.75it/s]\u001b[A\n",
            " 32% 16/50 [00:08<00:18,  1.87it/s]\u001b[A\n",
            " 34% 17/50 [00:09<00:17,  1.92it/s]\u001b[A\n",
            " 36% 18/50 [00:09<00:15,  2.03it/s]\u001b[A\n",
            " 38% 19/50 [00:10<00:17,  1.74it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:17,  1.70it/s]\u001b[A\n",
            " 42% 21/50 [00:11<00:16,  1.77it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:15,  1.78it/s]\u001b[A\n",
            " 46% 23/50 [00:12<00:15,  1.74it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.87it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:15,  1.66it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:14,  1.66it/s]\u001b[A\n",
            " 54% 27/50 [00:15<00:13,  1.77it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:11,  1.88it/s]\u001b[A\n",
            " 58% 29/50 [00:16<00:11,  1.77it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.02it/s]\u001b[A\n",
            " 62% 31/50 [00:16<00:09,  1.96it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:11,  1.56it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:11,  1.54it/s]\u001b[A\n",
            " 68% 34/50 [00:19<00:10,  1.47it/s]\u001b[A\n",
            " 70% 35/50 [00:20<00:10,  1.46it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.45it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:08,  1.45it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:07,  1.58it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:07,  1.50it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:06,  1.63it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:05,  1.62it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.65it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.69it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.69it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:03,  1.59it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.55it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.57it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.41it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.40it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.0303678512573242, 'eval_runtime': 30.2421, 'eval_samples_per_second': 3.307, 'eval_steps_per_second': 1.653, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [08:41<36:26, 49.69s/it]\n",
            "100% 50/50 [00:29<00:00,  1.48it/s]\u001b[A\n",
            "{'loss': 1.3578, 'grad_norm': 0.29871049523353577, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.2017, 'grad_norm': 0.226945698261261, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [16:41<27:41, 48.87s/it][INFO|trainer.py:3788] 2024-07-21 14:08:29,712 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:08:29,713 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:08:29,713 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.66it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:15,  3.00it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:22,  2.01it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:23,  1.95it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:20,  2.16it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:21,  1.97it/s]\u001b[A\n",
            " 16% 8/50 [00:04<00:25,  1.64it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:27,  1.51it/s]\u001b[A\n",
            " 20% 10/50 [00:05<00:25,  1.54it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:25,  1.50it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:23,  1.59it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:22,  1.65it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:22,  1.60it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:19,  1.76it/s]\u001b[A\n",
            " 32% 16/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
            " 34% 17/50 [00:09<00:17,  1.93it/s]\u001b[A\n",
            " 36% 18/50 [00:09<00:15,  2.04it/s]\u001b[A\n",
            " 38% 19/50 [00:10<00:17,  1.74it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:17,  1.71it/s]\u001b[A\n",
            " 42% 21/50 [00:11<00:16,  1.76it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:15,  1.78it/s]\u001b[A\n",
            " 46% 23/50 [00:12<00:15,  1.74it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.87it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:15,  1.66it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:14,  1.66it/s]\u001b[A\n",
            " 54% 27/50 [00:15<00:13,  1.77it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:11,  1.88it/s]\u001b[A\n",
            " 58% 29/50 [00:16<00:11,  1.77it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.02it/s]\u001b[A\n",
            " 62% 31/50 [00:17<00:09,  1.95it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:11,  1.56it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:11,  1.55it/s]\u001b[A\n",
            " 68% 34/50 [00:19<00:10,  1.47it/s]\u001b[A\n",
            " 70% 35/50 [00:20<00:10,  1.46it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.45it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:08,  1.45it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:07,  1.57it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:07,  1.50it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:06,  1.63it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:05,  1.62it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.65it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.70it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.70it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:03,  1.59it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.56it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.58it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.42it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.41it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.9090538024902344, 'eval_runtime': 30.2161, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 1.655, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [17:11<27:41, 48.87s/it]\n",
            "100% 50/50 [00:29<00:00,  1.48it/s]\u001b[A\n",
            "{'loss': 1.2694, 'grad_norm': 0.25397178530693054, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.2654, 'grad_norm': 0.18048636615276337, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [25:11<19:26, 48.62s/it][INFO|trainer.py:3788] 2024-07-21 14:17:00,046 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:17:00,046 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:17:00,046 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.63it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:15,  3.03it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:22,  2.03it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:22,  1.97it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:20,  2.16it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:21,  1.99it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:25,  1.65it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:26,  1.52it/s]\u001b[A\n",
            " 20% 10/50 [00:05<00:25,  1.56it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:25,  1.52it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:23,  1.60it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:22,  1.67it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:22,  1.60it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:19,  1.76it/s]\u001b[A\n",
            " 32% 16/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
            " 34% 17/50 [00:09<00:17,  1.93it/s]\u001b[A\n",
            " 36% 18/50 [00:09<00:15,  2.05it/s]\u001b[A\n",
            " 38% 19/50 [00:10<00:17,  1.75it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:17,  1.71it/s]\u001b[A\n",
            " 42% 21/50 [00:11<00:16,  1.78it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:15,  1.80it/s]\u001b[A\n",
            " 46% 23/50 [00:12<00:15,  1.76it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.89it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:14,  1.68it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:14,  1.68it/s]\u001b[A\n",
            " 54% 27/50 [00:14<00:12,  1.78it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:11,  1.90it/s]\u001b[A\n",
            " 58% 29/50 [00:16<00:11,  1.78it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.03it/s]\u001b[A\n",
            " 62% 31/50 [00:16<00:09,  1.97it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:11,  1.57it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:10,  1.56it/s]\u001b[A\n",
            " 68% 34/50 [00:19<00:10,  1.48it/s]\u001b[A\n",
            " 70% 35/50 [00:19<00:10,  1.47it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.46it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:08,  1.45it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:07,  1.58it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:07,  1.50it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:06,  1.63it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:05,  1.63it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.66it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.70it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.70it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:03,  1.59it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.55it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.58it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.42it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.41it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8810114860534668, 'eval_runtime': 30.0751, 'eval_samples_per_second': 3.325, 'eval_steps_per_second': 1.663, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [25:41<19:26, 48.62s/it]\n",
            "100% 50/50 [00:29<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 1.2503, 'grad_norm': 0.15677663683891296, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.2237, 'grad_norm': 0.1980447769165039, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [33:52<11:24, 48.86s/it][INFO|trainer.py:3788] 2024-07-21 14:25:41,021 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:25:41,021 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:25:41,021 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.69it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:15,  3.05it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:22,  2.03it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:22,  1.97it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:20,  2.17it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:21,  1.98it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:25,  1.64it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:27,  1.52it/s]\u001b[A\n",
            " 20% 10/50 [00:05<00:25,  1.54it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:25,  1.51it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:23,  1.59it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:22,  1.66it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:22,  1.60it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:19,  1.76it/s]\u001b[A\n",
            " 32% 16/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
            " 34% 17/50 [00:09<00:17,  1.93it/s]\u001b[A\n",
            " 36% 18/50 [00:09<00:15,  2.06it/s]\u001b[A\n",
            " 38% 19/50 [00:10<00:17,  1.75it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:17,  1.71it/s]\u001b[A\n",
            " 42% 21/50 [00:11<00:16,  1.78it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:15,  1.79it/s]\u001b[A\n",
            " 46% 23/50 [00:12<00:15,  1.75it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.88it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:14,  1.67it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:14,  1.67it/s]\u001b[A\n",
            " 54% 27/50 [00:14<00:12,  1.77it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:11,  1.88it/s]\u001b[A\n",
            " 58% 29/50 [00:16<00:11,  1.77it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.02it/s]\u001b[A\n",
            " 62% 31/50 [00:16<00:09,  1.96it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:11,  1.56it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:11,  1.54it/s]\u001b[A\n",
            " 68% 34/50 [00:19<00:10,  1.47it/s]\u001b[A\n",
            " 70% 35/50 [00:20<00:10,  1.46it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.45it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:08,  1.45it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:07,  1.58it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:07,  1.50it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:06,  1.63it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:05,  1.63it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.66it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.70it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.70it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:03,  1.59it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.55it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.58it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.42it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.41it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8711969256401062, 'eval_runtime': 30.1318, 'eval_samples_per_second': 3.319, 'eval_steps_per_second': 1.659, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [34:22<11:24, 48.86s/it]\n",
            "100% 50/50 [00:29<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 1.1865, 'grad_norm': 0.21203872561454773, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.1996, 'grad_norm': 0.1928565949201584, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [42:23<03:09, 47.28s/it][INFO|trainer.py:3788] 2024-07-21 14:34:11,991 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:34:11,991 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:34:11,991 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:08,  5.71it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:15,  2.99it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:22,  2.02it/s]\u001b[A\n",
            " 10% 5/50 [00:02<00:23,  1.95it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:20,  2.16it/s]\u001b[A\n",
            " 14% 7/50 [00:03<00:21,  1.97it/s]\u001b[A\n",
            " 16% 8/50 [00:04<00:25,  1.64it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:27,  1.52it/s]\u001b[A\n",
            " 20% 10/50 [00:05<00:25,  1.54it/s]\u001b[A\n",
            " 22% 11/50 [00:06<00:26,  1.50it/s]\u001b[A\n",
            " 24% 12/50 [00:06<00:23,  1.59it/s]\u001b[A\n",
            " 26% 13/50 [00:07<00:22,  1.65it/s]\u001b[A\n",
            " 28% 14/50 [00:07<00:22,  1.60it/s]\u001b[A\n",
            " 30% 15/50 [00:08<00:19,  1.76it/s]\u001b[A\n",
            " 32% 16/50 [00:08<00:18,  1.88it/s]\u001b[A\n",
            " 34% 17/50 [00:09<00:17,  1.93it/s]\u001b[A\n",
            " 36% 18/50 [00:09<00:15,  2.05it/s]\u001b[A\n",
            " 38% 19/50 [00:10<00:17,  1.75it/s]\u001b[A\n",
            " 40% 20/50 [00:11<00:17,  1.71it/s]\u001b[A\n",
            " 42% 21/50 [00:11<00:16,  1.77it/s]\u001b[A\n",
            " 44% 22/50 [00:12<00:15,  1.78it/s]\u001b[A\n",
            " 46% 23/50 [00:12<00:15,  1.74it/s]\u001b[A\n",
            " 48% 24/50 [00:13<00:13,  1.87it/s]\u001b[A\n",
            " 50% 25/50 [00:13<00:15,  1.67it/s]\u001b[A\n",
            " 52% 26/50 [00:14<00:14,  1.67it/s]\u001b[A\n",
            " 54% 27/50 [00:15<00:13,  1.77it/s]\u001b[A\n",
            " 56% 28/50 [00:15<00:11,  1.88it/s]\u001b[A\n",
            " 58% 29/50 [00:16<00:11,  1.77it/s]\u001b[A\n",
            " 60% 30/50 [00:16<00:09,  2.02it/s]\u001b[A\n",
            " 62% 31/50 [00:16<00:09,  1.96it/s]\u001b[A\n",
            " 64% 32/50 [00:17<00:11,  1.56it/s]\u001b[A\n",
            " 66% 33/50 [00:18<00:11,  1.54it/s]\u001b[A\n",
            " 68% 34/50 [00:19<00:10,  1.47it/s]\u001b[A\n",
            " 70% 35/50 [00:20<00:10,  1.46it/s]\u001b[A\n",
            " 72% 36/50 [00:20<00:09,  1.45it/s]\u001b[A\n",
            " 74% 37/50 [00:21<00:08,  1.45it/s]\u001b[A\n",
            " 76% 38/50 [00:21<00:07,  1.57it/s]\u001b[A\n",
            " 78% 39/50 [00:22<00:07,  1.50it/s]\u001b[A\n",
            " 80% 40/50 [00:23<00:06,  1.63it/s]\u001b[A\n",
            " 82% 41/50 [00:23<00:05,  1.63it/s]\u001b[A\n",
            " 84% 42/50 [00:24<00:04,  1.65it/s]\u001b[A\n",
            " 86% 43/50 [00:24<00:04,  1.69it/s]\u001b[A\n",
            " 88% 44/50 [00:25<00:03,  1.69it/s]\u001b[A\n",
            " 90% 45/50 [00:26<00:03,  1.59it/s]\u001b[A\n",
            " 92% 46/50 [00:26<00:02,  1.55it/s]\u001b[A\n",
            " 94% 47/50 [00:27<00:01,  1.57it/s]\u001b[A\n",
            " 96% 48/50 [00:28<00:01,  1.41it/s]\u001b[A\n",
            " 98% 49/50 [00:29<00:00,  1.40it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8676299452781677, 'eval_runtime': 30.2337, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 1.654, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [42:53<03:09, 47.28s/it]\n",
            "100% 50/50 [00:29<00:00,  1.48it/s]\u001b[A\n",
            "100% 54/54 [46:09<00:00, 51.78s/it][INFO|trainer.py:3478] 2024-07-21 14:37:58,015 >> Saving model checkpoint to /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 14:37:58,840 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 14:37:58,841 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 14:37:59,199 >> tokenizer config file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 14:37:59,204 >> Special tokens file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-21 14:37:59,913 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2771.6177, 'train_samples_per_second': 0.974, 'train_steps_per_second': 0.019, 'train_loss': 1.3167672333893952, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [46:11<00:00, 51.33s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-21 14:37:59,918 >> Saving model checkpoint to /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 14:38:00,362 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 14:38:00,363 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 14:38:00,713 >> tokenizer config file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 14:38:00,716 >> Special tokens file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 31015165GF\n",
            "  train_loss               =     1.3168\n",
            "  train_runtime            = 0:46:11.61\n",
            "  train_samples_per_second =      0.974\n",
            "  train_steps_per_second   =      0.019\n",
            "Figure saved at: /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16/training_eval_loss.png\n",
            "07/21/2024 14:38:01 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-21 14:38:01,137 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:38:01,138 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:38:01,138 >>   Batch size = 2\n",
            "100% 50/50 [00:29<00:00,  1.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     0.8679\n",
            "  eval_runtime            = 0:00:30.23\n",
            "  eval_samples_per_second =      3.308\n",
            "  eval_steps_per_second   =      1.654\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:449] 2024-07-21 14:38:31,385 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/phi3_QLoRA_Rank/train_Rank8_NF8 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESrd5qZhjZ1M",
        "outputId": "e9687079-e10f-42f2-8935-1e0e16f0b3c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 10:29:02.401200: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 10:29:02.401253: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 10:29:02.402671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 10:29:02.409995: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 10:29:03.589047: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 10:29:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 23.6MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 3.35MB/s]\n",
            "tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 2.22MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.15MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 3.99MB/s]\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 10:29:14,492 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 10:29:14,492 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 10:29:14,492 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 10:29:14,492 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 10:29:14,492 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 10:29:14,568 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 10:29:14 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 10:29:14 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 10:29:14 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "Generating train split: 100 examples [00:00, 3457.17 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 449.04 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 115.17 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 5.98MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 10:29:17,917 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 47.6MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 10:29:18,555 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 10:29:18,556 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 10:29:18 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 200MB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 60.2MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 10:29:19,874 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<01:20, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:00<01:06, 74.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<01:03, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<01:01, 80.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:00<01:02, 79.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:00<01:01, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.97G [00:00<01:00, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:01<00:59, 81.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:01<01:01, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:01<01:01, 78.6MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:01<01:03, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:01<01:01, 78.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:01<01:00, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:01<00:59, 80.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:01<00:59, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:02<00:59, 80.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:02<00:58, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:02<00:57, 82.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:02<00:57, 82.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:02<00:58, 82.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:02<00:58, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:02<00:56, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.97G [00:02<00:56, 84.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:03<00:56, 84.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:03<00:56, 82.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:03<00:58, 80.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:03<00:58, 80.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:03<00:57, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:03<01:00, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:03<00:59, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:04<00:58, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:04<00:58, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:04<00:57, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:04<00:56, 81.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:04<00:56, 81.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:04<00:58, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:04<00:58, 79.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:04<00:57, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:05<01:02, 73.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:05<00:59, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:05<00:59, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:05<00:58, 77.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:05<00:59, 75.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:05<00:58, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.97G [00:05<00:58, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:06<00:56, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:06<00:55, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:06<00:56, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:06<00:57, 77.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:06<00:55, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:06<00:55, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:06<00:55, 80.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.97G [00:06<00:54, 80.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:07<00:55, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:07<00:54, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:07<00:54, 81.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:07<00:54, 81.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:07<00:53, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:07<00:55, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:07<00:54, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:08<00:53, 81.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:08<00:53, 80.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:08<00:53, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:08<00:53, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:08<00:54, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:08<00:56, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:08<00:55, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:08<00:54, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:09<00:54, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:09<00:53, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:09<00:52, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.97G [00:09<00:53, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:09<00:52, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:09<00:53, 78.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:09<00:52, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:10<00:51, 81.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:10<00:50, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.97G [00:10<00:51, 81.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:10<00:51, 80.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:10<00:52, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:10<00:51, 80.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.97G [00:10<00:51, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:10<00:52, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:11<00:52, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:11<00:50, 80.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:11<00:50, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:11<00:49, 81.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:11<00:49, 81.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:11<00:49, 80.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:11<00:49, 81.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:11<00:50, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:12<00:50, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:12<00:50, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:12<00:50, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:12<00:52, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:12<00:52, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.97G [00:12<00:51, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:12<00:52, 75.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:13<00:51, 75.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:13<00:51, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:13<01:15, 51.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:13<00:55, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:13<00:53, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.97G [00:14<00:53, 72.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:14<00:51, 74.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:14<00:52, 73.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:14<00:52, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:14<00:51, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:14<00:50, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:14<00:51, 73.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:15<00:51, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:15<00:52, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:15<00:51, 73.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:15<00:49, 76.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:15<00:48, 77.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:15<00:50, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:15<00:49, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:15<00:47, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:16<00:47, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:16<00:46, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:16<00:45, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.97G [00:16<00:46, 80.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:16<00:45, 81.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:16<00:44, 81.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:16<00:44, 81.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:16<00:44, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:17<00:46, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:17<00:48, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:17<00:47, 75.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:17<00:46, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:17<00:46, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:17<00:46, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:17<00:46, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:18<00:45, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:18<00:46, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:18<00:45, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:18<00:45, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:18<00:46, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:18<00:46, 74.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:18<00:46, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:19<00:45, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [00:19<00:45, 76.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:19<00:44, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:19<00:44, 77.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:19<00:44, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:19<00:46, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:19<00:45, 74.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:20<00:45, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:20<00:45, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:20<00:54, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:20<00:50, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:20<00:47, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:20<00:46, 72.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:20<00:44, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:21<00:44, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:21<00:45, 72.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:21<00:48, 68.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:21<00:45, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:21<00:44, 74.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:21<00:49, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:22<00:46, 69.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:22<00:45, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:22<00:44, 73.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:22<00:47, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:22<00:44, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:22<00:42, 75.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [00:22<00:41, 77.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:22<00:41, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.97G [00:23<00:40, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:23<00:40, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:23<00:40, 78.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [00:23<00:39, 79.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:23<00:39, 79.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:23<00:38, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:23<00:39, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:24<00:39, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:24<00:38, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:24<00:38, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [00:24<00:39, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:24<00:38, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [00:24<00:37, 81.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:24<00:39, 77.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:24<00:38, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [00:25<00:38, 79.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:25<00:37, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:25<00:36, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [00:25<00:38, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:25<00:37, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:25<00:39, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [00:25<00:38, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:26<00:37, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:26<00:36, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:26<00:37, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [00:26<00:39, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:26<00:40, 71.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [00:26<00:40, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:26<00:39, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:27<00:39, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [00:27<00:38, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [00:27<00:37, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:27<00:36, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.97G [00:27<00:36, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:27<00:36, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:27<00:36, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [00:27<00:36, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:28<00:35, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:28<00:36, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [00:28<00:36, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [00:28<00:37, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:28<00:36, 74.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:28<00:35, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [00:28<00:34, 78.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:29<00:35, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.97G [00:29<00:34, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [00:29<00:34, 78.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:29<00:33, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.97G [00:29<00:34, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [00:29<00:33, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:29<00:33, 79.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.97G [00:29<00:33, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [00:30<00:32, 80.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:30<00:33, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:30<00:33, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [00:30<00:33, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:30<00:33, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:30<00:33, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.97G [00:30<00:38, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:31<00:39, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [00:31<00:37, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.97G [00:31<00:35, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:31<00:34, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:31<00:33, 75.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:31<00:32, 76.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:31<00:32, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:32<00:31, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [00:32<00:31, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:32<00:31, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:32<00:30, 80.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.97G [00:32<00:31, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:32<00:31, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:32<00:32, 75.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:33<00:33, 72.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:33<00:32, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [00:33<00:32, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [00:33<00:32, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:33<00:32, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [00:33<00:32, 73.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:33<00:31, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:34<00:30, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [00:34<00:29, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:34<00:29, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:34<00:29, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:34<00:29, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:34<00:30, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:34<00:29, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:34<00:29, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:35<00:29, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:35<00:29, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:35<00:29, 77.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:35<00:28, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:35<00:29, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [00:35<00:28, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:35<00:28, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [00:36<00:27, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:36<00:27, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:36<00:27, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:36<00:27, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:36<00:27, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:36<00:26, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:36<00:27, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:36<00:27, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:37<00:30, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:37<00:28, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:37<00:27, 75.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.97G [00:37<00:28, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:37<00:28, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:37<00:27, 74.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:37<00:27, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:38<00:27, 75.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [00:38<00:27, 73.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.97G [00:38<00:26, 75.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [00:38<00:25, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:38<00:25, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.97G [00:38<00:24, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:38<00:24, 80.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:39<00:24, 81.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:39<00:24, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:39<00:24, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:39<00:24, 80.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:39<00:23, 82.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [00:39<00:23, 82.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:39<00:23, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [00:39<00:23, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:40<00:28, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:40<00:29, 62.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [00:40<00:27, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:40<00:26, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:40<00:24, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:40<00:26, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:41<00:29, 61.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:41<00:28, 63.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:41<00:27, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.97G [00:41<00:26, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:41<00:25, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:42<00:37, 46.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:42<00:26, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:42<00:24, 71.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.97G [00:42<00:23, 73.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:42<00:23, 73.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:42<00:23, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [00:42<00:22, 74.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:43<00:22, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:43<00:22, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:43<00:21, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:43<00:21, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:43<00:21, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [00:43<00:22, 72.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [00:44<00:24, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:44<00:24, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.97G [00:44<00:22, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:44<00:21, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:44<00:21, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.97G [00:44<00:20, 75.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:44<00:20, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:44<00:19, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [00:45<00:19, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [00:45<00:19, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:45<00:19, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [00:45<00:19, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [00:45<00:19, 76.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:45<00:19, 75.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.97G [00:45<00:19, 74.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [00:46<00:19, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:46<00:18, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.97G [00:46<00:18, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:46<00:18, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [00:46<00:18, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [00:46<00:18, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:46<00:17, 77.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [00:46<00:17, 78.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:47<00:17, 78.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [00:47<00:17, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [00:47<00:17, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:47<00:17, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [00:47<00:16, 79.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [00:47<00:16, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.97G [00:47<00:16, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [00:48<00:16, 80.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:48<00:17, 75.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [00:48<00:16, 75.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [00:48<00:16, 76.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:48<00:16, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.97G [00:48<00:15, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [00:48<00:15, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [00:48<00:15, 79.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [00:49<00:15, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [00:49<00:15, 79.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [00:49<00:14, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:49<00:14, 81.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [00:49<00:14, 81.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [00:49<00:14, 81.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [00:49<00:15, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [00:50<00:15, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:50<00:15, 74.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [00:50<00:15, 74.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [00:50<00:14, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [00:50<00:14, 77.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.97G [00:50<00:14, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:50<00:14, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [00:51<00:13, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [00:51<00:13, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [00:51<00:13, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:51<00:13, 79.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [00:51<00:12, 79.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [00:51<00:13, 78.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [00:51<00:13, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:51<00:13, 75.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [00:52<00:12, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [00:52<00:12, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [00:52<00:12, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [00:52<00:12, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [00:52<00:12, 76.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:52<00:12, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.97G [00:52<00:12, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:53<00:12, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [00:53<00:12, 70.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:53<00:12, 72.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [00:53<00:12, 72.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [00:53<00:11, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.97G [00:53<00:11, 73.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [00:53<00:11, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:54<00:11, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [00:54<00:10, 76.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [00:54<00:10, 74.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:54<00:10, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.97G [00:54<00:10, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:54<00:10, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [00:54<00:10, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [00:55<00:09, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [00:55<00:09, 78.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:55<00:09, 77.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [00:55<00:09, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [00:55<00:09, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [00:55<00:09, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [00:55<00:08, 79.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [00:55<00:08, 79.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [00:56<00:08, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.97G [00:56<00:08, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [00:56<00:08, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [00:56<00:08, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [00:56<00:08, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [00:56<00:08, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [00:56<00:08, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:57<00:07, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [00:57<00:07, 76.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [00:57<00:07, 73.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [00:57<00:07, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [00:57<00:07, 73.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [00:57<00:07, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [00:57<00:07, 74.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [00:58<00:07, 73.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [00:58<00:07, 74.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.97G [00:58<00:07, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:58<00:06, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.97G [00:58<00:06, 72.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [00:58<00:06, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [00:58<00:06, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [00:59<00:06, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.97G [00:59<00:05, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [00:59<00:05, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [00:59<00:05, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [00:59<00:05, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [00:59<00:05, 78.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [00:59<00:05, 78.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [00:59<00:05, 74.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [01:00<00:05, 75.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [01:00<00:04, 77.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [01:00<00:04, 76.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [01:00<00:04, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [01:00<00:04, 76.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [01:00<00:04, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [01:00<00:04, 77.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.97G [01:01<00:03, 78.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [01:01<00:03, 79.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [01:01<00:03, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [01:01<00:03, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [01:01<00:03, 77.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [01:01<00:03, 79.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [01:01<00:03, 80.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [01:01<00:02, 80.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [01:02<00:02, 80.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [01:02<00:02, 80.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [01:02<00:02, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.97G [01:02<00:02, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [01:02<00:02, 76.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [01:02<00:02, 77.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [01:02<00:02, 76.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [01:03<00:01, 78.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [01:03<00:01, 79.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [01:03<00:01, 79.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [01:03<00:01, 78.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [01:03<00:01, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [01:03<00:01, 75.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [01:03<00:01, 76.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [01:04<00:00, 78.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [01:04<00:00, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [01:04<00:01, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [01:04<00:00, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.97G [01:04<00:00, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.97G [01:05<00:00, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [01:05<00:00, 76.3MB/s]\n",
            "Downloading shards:  50% 1/2 [01:05<01:05, 65.59s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/2.67G [00:00<01:03, 41.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:00<00:45, 57.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:00<00:39, 67.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:37, 70.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.67G [00:00<00:36, 71.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:00<00:34, 74.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:01<00:34, 75.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:01<00:33, 77.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:01<00:38, 67.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 105M/2.67G [00:01<00:36, 70.8MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 115M/2.67G [00:01<00:34, 73.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:01<00:33, 75.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:01<00:33, 76.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:02<00:32, 78.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:02<00:31, 79.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 168M/2.67G [00:02<00:31, 80.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:02<00:30, 81.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:02<00:30, 81.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:02<00:32, 76.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 210M/2.67G [00:02<00:35, 68.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 220M/2.67G [00:03<00:33, 72.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:03<00:33, 72.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:03<00:32, 74.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:03<00:32, 74.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:03<00:31, 77.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 273M/2.67G [00:03<00:30, 78.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:03<00:30, 78.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:04<00:37, 64.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:04<00:34, 68.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 315M/2.67G [00:04<00:33, 70.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:04<00:32, 72.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:04<00:32, 72.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:04<00:31, 74.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:04<00:31, 73.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:05<00:38, 59.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 377M/2.67G [00:05<00:35, 64.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:05<00:33, 68.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:05<00:31, 72.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:05<00:30, 74.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:05<00:29, 77.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 430M/2.67G [00:05<00:28, 79.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:06<00:27, 80.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:06<00:28, 78.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:06<00:28, 77.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 472M/2.67G [00:06<00:28, 76.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:06<00:27, 79.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:06<00:32, 68.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:06<00:30, 69.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:07<00:29, 72.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:07<00:30, 70.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 535M/2.67G [00:07<00:28, 74.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:07<00:28, 75.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:07<00:26, 78.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:07<00:26, 80.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 577M/2.67G [00:07<00:29, 69.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:08<00:28, 73.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:08<00:27, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:08<00:26, 76.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:08<00:26, 77.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 629M/2.67G [00:08<00:27, 73.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 640M/2.67G [00:08<00:26, 75.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:08<00:26, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:09<00:31, 63.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:09<00:30, 66.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 682M/2.67G [00:09<00:28, 69.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:09<00:27, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:09<00:39, 49.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:10<00:28, 67.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:10<00:30, 64.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 744M/2.67G [00:10<00:29, 66.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:10<00:27, 69.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:10<00:25, 73.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:10<00:25, 74.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 786M/2.67G [00:10<00:24, 75.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:11<00:24, 76.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:11<00:23, 78.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:11<00:26, 69.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:11<00:25, 71.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 839M/2.67G [00:11<00:25, 71.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 849M/2.67G [00:11<00:24, 73.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:11<00:23, 76.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:12<00:23, 77.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:12<00:22, 77.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 891M/2.67G [00:12<00:22, 78.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:12<00:23, 76.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:12<00:23, 74.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:12<00:23, 75.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:12<00:22, 76.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:13<00:26, 64.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 954M/2.67G [00:13<00:25, 67.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:13<00:24, 68.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:13<00:23, 71.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:13<00:23, 70.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 996M/2.67G [00:13<00:23, 72.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:13<00:22, 73.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:14<00:27, 59.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:14<00:25, 64.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:14<00:24, 67.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:14<00:23, 68.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.06G/2.67G [00:14<00:22, 70.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:14<00:22, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:14<00:21, 75.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:15<00:20, 75.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:15<00:23, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:15<00:22, 69.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:15<00:21, 72.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:15<00:20, 74.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.14G/2.67G [00:15<00:19, 76.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:15<00:19, 78.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:16<00:19, 79.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:16<00:18, 79.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:16<00:22, 65.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:16<00:21, 68.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:16<00:20, 71.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.22G/2.67G [00:16<00:20, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:16<00:19, 74.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:17<00:18, 77.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.25G/2.67G [00:17<00:18, 78.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:17<00:17, 78.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:17<00:17, 78.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:17<00:17, 77.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:17<00:19, 70.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.30G/2.67G [00:17<00:19, 71.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:18<00:18, 73.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:18<00:17, 75.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:18<00:17, 77.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:18<00:16, 78.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:18<00:16, 78.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.36G/2.67G [00:18<00:16, 78.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:19<00:20, 61.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:19<00:20, 63.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:19<00:19, 66.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:19<00:18, 69.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:19<00:17, 70.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:19<00:16, 73.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:19<00:16, 74.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:20<00:19, 63.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:20<00:18, 67.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.47G/2.67G [00:20<00:17, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:20<00:16, 70.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:20<00:16, 71.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:20<00:15, 73.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:20<00:15, 76.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:21<00:14, 77.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.53G/2.67G [00:21<00:17, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:21<00:16, 69.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:21<00:15, 72.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.56G/2.67G [00:21<00:15, 73.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:21<00:14, 75.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:21<00:14, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:22<00:14, 76.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:22<00:13, 77.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:22<00:14, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.63G/2.67G [00:22<00:13, 74.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:22<00:13, 75.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:22<00:13, 75.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:22<00:13, 76.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:23<00:13, 74.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:23<00:13, 76.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.69G/2.67G [00:23<00:13, 71.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:23<00:13, 73.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:23<00:15, 63.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.72G/2.67G [00:23<00:14, 67.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:23<00:13, 69.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:24<00:12, 72.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:24<00:12, 71.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:24<00:12, 73.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:24<00:12, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [00:24<00:11, 75.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:24<00:16, 52.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:25<00:11, 71.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [00:25<00:11, 72.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:25<00:11, 73.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.85G/2.67G [00:25<00:11, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [00:25<00:10, 75.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:25<00:10, 76.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.88G/2.67G [00:26<00:12, 65.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.89G/2.67G [00:26<00:12, 62.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:26<00:11, 66.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [00:26<00:11, 68.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [00:26<00:10, 71.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:26<00:09, 74.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.94G/2.67G [00:26<00:09, 74.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.95G/2.67G [00:27<00:09, 73.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:27<00:09, 76.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [00:27<00:08, 78.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.98G/2.67G [00:27<00:08, 79.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:27<00:08, 80.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.00G/2.67G [00:27<00:08, 79.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [00:27<00:08, 79.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:27<00:08, 80.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.03G/2.67G [00:28<00:09, 63.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.04G/2.67G [00:28<00:09, 66.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:28<00:08, 69.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [00:28<00:08, 70.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [00:28<00:08, 71.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:28<00:08, 72.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.10G/2.67G [00:29<00:07, 73.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.11G/2.67G [00:29<00:09, 61.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:29<00:08, 63.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [00:29<00:07, 68.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:29<00:07, 70.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [00:29<00:07, 71.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [00:29<00:06, 72.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [00:30<00:06, 72.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:30<00:06, 74.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [00:30<00:07, 64.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:30<00:06, 67.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.21G/2.67G [00:30<00:06, 70.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:30<00:06, 71.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:31<00:05, 73.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [00:31<00:05, 72.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.25G/2.67G [00:31<00:05, 76.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:31<00:05, 77.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [00:31<00:05, 69.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:31<00:05, 72.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [00:31<00:05, 74.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.31G/2.67G [00:31<00:04, 76.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:32<00:04, 75.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [00:32<00:04, 76.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [00:32<00:04, 77.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [00:32<00:04, 78.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [00:32<00:04, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.37G/2.67G [00:32<00:03, 75.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:32<00:03, 75.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:33<00:03, 77.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [00:33<00:03, 74.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [00:33<00:03, 76.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [00:33<00:03, 73.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:33<00:03, 60.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [00:33<00:03, 64.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:34<00:03, 65.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [00:34<00:03, 65.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [00:34<00:02, 67.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [00:34<00:02, 68.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:34<00:02, 70.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.51G/2.67G [00:34<00:02, 71.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:35<00:02, 60.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.53G/2.67G [00:35<00:02, 64.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:35<00:01, 69.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:35<00:01, 69.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.56G/2.67G [00:35<00:01, 72.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:35<00:01, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.58G/2.67G [00:35<00:01, 72.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:35<00:01, 74.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [00:36<00:00, 74.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [00:36<00:00, 76.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [00:36<00:00, 61.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [00:36<00:00, 66.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.64G/2.67G [00:36<00:00, 71.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.65G/2.67G [00:36<00:00, 74.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:37<00:00, 72.0MB/s]\n",
            "Downloading shards: 100% 2/2 [01:43<00:00, 51.57s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 10:31:03,013 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 10:31:03,015 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.43s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 10:31:05,942 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 10:31:05,942 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.55MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 10:31:06,333 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 10:31:06,334 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 10:31:06 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 10:31:13 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 10:31:13 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF4\n",
            "07/23/2024 10:31:13 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-23 10:31:13,419 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 10:31:13,419 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 10:31:13,419 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-23 10:31:13,684 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [42:44<00:00, 47.39s/it]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.662 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [42:45<00:00, 51.32s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    26.5774\n",
            "  predict_rouge-1            =    33.8597\n",
            "  predict_rouge-2            =    11.6676\n",
            "  predict_rouge-l            =    21.1282\n",
            "  predict_runtime            = 0:43:19.76\n",
            "  predict_samples_per_second =      0.038\n",
            "  predict_steps_per_second   =      0.019\n",
            "07/23/2024 11:14:33 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank16_nf4_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 4 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/phi3_QLoRA_Rank/eval_qLoRA_Rank16_nf4_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/phi3_QLoRA_Rank/train_Rank16_NF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgbI6UeFjZ6H",
        "collapsed": true,
        "outputId": "556ad9be-66c6-4460-b484-162faf92133e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 11:14:38.374960: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 11:14:38.375007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 11:14:38.376463: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 11:14:38.384368: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 11:14:39.521790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 11:14:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:14:46,056 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:14:46,056 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:14:46,056 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:14:46,056 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:14:46,056 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 11:14:46,119 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 11:14:46 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 11:14:46 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 11:14:46 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "Generating train split: 1273 examples [00:00, 2532.26 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 420.20 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 114.08 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:14:51,162 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:14:51,545 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 11:14:51,546 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 11:14:51 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 11:14:51,775 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 11:14:51,776 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:14:51,777 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.62s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 11:14:55,069 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 11:14:55,069 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 11:14:55,266 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:14:55,266 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 11:14:55 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 11:14:56 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 11:14:56 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF4\n",
            "07/23/2024 11:14:56 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-23 11:14:56,445 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 11:14:56,446 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 11:14:56,446 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-23 11:14:56,494 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 34/34 [13:09<00:00, 12.55s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.631 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [13:10<00:00, 23.25s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    47.2045\n",
            "  predict_rouge-1            =     52.801\n",
            "  predict_rouge-2            =     42.343\n",
            "  predict_rouge-l            =    51.7696\n",
            "  predict_runtime            = 0:13:22.68\n",
            "  predict_samples_per_second =      0.125\n",
            "  predict_steps_per_second   =      0.042\n",
            "07/23/2024 11:28:19 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/LoRA_Rank/eval_qLoRA_Rank16_nf4_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 4 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/phi3_QLoRA_Rank/eval_qLoRA_Rank16_nf4_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/phi3_QLoRA_Rank/train_Rank16_NF4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIgu36b0jKMK",
        "outputId": "a708e7eb-13f5-48b2-d021-8e9779127ead",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-21 14:38:40.974976: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-21 14:38:40.975037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-21 14:38:40.976470: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-21 14:38:40.984579: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-21 14:38:42.326741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/21/2024 14:38:49 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/21/2024 14:38:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 14:38:49,560 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 14:38:49,560 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 14:38:49,560 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 14:38:49,560 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 14:38:49,560 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-21 14:38:49,634 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/21/2024 14:38:49 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/21/2024 14:38:49 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/21/2024 14:38:49 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/21/2024 14:38:50 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 14:38:51,352 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 14:38:51,748 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 14:38:51,749 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/21/2024 14:38:51 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-21 14:38:51,972 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-21 14:38:51,973 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 14:38:51,974 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.91s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-21 14:38:58,085 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-21 14:38:58,085 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-21 14:38:58,300 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 14:38:58,301 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,o_proj,gate_up_proj,down_proj\n",
            "07/21/2024 14:38:58 - INFO - llamafactory.model.loader - trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6543\n",
            "[INFO|trainer.py:642] 2024-07-21 14:38:58,777 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-21 14:38:59,199 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-21 14:38:59,199 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-21 14:38:59,199 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-21 14:38:59,199 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-21 14:38:59,199 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-21 14:38:59,199 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-21 14:38:59,199 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-21 14:38:59,203 >>   Number of trainable parameters = 25,165,824\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.689, 'grad_norm': 0.5149361491203308, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.5298, 'grad_norm': 0.4276316165924072, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [08:27<37:24, 51.01s/it][INFO|trainer.py:3788] 2024-07-21 14:47:26,973 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:47:26,973 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:47:26,973 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.89it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.66it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.39it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.34it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:16,  2.61it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.36it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:21,  1.93it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.77it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:21,  1.83it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.76it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.90it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:18,  2.00it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:18,  1.91it/s]\u001b[A\n",
            " 30% 15/50 [00:06<00:16,  2.13it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:14,  2.28it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:13,  2.37it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.55it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:14,  2.10it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:14,  2.05it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.15it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:12,  2.18it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:12,  2.13it/s]\u001b[A\n",
            " 48% 24/50 [00:10<00:11,  2.30it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  2.01it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:11,  2.00it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:10,  2.15it/s]\u001b[A\n",
            " 56% 28/50 [00:12<00:09,  2.30it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:09,  2.16it/s]\u001b[A\n",
            " 60% 30/50 [00:13<00:08,  2.46it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:07,  2.40it/s]\u001b[A\n",
            " 64% 32/50 [00:14<00:09,  1.85it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.85it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.73it/s]\u001b[A\n",
            " 70% 35/50 [00:16<00:08,  1.72it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.71it/s]\u001b[A\n",
            " 74% 37/50 [00:17<00:07,  1.71it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.89it/s]\u001b[A\n",
            " 78% 39/50 [00:18<00:06,  1.79it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.97it/s]\u001b[A\n",
            " 82% 41/50 [00:19<00:04,  1.96it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:03,  2.01it/s]\u001b[A\n",
            " 86% 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:02,  2.04it/s]\u001b[A\n",
            " 90% 45/50 [00:21<00:02,  1.89it/s]\u001b[A\n",
            " 92% 46/50 [00:22<00:02,  1.84it/s]\u001b[A\n",
            " 94% 47/50 [00:22<00:01,  1.85it/s]\u001b[A\n",
            " 96% 48/50 [00:23<00:01,  1.65it/s]\u001b[A\n",
            " 98% 49/50 [00:24<00:00,  1.64it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.006522536277771, 'eval_runtime': 25.2429, 'eval_samples_per_second': 3.962, 'eval_steps_per_second': 1.981, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [08:53<37:24, 51.01s/it]\n",
            "100% 50/50 [00:24<00:00,  1.74it/s]\u001b[A\n",
            "{'loss': 1.3366, 'grad_norm': 0.32523083686828613, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.1762, 'grad_norm': 0.22132082283496857, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [17:06<28:22, 50.07s/it][INFO|trainer.py:3788] 2024-07-21 14:56:05,245 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 14:56:05,245 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 14:56:05,245 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  7.03it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.70it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.39it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.33it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:16,  2.61it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.35it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:21,  1.93it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.77it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:21,  1.82it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.76it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.90it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:18,  1.99it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:18,  1.91it/s]\u001b[A\n",
            " 30% 15/50 [00:06<00:16,  2.13it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:14,  2.27it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:13,  2.37it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.54it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:14,  2.09it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:14,  2.05it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.14it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:12,  2.17it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:12,  2.12it/s]\u001b[A\n",
            " 48% 24/50 [00:10<00:11,  2.29it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  2.00it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.99it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:10,  2.13it/s]\u001b[A\n",
            " 56% 28/50 [00:12<00:09,  2.28it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:09,  2.14it/s]\u001b[A\n",
            " 60% 30/50 [00:13<00:08,  2.44it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:07,  2.40it/s]\u001b[A\n",
            " 64% 32/50 [00:14<00:09,  1.85it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.85it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.73it/s]\u001b[A\n",
            " 70% 35/50 [00:16<00:08,  1.72it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.71it/s]\u001b[A\n",
            " 74% 37/50 [00:17<00:07,  1.70it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.88it/s]\u001b[A\n",
            " 78% 39/50 [00:18<00:06,  1.77it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.95it/s]\u001b[A\n",
            " 82% 41/50 [00:19<00:04,  1.94it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:04,  1.99it/s]\u001b[A\n",
            " 86% 43/50 [00:20<00:03,  2.04it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:02,  2.03it/s]\u001b[A\n",
            " 90% 45/50 [00:21<00:02,  1.89it/s]\u001b[A\n",
            " 92% 46/50 [00:22<00:02,  1.83it/s]\u001b[A\n",
            " 94% 47/50 [00:23<00:01,  1.85it/s]\u001b[A\n",
            " 96% 48/50 [00:23<00:01,  1.65it/s]\u001b[A\n",
            " 98% 49/50 [00:24<00:00,  1.64it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8846345543861389, 'eval_runtime': 25.314, 'eval_samples_per_second': 3.95, 'eval_steps_per_second': 1.975, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [17:31<28:22, 50.07s/it]\n",
            "100% 50/50 [00:24<00:00,  1.74it/s]\u001b[A\n",
            "{'loss': 1.2459, 'grad_norm': 0.16572298109531403, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.2462, 'grad_norm': 0.2065998911857605, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [25:44<19:58, 49.93s/it][INFO|trainer.py:3788] 2024-07-21 15:04:43,565 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 15:04:43,565 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 15:04:43,565 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.87it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.69it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.40it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.36it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:16,  2.63it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.37it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:21,  1.95it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:22,  1.79it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:21,  1.84it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:21,  1.77it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:19,  1.91it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:18,  2.00it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:18,  1.92it/s]\u001b[A\n",
            " 30% 15/50 [00:06<00:16,  2.14it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:14,  2.30it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:13,  2.39it/s]\u001b[A\n",
            " 36% 18/50 [00:07<00:12,  2.57it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:14,  2.11it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.16it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:12,  2.18it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:12,  2.13it/s]\u001b[A\n",
            " 48% 24/50 [00:10<00:11,  2.30it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  2.01it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.99it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:10,  2.14it/s]\u001b[A\n",
            " 56% 28/50 [00:12<00:09,  2.28it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:09,  2.15it/s]\u001b[A\n",
            " 60% 30/50 [00:13<00:08,  2.44it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:07,  2.39it/s]\u001b[A\n",
            " 64% 32/50 [00:14<00:09,  1.84it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.84it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.72it/s]\u001b[A\n",
            " 70% 35/50 [00:16<00:08,  1.71it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.71it/s]\u001b[A\n",
            " 74% 37/50 [00:17<00:07,  1.71it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.89it/s]\u001b[A\n",
            " 78% 39/50 [00:18<00:06,  1.78it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.95it/s]\u001b[A\n",
            " 82% 41/50 [00:19<00:04,  1.96it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:03,  2.00it/s]\u001b[A\n",
            " 86% 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:02,  2.05it/s]\u001b[A\n",
            " 90% 45/50 [00:21<00:02,  1.90it/s]\u001b[A\n",
            " 92% 46/50 [00:22<00:02,  1.85it/s]\u001b[A\n",
            " 94% 47/50 [00:22<00:01,  1.86it/s]\u001b[A\n",
            " 96% 48/50 [00:23<00:01,  1.66it/s]\u001b[A\n",
            " 98% 49/50 [00:24<00:00,  1.65it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8568159341812134, 'eval_runtime': 25.1988, 'eval_samples_per_second': 3.968, 'eval_steps_per_second': 1.984, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [26:09<19:58, 49.93s/it]\n",
            "100% 50/50 [00:24<00:00,  1.75it/s]\u001b[A\n",
            "{'loss': 1.2298, 'grad_norm': 0.15258745849132538, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.1989, 'grad_norm': 0.1514197587966919, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [34:32<11:40, 50.00s/it][INFO|trainer.py:3788] 2024-07-21 15:13:31,873 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 15:13:31,873 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 15:13:31,873 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.92it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.70it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.41it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:19,  2.36it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:16,  2.62it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.37it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:21,  1.93it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:23,  1.78it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:21,  1.83it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:22,  1.77it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:20,  1.90it/s]\u001b[A\n",
            " 26% 13/50 [00:06<00:18,  1.99it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:18,  1.90it/s]\u001b[A\n",
            " 30% 15/50 [00:06<00:16,  2.12it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:14,  2.27it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:13,  2.36it/s]\u001b[A\n",
            " 36% 18/50 [00:08<00:12,  2.55it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:14,  2.10it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.16it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:12,  2.18it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:12,  2.13it/s]\u001b[A\n",
            " 48% 24/50 [00:10<00:11,  2.29it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  2.01it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  2.00it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:10,  2.14it/s]\u001b[A\n",
            " 56% 28/50 [00:12<00:09,  2.29it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:09,  2.15it/s]\u001b[A\n",
            " 60% 30/50 [00:13<00:08,  2.46it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:07,  2.40it/s]\u001b[A\n",
            " 64% 32/50 [00:14<00:09,  1.85it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.85it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.73it/s]\u001b[A\n",
            " 70% 35/50 [00:16<00:08,  1.72it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.71it/s]\u001b[A\n",
            " 74% 37/50 [00:17<00:07,  1.70it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.88it/s]\u001b[A\n",
            " 78% 39/50 [00:18<00:06,  1.77it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.95it/s]\u001b[A\n",
            " 82% 41/50 [00:19<00:04,  1.95it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:03,  2.00it/s]\u001b[A\n",
            " 86% 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:02,  2.05it/s]\u001b[A\n",
            " 90% 45/50 [00:21<00:02,  1.90it/s]\u001b[A\n",
            " 92% 46/50 [00:22<00:02,  1.85it/s]\u001b[A\n",
            " 94% 47/50 [00:22<00:01,  1.87it/s]\u001b[A\n",
            " 96% 48/50 [00:23<00:01,  1.66it/s]\u001b[A\n",
            " 98% 49/50 [00:24<00:00,  1.65it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8483554124832153, 'eval_runtime': 25.2286, 'eval_samples_per_second': 3.964, 'eval_steps_per_second': 1.982, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [34:57<11:40, 50.00s/it]\n",
            "100% 50/50 [00:24<00:00,  1.75it/s]\u001b[A\n",
            "{'loss': 1.1607, 'grad_norm': 0.20631854236125946, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.181, 'grad_norm': 0.1887480467557907, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [43:10<03:13, 48.39s/it][INFO|trainer.py:3788] 2024-07-21 15:22:10,000 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 15:22:10,000 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 15:22:10,000 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:06,  6.86it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:12,  3.70it/s]\u001b[A\n",
            "  8% 4/50 [00:01<00:19,  2.41it/s]\u001b[A\n",
            " 10% 5/50 [00:01<00:18,  2.37it/s]\u001b[A\n",
            " 12% 6/50 [00:02<00:16,  2.64it/s]\u001b[A\n",
            " 14% 7/50 [00:02<00:18,  2.38it/s]\u001b[A\n",
            " 16% 8/50 [00:03<00:21,  1.95it/s]\u001b[A\n",
            " 18% 9/50 [00:04<00:22,  1.79it/s]\u001b[A\n",
            " 20% 10/50 [00:04<00:21,  1.83it/s]\u001b[A\n",
            " 22% 11/50 [00:05<00:21,  1.77it/s]\u001b[A\n",
            " 24% 12/50 [00:05<00:19,  1.91it/s]\u001b[A\n",
            " 26% 13/50 [00:05<00:18,  2.00it/s]\u001b[A\n",
            " 28% 14/50 [00:06<00:18,  1.92it/s]\u001b[A\n",
            " 30% 15/50 [00:06<00:16,  2.14it/s]\u001b[A\n",
            " 32% 16/50 [00:07<00:14,  2.29it/s]\u001b[A\n",
            " 34% 17/50 [00:07<00:13,  2.38it/s]\u001b[A\n",
            " 36% 18/50 [00:07<00:12,  2.56it/s]\u001b[A\n",
            " 38% 19/50 [00:08<00:14,  2.11it/s]\u001b[A\n",
            " 40% 20/50 [00:09<00:14,  2.06it/s]\u001b[A\n",
            " 42% 21/50 [00:09<00:13,  2.16it/s]\u001b[A\n",
            " 44% 22/50 [00:10<00:12,  2.19it/s]\u001b[A\n",
            " 46% 23/50 [00:10<00:12,  2.13it/s]\u001b[A\n",
            " 48% 24/50 [00:10<00:11,  2.30it/s]\u001b[A\n",
            " 50% 25/50 [00:11<00:12,  2.00it/s]\u001b[A\n",
            " 52% 26/50 [00:12<00:12,  1.99it/s]\u001b[A\n",
            " 54% 27/50 [00:12<00:10,  2.14it/s]\u001b[A\n",
            " 56% 28/50 [00:12<00:09,  2.29it/s]\u001b[A\n",
            " 58% 29/50 [00:13<00:09,  2.15it/s]\u001b[A\n",
            " 60% 30/50 [00:13<00:08,  2.46it/s]\u001b[A\n",
            " 62% 31/50 [00:14<00:07,  2.41it/s]\u001b[A\n",
            " 64% 32/50 [00:14<00:09,  1.85it/s]\u001b[A\n",
            " 66% 33/50 [00:15<00:09,  1.85it/s]\u001b[A\n",
            " 68% 34/50 [00:16<00:09,  1.74it/s]\u001b[A\n",
            " 70% 35/50 [00:16<00:08,  1.72it/s]\u001b[A\n",
            " 72% 36/50 [00:17<00:08,  1.71it/s]\u001b[A\n",
            " 74% 37/50 [00:17<00:07,  1.71it/s]\u001b[A\n",
            " 76% 38/50 [00:18<00:06,  1.89it/s]\u001b[A\n",
            " 78% 39/50 [00:18<00:06,  1.78it/s]\u001b[A\n",
            " 80% 40/50 [00:19<00:05,  1.96it/s]\u001b[A\n",
            " 82% 41/50 [00:19<00:04,  1.96it/s]\u001b[A\n",
            " 84% 42/50 [00:20<00:03,  2.01it/s]\u001b[A\n",
            " 86% 43/50 [00:20<00:03,  2.06it/s]\u001b[A\n",
            " 88% 44/50 [00:21<00:02,  2.05it/s]\u001b[A\n",
            " 90% 45/50 [00:21<00:02,  1.90it/s]\u001b[A\n",
            " 92% 46/50 [00:22<00:02,  1.85it/s]\u001b[A\n",
            " 94% 47/50 [00:22<00:01,  1.86it/s]\u001b[A\n",
            " 96% 48/50 [00:23<00:01,  1.66it/s]\u001b[A\n",
            " 98% 49/50 [00:24<00:00,  1.65it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8421683311462402, 'eval_runtime': 25.1548, 'eval_samples_per_second': 3.975, 'eval_steps_per_second': 1.988, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [43:35<03:13, 48.39s/it]\n",
            "100% 50/50 [00:24<00:00,  1.75it/s]\u001b[A\n",
            "100% 54/54 [46:56<00:00, 52.45s/it][INFO|trainer.py:3478] 2024-07-21 15:25:56,020 >> Saving model checkpoint to /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 15:25:56,489 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 15:25:56,490 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 15:25:56,829 >> tokenizer config file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 15:25:56,832 >> Special tokens file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-21 15:25:57,487 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2818.284, 'train_samples_per_second': 0.958, 'train_steps_per_second': 0.019, 'train_loss': 1.2931842274136014, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [46:58<00:00, 52.19s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-21 15:25:57,491 >> Saving model checkpoint to /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 15:25:57,905 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 15:25:57,906 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 15:25:58,234 >> tokenizer config file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 15:25:58,238 >> Special tokens file saved in /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 31015165GF\n",
            "  train_loss               =     1.2932\n",
            "  train_runtime            = 0:46:58.28\n",
            "  train_samples_per_second =      0.958\n",
            "  train_steps_per_second   =      0.019\n",
            "Figure saved at: /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8/training_eval_loss.png\n",
            "07/21/2024 15:25:58 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-21 15:25:58,583 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 15:25:58,583 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 15:25:58,583 >>   Batch size = 2\n",
            "100% 50/50 [00:24<00:00,  2.01it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =     0.8418\n",
            "  eval_runtime            = 0:00:25.38\n",
            "  eval_samples_per_second =      3.939\n",
            "  eval_steps_per_second   =       1.97\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:449] 2024-07-21 15:26:23,985 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asPBYvc5mTDh",
        "collapsed": true,
        "outputId": "3d035f04-568e-44b7-efa9-db1e79740a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 11:28:24.013076: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 11:28:24.013122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 11:28:24.014475: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 11:28:24.021548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 11:28:25.164828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 11:28:31 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:28:31,719 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:28:31,719 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:28:31,719 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:28:31,719 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:28:31,719 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 11:28:31,781 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 11:28:31 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 11:28:31 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 11:28:31 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:28:32,756 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:28:33,226 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 11:28:33,227 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 11:28:33 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 11:28:33,487 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 11:28:33,487 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:28:33,489 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.42s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 11:28:36,395 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 11:28:36,395 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 11:28:36,632 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:28:36,633 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 11:28:36 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 11:28:41 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 11:28:41 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8\n",
            "07/23/2024 11:28:41 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-23 11:28:41,953 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 11:28:41,953 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 11:28:41,953 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-23 11:28:42,146 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [24:59<00:00, 33.98s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.632 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [25:00<00:00, 30.02s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    15.8447\n",
            "  predict_rouge-1            =    23.9213\n",
            "  predict_rouge-2            =      8.649\n",
            "  predict_rouge-l            =    17.4515\n",
            "  predict_runtime            = 0:25:33.12\n",
            "  predict_samples_per_second =      0.065\n",
            "  predict_steps_per_second   =      0.033\n",
            "07/23/2024 11:54:15 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank16_nf8_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank16_nf8_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68dJkfQGmTFq",
        "collapsed": true,
        "outputId": "a4afee8c-2b45-4656-aa16-0a7c10446972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 11:54:20.025060: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 11:54:20.025105: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 11:54:20.026449: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 11:54:20.033873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 11:54:21.161608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 11:54:27 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:54:27,718 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:54:27,718 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:54:27,718 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:54:27,718 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 11:54:27,718 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 11:54:27,788 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 11:54:27 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 11:54:27 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 11:54:27 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:54:28,721 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 11:54:29,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 11:54:29,167 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 11:54:29 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 11:54:29,421 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 11:54:29,422 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:54:29,423 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.40s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 11:54:32,284 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 11:54:32,284 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 11:54:32,519 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 11:54:32,520 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 11:54:32 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 11:54:33 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 11:54:33 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8\n",
            "07/23/2024 11:54:33 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-23 11:54:33,375 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 11:54:33,375 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 11:54:33,375 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-23 11:54:33,571 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [10:36<00:00, 12.80s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.652 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [10:36<00:00, 12.74s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    56.7044\n",
            "  predict_rouge-1            =    62.1731\n",
            "  predict_rouge-2            =    52.0343\n",
            "  predict_rouge-l            =    61.2065\n",
            "  predict_runtime            = 0:10:43.87\n",
            "  predict_samples_per_second =      0.155\n",
            "  predict_steps_per_second   =      0.078\n",
            "07/23/2024 12:05:17 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank16_nf8_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank16_nf8_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/QLoRA_Rank/train_Rank16_NF8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-_rOQXfjKOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "418526e8-8303-4a11-bca9-65d6b9cfd5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 12:05:22.080716: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 12:05:22.080756: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 12:05:22.082053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 12:05:22.089225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 12:05:23.267342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 12:05:29 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:05:30,345 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:05:30,345 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:05:30,345 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:05:30,345 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:05:30,345 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 12:05:30,409 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 12:05:30 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 12:05:30 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 12:05:30 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 12:05:31,367 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 12:05:31,819 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 12:05:31,820 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 12:05:31 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 12:05:32,072 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 12:05:32,072 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 12:05:32,073 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.42s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 12:05:34,966 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 12:05:34,966 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 12:05:35,198 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 12:05:35,198 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 12:05:35 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 12:05:42 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/23/2024 12:05:42 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/phi3/qlora_lr/train_lr_5e-05\n",
            "07/23/2024 12:05:42 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-23 12:05:42,142 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 12:05:42,142 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 12:05:42,142 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-23 12:05:42,317 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [16:48<00:00, 12.10s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.666 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [16:49<00:00, 20.18s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    15.9584\n",
            "  predict_rouge-1            =    22.8949\n",
            "  predict_rouge-2            =      8.128\n",
            "  predict_rouge-l            =     16.784\n",
            "  predict_runtime            = 0:17:06.67\n",
            "  predict_samples_per_second =      0.097\n",
            "  predict_steps_per_second   =      0.049\n",
            "07/23/2024 12:22:48 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank8_nf8_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank8_nf8_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/phi3/qlora_lr/train_lr_5e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5hCgo459EUe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f04677e-c1be-458a-ffdb-f9eb542dd1cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-23 12:22:53.665119: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-23 12:22:53.665181: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-23 12:22:53.666561: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-23 12:22:53.673679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-23 12:22:54.801103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/23/2024 12:23:01 - WARNING - llamafactory.hparams.parser - Evaluating model in 4/8-bit mode may cause lower scores.\n",
            "07/23/2024 12:23:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:23:01,438 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:23:01,438 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:23:01,438 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:23:01,438 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-23 12:23:01,438 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-23 12:23:01,502 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/23/2024 12:23:01 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/23/2024 12:23:01 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/23/2024 12:23:01 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 12:23:02,395 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-23 12:23:02,785 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-23 12:23:02,786 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/23/2024 12:23:02 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "07/23/2024 12:23:02 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-23 12:23:03,013 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-23 12:23:03,014 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 12:23:03,015 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.56s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-23 12:23:08,406 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-23 12:23:08,406 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-23 12:23:08,623 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-23 12:23:08,624 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/23/2024 12:23:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/23/2024 12:23:09 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/phi3/qlora_lr/train_lr_5e-05\n",
            "07/23/2024 12:23:09 - INFO - llamafactory.model.loader - all params: 3,833,662,464\n",
            "[INFO|trainer.py:3788] 2024-07-23 12:23:09,109 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-23 12:23:09,109 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-23 12:23:09,109 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-23 12:23:09,147 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "100% 50/50 [02:53<00:00,  2.93s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.656 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [02:53<00:00,  3.48s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    60.6309\n",
            "  predict_rouge-1            =    65.4702\n",
            "  predict_rouge-2            =    54.5481\n",
            "  predict_rouge-l            =    65.2979\n",
            "  predict_runtime            = 0:02:58.81\n",
            "  predict_samples_per_second =      0.559\n",
            "  predict_steps_per_second   =       0.28\n",
            "07/23/2024 12:26:07 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank8_nf8_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/QLoRA_Rank/eval_qLoRA_Rank8_nf8_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/phi3/qlora_lr/train_lr_5e-05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C0vnAxZXj5Cc",
        "outputId": "3d6ea890-6130-4bbd-cade-ea21c7e8d105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 09:59:39.417483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 09:59:39.469267: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 09:59:39.469313: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 09:59:39.470872: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 09:59:39.478657: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 09:59:40.660134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 09:59:46 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 09:59:46 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 09:59:46,358 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 09:59:46,358 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 09:59:46,359 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 09:59:46,359 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 09:59:46,359 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 09:59:46,425 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 09:59:46 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 09:59:46 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 09:59:46 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/14/2024 09:59:46 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 09:59:46,938 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 09:59:47,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 09:59:47,058 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 09:59:47 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 09:59:47,143 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 09:59:47,144 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 09:59:47,145 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.46s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 09:59:52,367 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 09:59:52,367 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 09:59:52,412 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 09:59:52,412 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_up_proj,o_proj,down_proj,qkv_proj\n",
            "07/14/2024 09:59:52 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 09:59:52,763 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 09:59:53,806 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 09:59:53,806 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 09:59:53,806 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 09:59:53,806 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 09:59:53,806 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 09:59:53,806 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 09:59:53,806 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 09:59:53,810 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.7334, 'grad_norm': 0.7777190208435059, 'learning_rate': 9.789947561577445e-06, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.732, 'grad_norm': 0.706644594669342, 'learning_rate': 9.177439057064684e-06, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:40<07:21, 10.02s/it][INFO|trainer.py:3719] 2024-07-14 10:01:34,628 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:01:34,628 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:01:34,628 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.72it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.74it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.56it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.25it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.08it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.93it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.67it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.55it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.57it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.40it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.57it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.69it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.71it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.73it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.59it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.63it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.68it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  4.68it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.70it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:05,  4.71it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.61it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.65it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.68it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.65it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.70it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.71it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.48it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:03,  4.52it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.47it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.48it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.49it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.50it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.57it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.54it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.59it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.59it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.62it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.64it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.52it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.53it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.40it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.41it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3316409587860107, 'eval_accuracy': 0.6996459260704475, 'eval_runtime': 10.9312, 'eval_samples_per_second': 9.148, 'eval_steps_per_second': 4.574, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:51<07:21, 10.02s/it]\n",
            "100% 50/50 [00:10<00:00,  4.52it/s]\u001b[A\n",
            "{'loss': 1.661, 'grad_norm': 0.8469609618186951, 'learning_rate': 8.213938048432697e-06, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.5332, 'grad_norm': 0.7105050683021545, 'learning_rate': 6.980398830195785e-06, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:29<05:38,  9.96s/it][INFO|trainer.py:3719] 2024-07-14 10:03:23,076 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:03:23,076 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:03:23,076 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.79it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.75it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.58it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.26it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.11it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.95it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.69it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:08,  4.56it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.60it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.61it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.65it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.62it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.66it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.66it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.66it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.68it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.52it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.60it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.62it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:05,  4.62it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.53it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.58it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.57it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.64it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.64it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.41it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:04,  4.12it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.16it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.23it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.29it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.34it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.38it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.46it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.48it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.45it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.31it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2228868007659912, 'eval_accuracy': 0.7098803207680973, 'eval_runtime': 11.1071, 'eval_samples_per_second': 9.003, 'eval_steps_per_second': 4.502, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:40<05:38,  9.96s/it]\n",
            "100% 50/50 [00:10<00:00,  4.39it/s]\u001b[A\n",
            "{'loss': 1.5932, 'grad_norm': 0.6805017590522766, 'learning_rate': 5.5804645706261515e-06, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.5714, 'grad_norm': 0.6287615895271301, 'learning_rate': 4.131759111665349e-06, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:18<03:58,  9.94s/it][INFO|trainer.py:3719] 2024-07-14 10:05:11,883 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:05:11,883 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:05:11,883 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.65it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.74it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.48it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.13it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.00it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.85it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.60it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.50it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.54it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.51it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.58it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.56it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.66it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.53it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.57it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.63it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.66it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.68it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.70it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.68it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.67it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.42it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.53it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.48it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.36it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1582833528518677, 'eval_accuracy': 0.7253271314432637, 'eval_runtime': 11.0461, 'eval_samples_per_second': 9.053, 'eval_steps_per_second': 4.526, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:29<03:58,  9.94s/it]\n",
            "100% 50/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            "{'loss': 1.5562, 'grad_norm': 0.7159940600395203, 'learning_rate': 2.7560040989976894e-06, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.5158, 'grad_norm': 0.619800865650177, 'learning_rate': 1.5687918106563326e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:08<02:19,  9.95s/it][INFO|trainer.py:3719] 2024-07-14 10:07:02,179 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:07:02,179 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:07:02,179 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.50it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.68it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.46it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.15it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.99it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.85it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.59it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.49it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.47it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.53it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.61it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.66it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.53it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.57it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.60it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.62it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.62it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.50it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.60it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.58it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.65it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.65it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.41it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.45it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.43it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.48it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.50it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.48it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.44it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.29it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.30it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.131240725517273, 'eval_accuracy': 0.7485599635567636, 'eval_runtime': 11.1163, 'eval_samples_per_second': 8.996, 'eval_steps_per_second': 4.498, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:19<02:19,  9.95s/it]\n",
            "100% 50/50 [00:10<00:00,  4.41it/s]\u001b[A\n",
            "{'loss': 1.4902, 'grad_norm': 0.7432420253753662, 'learning_rate': 6.698729810778065e-07, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.5028, 'grad_norm': 0.7545288801193237, 'learning_rate': 1.3477564710088097e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [08:57<00:39,  9.77s/it][INFO|trainer.py:3719] 2024-07-14 10:08:51,311 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:08:51,312 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:08:51,312 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.44it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.60it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.45it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.12it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.97it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.83it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.61it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.48it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.51it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.45it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.54it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.61it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.60it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.47it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.59it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.61it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.60it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.57it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.63it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.39it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.48it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.51it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.49it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.50it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.39it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.42it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1267600059509277, 'eval_accuracy': 0.7459232667956581, 'eval_runtime': 11.1005, 'eval_samples_per_second': 9.009, 'eval_steps_per_second': 4.504, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [09:08<00:39,  9.77s/it]\n",
            "100% 50/50 [00:10<00:00,  4.52it/s]\u001b[A\n",
            "100% 54/54 [09:48<00:00, 11.05s/it][INFO|trainer.py:2329] 2024-07-14 10:09:42,125 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 588.3154, 'train_samples_per_second': 4.589, 'train_steps_per_second': 0.092, 'train_loss': 1.5830894222965948, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [09:48<00:00, 10.89s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 10:09:42,130 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:09:42,352 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:09:42,353 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 10:09:42,548 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 10:09:42,551 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 30911033GF\n",
            "  train_loss               =     1.5831\n",
            "  train_runtime            = 0:09:48.31\n",
            "  train_samples_per_second =      4.589\n",
            "  train_steps_per_second   =      0.092\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 10:09:43,039 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:09:43,039 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:09:43,039 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7483\n",
            "  eval_loss               =     1.1254\n",
            "  eval_runtime            = 0:00:11.09\n",
            "  eval_samples_per_second =       9.01\n",
            "  eval_steps_per_second   =      4.505\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:450] 2024-07-14 10:09:54,151 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7483322151779749}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 1e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_lr/train_lr_1e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0VytthiGj5EV",
        "outputId": "7dd17108-9c20-4db3-9ac3-7e41fac70cad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 10:09:58.847445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 10:09:58.897304: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 10:09:58.897365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 10:09:58.898786: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 10:09:58.906091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 10:10:00.091636: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 10:10:05 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 10:10:05 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:10:05,725 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:10:05,725 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:10:05,725 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:10:05,725 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:10:05,725 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 10:10:05,793 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 10:10:05 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 10:10:05 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 10:10:05 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/14/2024 10:10:05 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:10:06,349 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:10:06,487 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:10:06,488 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 10:10:06 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 10:10:06,591 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 10:10:06,591 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:10:06,592 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.29s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 10:10:11,462 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 10:10:11,462 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 10:10:11,532 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:10:11,533 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,gate_up_proj,o_proj,qkv_proj\n",
            "07/14/2024 10:10:11 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 10:10:11,878 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 10:10:12,923 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 10:10:12,923 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 10:10:12,923 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 10:10:12,923 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 10:10:12,923 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 10:10:12,923 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 10:10:12,924 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 10:10:12,927 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.4203, 'grad_norm': 0.2868548035621643, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.2342, 'grad_norm': 0.19251039624214172, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:40<07:20, 10.02s/it][INFO|trainer.py:3719] 2024-07-14 10:11:53,713 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:11:53,713 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:11:53,713 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.64it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.73it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.55it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.25it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.08it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.89it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.64it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.51it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.56it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.40it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.56it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.67it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.71it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.72it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.58it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.63it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.67it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:05,  4.70it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.69it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:05,  4.68it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.68it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.66it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.70it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.70it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.44it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:03,  4.49it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.45it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.45it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.47it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.55it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.55it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.58it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.61it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.55it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.57it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8196539878845215, 'eval_accuracy': 0.8267431827267981, 'eval_runtime': 10.9528, 'eval_samples_per_second': 9.13, 'eval_steps_per_second': 4.565, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:51<07:20, 10.02s/it]\n",
            "100% 50/50 [00:10<00:00,  4.53it/s]\u001b[A\n",
            "{'loss': 1.1716, 'grad_norm': 0.2080451101064682, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.0832, 'grad_norm': 0.19235891103744507, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:29<05:38,  9.96s/it][INFO|trainer.py:3719] 2024-07-14 10:13:42,108 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:13:42,108 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:13:42,108 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.88it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.76it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.61it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.31it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.09it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.93it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.67it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.53it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.55it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:09,  4.22it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.30it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.33it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.41it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.46it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.58it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.49it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.62it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.62it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.64it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.65it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.42it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.43it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.54it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.58it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.60it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.64it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.64it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.52it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.53it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.40it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.36it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8138673901557922, 'eval_accuracy': 0.830355658689075, 'eval_runtime': 11.0854, 'eval_samples_per_second': 9.021, 'eval_steps_per_second': 4.51, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:40<05:38,  9.96s/it]\n",
            "100% 50/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            "{'loss': 1.1289, 'grad_norm': 0.1806526631116867, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.1379, 'grad_norm': 0.17448976635932922, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:18<03:58,  9.95s/it][INFO|trainer.py:3719] 2024-07-14 10:15:31,012 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:15:31,012 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:15:31,012 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.66it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.62it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.50it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.19it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.04it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.89it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.63it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.49it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.52it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.59it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.53it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.57it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.60it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.61it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.63it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.47it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.49it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.58it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.60it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.63it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.62it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.64it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.67it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.68it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.42it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.44it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.48it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.54it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.50it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.53it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.39it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.42it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.816508412361145, 'eval_accuracy': 0.8304308923767083, 'eval_runtime': 11.054, 'eval_samples_per_second': 9.046, 'eval_steps_per_second': 4.523, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:29<03:58,  9.95s/it]\n",
            "100% 50/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            "{'loss': 1.123, 'grad_norm': 0.21165731549263, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.0722, 'grad_norm': 0.20177988708019257, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:08<02:19,  9.93s/it][INFO|trainer.py:3719] 2024-07-14 10:17:21,234 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:17:21,234 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:17:21,234 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.42it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.58it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.48it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.19it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.01it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.84it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.58it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.46it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.42it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.51it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.51it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.47it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.45it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.51it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:07,  4.42it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.58it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.60it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.62it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.65it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.70it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.70it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.44it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.49it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.52it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.50it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.50it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.40it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8086001873016357, 'eval_accuracy': 0.8351373487725049, 'eval_runtime': 11.0983, 'eval_samples_per_second': 9.01, 'eval_steps_per_second': 4.505, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:19<02:19,  9.93s/it]\n",
            "100% 50/50 [00:10<00:00,  4.49it/s]\u001b[A\n",
            "{'loss': 1.0252, 'grad_norm': 0.22585150599479675, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.0377, 'grad_norm': 0.25535154342651367, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [08:57<00:39,  9.76s/it][INFO|trainer.py:3719] 2024-07-14 10:19:10,403 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:19:10,403 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:19:10,403 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.55it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.60it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.47it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.18it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.00it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.83it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.60it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.49it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.47it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.54it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.57it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.67it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.53it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.52it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.57it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.60it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.64it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.63it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.64it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.63it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.67it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.69it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.45it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.48it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.43it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.51it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.53it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.60it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.50it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.53it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.41it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8134818077087402, 'eval_accuracy': 0.8358303318774015, 'eval_runtime': 11.0497, 'eval_samples_per_second': 9.05, 'eval_steps_per_second': 4.525, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [09:08<00:39,  9.76s/it]\n",
            "100% 50/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            "100% 54/54 [09:48<00:00, 11.06s/it][INFO|trainer.py:2329] 2024-07-14 10:20:01,212 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 588.2854, 'train_samples_per_second': 4.59, 'train_steps_per_second': 0.092, 'train_loss': 1.1380351031268086, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [09:48<00:00, 10.89s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 10:20:01,217 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:20:01,446 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:20:01,447 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 10:20:01,648 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 10:20:01,652 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 30911033GF\n",
            "  train_loss               =      1.138\n",
            "  train_runtime            = 0:09:48.28\n",
            "  train_samples_per_second =       4.59\n",
            "  train_steps_per_second   =      0.092\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 10:20:02,170 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:20:02,170 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:20:02,170 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.59it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.8309\n",
            "  eval_loss               =      0.814\n",
            "  eval_runtime            = 0:00:11.11\n",
            "  eval_samples_per_second =      8.997\n",
            "  eval_steps_per_second   =      4.498\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:450] 2024-07-14 10:20:13,299 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8309096694268931}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M2e-LI0cj5GL",
        "outputId": "b2276740-349e-4060-f63d-06648ead90cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 10:20:18.069860: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 10:20:18.120742: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 10:20:18.120786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 10:20:18.122245: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 10:20:18.129687: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 10:20:19.317508: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 10:20:24 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 10:20:24 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:20:24,936 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:20:24,936 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:20:24,936 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:20:24,936 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:20:24,936 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 10:20:25,005 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 10:20:25 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 10:20:25 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 10:20:25 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/14/2024 10:20:25 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:20:25,524 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:20:25,667 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:20:25,668 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 10:20:25 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 10:20:25,771 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 10:20:25,772 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:20:25,773 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.43s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 10:20:30,929 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 10:20:30,929 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 10:20:31,000 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:20:31,000 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,qkv_proj,gate_up_proj,o_proj\n",
            "07/14/2024 10:20:31 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 10:20:31,347 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 10:20:32,401 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 10:20:32,401 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 10:20:32,401 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 10:20:32,401 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 10:20:32,401 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 10:20:32,401 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 10:20:32,402 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 10:20:32,405 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6781, 'grad_norm': 0.7267939448356628, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.526, 'grad_norm': 0.6045851111412048, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:40<07:20, 10.01s/it][INFO|trainer.py:3719] 2024-07-14 10:22:13,141 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:22:13,141 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:22:13,141 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.65it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.72it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.25it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.09it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.92it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.68it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.55it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.58it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.37it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.51it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.63it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.64it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.66it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.69it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.68it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.66it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.73it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.73it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.48it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:03,  4.54it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.47it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.48it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.51it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.58it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.51it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.58it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.60it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.62it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.65it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.55it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.56it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.42it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.002673625946045, 'eval_accuracy': 0.7949593085729691, 'eval_runtime': 10.9692, 'eval_samples_per_second': 9.116, 'eval_steps_per_second': 4.558, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:51<07:20, 10.01s/it]\n",
            "100% 50/50 [00:10<00:00,  4.51it/s]\u001b[A\n",
            "{'loss': 1.3339, 'grad_norm': 0.40261128544807434, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.1732, 'grad_norm': 0.32505568861961365, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:29<05:39,  9.99s/it][INFO|trainer.py:3719] 2024-07-14 10:24:01,643 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:24:01,643 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:24:01,643 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.82it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.85it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.65it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.36it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.20it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  5.03it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.77it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:08,  4.61it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.63it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.58it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.64it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.68it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.64it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.65it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.67it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.65it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.65it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.51it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.53it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.57it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.60it/s]\u001b[A\n",
            " 48% 24/50 [00:04<00:05,  4.57it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.52it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:03,  4.44it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.40it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.35it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8855045437812805, 'eval_accuracy': 0.8162709958368736, 'eval_runtime': 11.037, 'eval_samples_per_second': 9.06, 'eval_steps_per_second': 4.53, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:40<05:39,  9.99s/it]\n",
            "100% 50/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            "{'loss': 1.2438, 'grad_norm': 0.23605689406394958, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.2461, 'grad_norm': 0.2343757450580597, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:18<03:59,  9.97s/it][INFO|trainer.py:3719] 2024-07-14 10:25:50,626 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:25:50,626 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:25:50,626 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.74it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.75it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.23it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.06it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.91it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.63it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.50it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.53it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.60it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.56it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.64it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.58it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.60it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.52it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.51it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.37it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.40it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.61it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.49it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.51it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8559126257896423, 'eval_accuracy': 0.8218364906014008, 'eval_runtime': 11.0933, 'eval_samples_per_second': 9.014, 'eval_steps_per_second': 4.507, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:29<03:59,  9.97s/it]\n",
            "100% 50/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            "{'loss': 1.2306, 'grad_norm': 0.2232147753238678, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.1996, 'grad_norm': 0.23049794137477875, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:08<02:19,  9.95s/it][INFO|trainer.py:3719] 2024-07-14 10:27:41,043 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:27:41,043 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:27:41,043 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.68it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.61it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.44it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.16it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.01it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.86it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.60it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.48it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.57it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.54it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.61it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.61it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.47it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.50it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.53it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.56it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.53it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.54it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.41it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.43it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.51it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.50it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.49it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.35it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.34it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8440226912498474, 'eval_accuracy': 0.8266674197812819, 'eval_runtime': 11.1258, 'eval_samples_per_second': 8.988, 'eval_steps_per_second': 4.494, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:19<02:19,  9.95s/it]\n",
            "100% 50/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            "{'loss': 1.1619, 'grad_norm': 0.3045008182525635, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.1807, 'grad_norm': 0.2726382315158844, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [08:57<00:39,  9.77s/it][INFO|trainer.py:3719] 2024-07-14 10:29:30,283 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:29:30,283 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:29:30,283 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.48it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.63it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.47it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.18it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.02it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.86it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.60it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.49it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.54it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.60it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.58it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.65it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.66it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.66it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.52it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.57it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.46it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.55it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.52it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.53it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.60it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.50it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.50it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.841320812702179, 'eval_accuracy': 0.8276372819487409, 'eval_runtime': 11.0915, 'eval_samples_per_second': 9.016, 'eval_steps_per_second': 4.508, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [09:08<00:39,  9.77s/it]\n",
            "100% 50/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            "100% 54/54 [09:48<00:00, 11.05s/it][INFO|trainer.py:2329] 2024-07-14 10:30:21,121 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 588.7164, 'train_samples_per_second': 4.586, 'train_steps_per_second': 0.092, 'train_loss': 1.2913152906629775, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [09:48<00:00, 10.90s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 10:30:21,127 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:30:21,290 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:30:21,291 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 10:30:21,486 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 10:30:21,490 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 30911033GF\n",
            "  train_loss               =     1.2913\n",
            "  train_runtime            = 0:09:48.71\n",
            "  train_samples_per_second =      4.586\n",
            "  train_steps_per_second   =      0.092\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 10:30:22,005 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:30:22,005 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:30:22,005 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.58it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.8237\n",
            "  eval_loss               =     0.8405\n",
            "  eval_runtime            = 0:00:11.14\n",
            "  eval_samples_per_second =      8.969\n",
            "  eval_steps_per_second   =      4.484\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:450] 2024-07-14 10:30:33,168 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8236597548501753}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_lr/train_lr_5e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8zX2wpHUj5IN",
        "outputId": "a2f9846b-af7c-4e54-df8f-6fe879168c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 10:30:39.233587: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 10:30:39.284174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 10:30:39.284221: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 10:30:39.285669: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 10:30:39.293248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 10:30:40.492046: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 10:30:45 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 10:30:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:30:46,076 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:30:46,077 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:30:46,077 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:30:46,077 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:30:46,077 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 10:30:46,144 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 10:30:46 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 10:30:46 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 10:30:46 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/14/2024 10:30:46 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:30:46,694 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:30:46,791 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:30:46,791 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 10:30:46 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 10:30:46,937 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 10:30:46,938 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:30:46,939 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.47s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 10:30:52,172 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 10:30:52,172 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 10:30:52,227 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:30:52,228 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,down_proj,gate_up_proj,o_proj\n",
            "07/14/2024 10:30:52 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 10:30:52,578 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 10:30:53,633 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 10:30:53,633 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 10:30:53,633 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:2081] 2024-07-14 10:30:53,633 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 10:30:53,633 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 10:30:53,633 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 10:30:53,633 >>   Total optimization steps = 90\n",
            "[INFO|trainer.py:2087] 2024-07-14 10:30:53,636 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/90 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6799, 'grad_norm': 0.7708557844161987, 'learning_rate': 4.962019382530521e-05, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.5239, 'grad_norm': 0.5929334163665771, 'learning_rate': 4.849231551964771e-05, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 11% 10/90 [01:40<13:22, 10.03s/it][INFO|trainer.py:3719] 2024-07-14 10:32:34,629 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:32:34,629 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:32:34,629 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.62it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.62it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.46it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.15it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.98it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.84it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.58it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.47it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.35it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.50it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.55it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.44it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.49it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.52it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.60it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.64it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.61it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.63it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.39it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.52it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.46it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.52it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.54it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.49it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.52it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.9959011077880859, 'eval_accuracy': 0.7925130695886843, 'eval_runtime': 11.1154, 'eval_samples_per_second': 8.997, 'eval_steps_per_second': 4.498, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 11% 10/90 [01:52<13:22, 10.03s/it]\n",
            "100% 50/50 [00:10<00:00,  4.49it/s]\u001b[A\n",
            "{'loss': 1.328, 'grad_norm': 0.38992902636528015, 'learning_rate': 4.665063509461097e-05, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.1684, 'grad_norm': 0.30271685123443604, 'learning_rate': 4.415111107797445e-05, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 22% 20/90 [03:29<11:41, 10.02s/it][INFO|trainer.py:3719] 2024-07-14 10:34:23,588 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:34:23,588 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:34:23,588 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.64it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.76it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.57it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.21it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.02it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.88it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.61it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.51it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.54it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.56it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.60it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.55it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.54it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:07,  4.40it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.41it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.43it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.46it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.46it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.37it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.35it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.39it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:04,  4.46it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.46it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.55it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.33it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.31it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.33it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.33it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.35it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.37it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.46it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.50it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.44it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.41it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8795179128646851, 'eval_accuracy': 0.8154445030251939, 'eval_runtime': 11.2331, 'eval_samples_per_second': 8.902, 'eval_steps_per_second': 4.451, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 22% 20/90 [03:41<11:41, 10.02s/it]\n",
            "100% 50/50 [00:11<00:00,  4.42it/s]\u001b[A\n",
            "{'loss': 1.2371, 'grad_norm': 0.22224871814250946, 'learning_rate': 4.1069690242163484e-05, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.235, 'grad_norm': 0.22385573387145996, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 33% 30/90 [05:19<09:58,  9.98s/it][INFO|trainer.py:3719] 2024-07-14 10:36:12,880 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:36:12,880 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:36:12,880 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.62it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.71it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.50it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.17it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.01it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.84it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.57it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.45it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.54it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.52it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.55it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:08,  4.25it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.33it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.40it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:07,  4.33it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.39it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.45it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.57it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.61it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:04,  4.55it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.54it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.46it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.33it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8427798748016357, 'eval_accuracy': 0.8240981756844683, 'eval_runtime': 11.2169, 'eval_samples_per_second': 8.915, 'eval_steps_per_second': 4.458, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 33% 30/90 [05:30<09:58,  9.98s/it]\n",
            "100% 50/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            "{'loss': 1.2174, 'grad_norm': 0.21523530781269073, 'learning_rate': 3.355050358314172e-05, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.1833, 'grad_norm': 0.1986813098192215, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 44% 40/90 [07:09<08:18,  9.96s/it][INFO|trainer.py:3719] 2024-07-14 10:38:03,514 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:38:03,514 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:38:03,514 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.33it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.54it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.43it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.11it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.91it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:09,  4.78it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.55it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.45it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.54it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.48it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.52it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.44it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.52it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.58it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.59it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.56it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.55it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.64it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.63it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.35it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.37it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.41it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.50it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.43it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.42it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.35it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8261522650718689, 'eval_accuracy': 0.8289426065485002, 'eval_runtime': 11.1794, 'eval_samples_per_second': 8.945, 'eval_steps_per_second': 4.473, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 44% 40/90 [07:21<08:18,  9.96s/it]\n",
            "100% 50/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            "{'loss': 1.1427, 'grad_norm': 0.243098184466362, 'learning_rate': 2.5e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.1583, 'grad_norm': 0.23284302651882172, 'learning_rate': 2.0658795558326743e-05, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 56% 50/90 [08:59<06:31,  9.79s/it][INFO|trainer.py:3719] 2024-07-14 10:39:52,997 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:39:52,997 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:39:52,997 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.44it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.55it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.42it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.12it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.96it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.80it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.54it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.43it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.51it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.52it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.54it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.43it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.49it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.54it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.53it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.44it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.46it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.50it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:04,  4.56it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.42it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.35it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.42it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.47it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.34it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.35it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8225871324539185, 'eval_accuracy': 0.8271998538740664, 'eval_runtime': 11.1916, 'eval_samples_per_second': 8.935, 'eval_steps_per_second': 4.468, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 56% 50/90 [09:10<06:31,  9.79s/it]\n",
            "100% 50/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            "{'loss': 1.1949, 'grad_norm': 0.23686590790748596, 'learning_rate': 1.6449496416858284e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1510704}\n",
            "{'loss': 1.1891, 'grad_norm': 0.2237619012594223, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1649808}\n",
            " 67% 60/90 [10:50<05:05, 10.17s/it][INFO|trainer.py:3719] 2024-07-14 10:41:44,542 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:41:44,543 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:41:44,543 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.44it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.59it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.45it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.12it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.97it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.82it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.57it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.43it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.41it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.51it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.50it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.57it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.43it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.48it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.53it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.57it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.55it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.57it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.56it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.35it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.33it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.35it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.38it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.39it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.46it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.48it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.46it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.44it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.34it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.34it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8200973272323608, 'eval_accuracy': 0.8251144466794257, 'eval_runtime': 11.1916, 'eval_samples_per_second': 8.935, 'eval_steps_per_second': 4.468, 'epoch': 3.2, 'num_input_tokens_seen': 1649808}\n",
            " 67% 60/90 [11:02<05:05, 10.17s/it]\n",
            "100% 50/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            "{'loss': 1.1806, 'grad_norm': 0.23800310492515564, 'learning_rate': 8.930309757836517e-06, 'epoch': 3.47, 'num_input_tokens_seen': 1783392}\n",
            "{'loss': 1.1458, 'grad_norm': 0.20944316685199738, 'learning_rate': 5.848888922025553e-06, 'epoch': 3.73, 'num_input_tokens_seen': 1921392}\n",
            " 78% 70/90 [12:40<03:24, 10.24s/it][INFO|trainer.py:3719] 2024-07-14 10:43:34,131 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:43:34,131 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:43:34,131 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.47it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.56it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.45it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.15it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.99it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.83it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.55it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.44it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.45it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.41it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.43it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.46it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.47it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.47it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.50it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:07,  4.37it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.41it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.55it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.50it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.58it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.61it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.35it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.33it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.35it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.46it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.40it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.46it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.55it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.46it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.33it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8163896203041077, 'eval_accuracy': 0.8327853887849352, 'eval_runtime': 11.2105, 'eval_samples_per_second': 8.92, 'eval_steps_per_second': 4.46, 'epoch': 3.73, 'num_input_tokens_seen': 1921392}\n",
            " 78% 70/90 [12:51<03:24, 10.24s/it]\n",
            "100% 50/50 [00:10<00:00,  4.42it/s]\u001b[A\n",
            "{'loss': 1.1395, 'grad_norm': 0.20191746950149536, 'learning_rate': 3.3493649053890326e-06, 'epoch': 4.0, 'num_input_tokens_seen': 2058240}\n",
            "{'loss': 1.1288, 'grad_norm': 0.20901857316493988, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.27, 'num_input_tokens_seen': 2192832}\n",
            " 89% 80/90 [14:29<01:38,  9.86s/it][INFO|trainer.py:3719] 2024-07-14 10:45:23,443 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:45:23,443 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:45:23,443 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.56it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.60it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.45it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.14it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.97it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.81it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.58it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.46it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.49it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.45it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.47it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.46it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.51it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.57it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.60it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:06,  4.47it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.59it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.52it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.24it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:05,  4.35it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.39it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.49it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.52it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.31it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.34it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.39it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.48it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.43it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.53it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.60it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.49it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8200915455818176, 'eval_accuracy': 0.8285818450814106, 'eval_runtime': 11.1911, 'eval_samples_per_second': 8.936, 'eval_steps_per_second': 4.468, 'epoch': 4.27, 'num_input_tokens_seen': 2192832}\n",
            " 89% 80/90 [14:40<01:38,  9.86s/it]\n",
            "100% 50/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            "{'loss': 1.2001, 'grad_norm': 0.20926131308078766, 'learning_rate': 3.7980617469479953e-07, 'epoch': 4.53, 'num_input_tokens_seen': 2326656}\n",
            "{'loss': 1.174, 'grad_norm': 0.21981404721736908, 'learning_rate': 0.0, 'epoch': 4.8, 'num_input_tokens_seen': 2469120}\n",
            "100% 90/90 [16:20<00:00, 10.31s/it][INFO|trainer.py:3719] 2024-07-14 10:47:14,353 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:47:14,353 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:47:14,354 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.51it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.61it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.45it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.15it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.96it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:09,  4.78it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.53it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.39it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:09,  4.43it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.39it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.45it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.46it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.49it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.51it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:07,  4.53it/s]\u001b[A\n",
            " 38% 19/50 [00:04<00:07,  4.42it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.46it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.51it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.50it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.50it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.54it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.44it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.46it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.44it/s]\u001b[A\n",
            " 56% 28/50 [00:06<00:04,  4.49it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.50it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.57it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.56it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.33it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.37it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.32it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.33it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 74% 37/50 [00:08<00:02,  4.39it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.39it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:02,  4.45it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.47it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.51it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.45it/s]\u001b[A\n",
            " 92% 46/50 [00:10<00:00,  4.43it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.32it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.33it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8174182176589966, 'eval_accuracy': 0.8308616248785076, 'eval_runtime': 11.2549, 'eval_samples_per_second': 8.885, 'eval_steps_per_second': 4.442, 'epoch': 4.8, 'num_input_tokens_seen': 2469120}\n",
            "100% 90/90 [16:31<00:00, 10.31s/it]\n",
            "100% 50/50 [00:11<00:00,  4.42it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:2329] 2024-07-14 10:47:25,609 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 991.9727, 'train_samples_per_second': 4.536, 'train_steps_per_second': 0.091, 'train_loss': 1.2348182254367404, 'epoch': 4.8, 'num_input_tokens_seen': 2469120}\n",
            "100% 90/90 [16:31<00:00, 11.02s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 10:47:25,615 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_epoch/train_epoch5\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:47:25,806 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:47:25,807 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 10:47:26,002 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_epoch/train_epoch5/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 10:47:26,006 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_epoch/train_epoch5/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        4.8\n",
            "  num_input_tokens_seen    =    2469120\n",
            "  total_flos               = 51535085GF\n",
            "  train_loss               =     1.2348\n",
            "  train_runtime            = 0:16:31.97\n",
            "  train_samples_per_second =      4.536\n",
            "  train_steps_per_second   =      0.091\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_epoch/train_epoch5/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_epoch/train_epoch5/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_epoch/train_epoch5/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 10:47:26,522 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:47:26,522 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:47:26,522 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        4.8\n",
            "  eval_accuracy           =     0.8309\n",
            "  eval_loss               =     0.8174\n",
            "  eval_runtime            = 0:00:11.06\n",
            "  eval_samples_per_second =      9.038\n",
            "  eval_steps_per_second   =      4.519\n",
            "  num_input_tokens_seen   =    2469120\n",
            "[INFO|modelcard.py:450] 2024-07-14 10:47:37,601 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8308616248785076}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_epoch/train_epoch5 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LseeZgR6jOop",
        "outputId": "3b51f49f-b429-4b34-fba4-04bde79099cf",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 10:47:42.393240: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 10:47:42.444272: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 10:47:42.444337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 10:47:42.445800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 10:47:42.453417: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 10:47:43.650356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 10:47:49 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 10:47:49 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:47:49,257 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:47:49,257 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:47:49,257 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:47:49,257 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:47:49,257 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 10:47:49,327 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 10:47:49 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 10:47:49 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 10:47:49 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 250/250 [00:00<00:00, 1082.54 examples/s]\n",
            "07/14/2024 10:47:49 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Converting format of dataset (num_proc=16): 100% 250/250 [00:00<00:00, 1173.76 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 500/500 [00:00<00:00, 610.73 examples/s]\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:47:51,741 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:47:51,841 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:47:51,842 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 10:47:51 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 10:47:51,923 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 10:47:51,924 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:47:51,925 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.47s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 10:47:57,163 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 10:47:57,163 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 10:47:57,211 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:47:57,212 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,o_proj,down_proj,gate_up_proj\n",
            "07/14/2024 10:47:57 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 10:47:57,565 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 10:47:58,622 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 10:47:58,622 >>   Num examples = 450\n",
            "[INFO|trainer.py:2080] 2024-07-14 10:47:58,622 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 10:47:58,622 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 10:47:58,622 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 10:47:58,622 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 10:47:58,622 >>   Total optimization steps = 27\n",
            "[INFO|trainer.py:2087] 2024-07-14 10:47:58,625 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/27 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6309, 'grad_norm': 0.6902056932449341, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 130128}\n",
            "{'loss': 1.4906, 'grad_norm': 0.5703040957450867, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 271536}\n",
            " 37% 10/27 [01:38<02:48,  9.90s/it][INFO|trainer.py:3719] 2024-07-14 10:49:37,441 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:49:37,442 >>   Num examples = 50\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:49:37,442 >>   Batch size = 2\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:02,  8.21it/s]\u001b[A\n",
            " 12% 3/25 [00:00<00:03,  5.97it/s]\u001b[A\n",
            " 16% 4/25 [00:00<00:04,  5.22it/s]\u001b[A\n",
            " 20% 5/25 [00:00<00:03,  5.02it/s]\u001b[A\n",
            " 24% 6/25 [00:01<00:03,  4.92it/s]\u001b[A\n",
            " 28% 7/25 [00:01<00:03,  4.86it/s]\u001b[A\n",
            " 32% 8/25 [00:01<00:03,  4.74it/s]\u001b[A\n",
            " 36% 9/25 [00:01<00:03,  4.74it/s]\u001b[A\n",
            " 40% 10/25 [00:01<00:03,  4.67it/s]\u001b[A\n",
            " 44% 11/25 [00:02<00:03,  4.58it/s]\u001b[A\n",
            " 48% 12/25 [00:02<00:02,  4.65it/s]\u001b[A\n",
            " 52% 13/25 [00:02<00:02,  4.46it/s]\u001b[A\n",
            " 56% 14/25 [00:02<00:02,  4.57it/s]\u001b[A\n",
            " 60% 15/25 [00:03<00:02,  4.60it/s]\u001b[A\n",
            " 64% 16/25 [00:03<00:01,  4.58it/s]\u001b[A\n",
            " 68% 17/25 [00:03<00:01,  4.61it/s]\u001b[A\n",
            " 72% 18/25 [00:03<00:01,  4.67it/s]\u001b[A\n",
            " 76% 19/25 [00:03<00:01,  4.65it/s]\u001b[A\n",
            " 80% 20/25 [00:04<00:01,  4.69it/s]\u001b[A\n",
            " 84% 21/25 [00:04<00:00,  4.74it/s]\u001b[A\n",
            " 88% 22/25 [00:04<00:00,  4.71it/s]\u001b[A\n",
            " 92% 23/25 [00:04<00:00,  4.71it/s]\u001b[A\n",
            " 96% 24/25 [00:05<00:00,  4.74it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2801284790039062, 'eval_accuracy': 0.7623554898205562, 'eval_runtime': 5.4482, 'eval_samples_per_second': 9.177, 'eval_steps_per_second': 4.589, 'epoch': 1.07, 'num_input_tokens_seen': 271536}\n",
            " 37% 10/27 [01:44<02:48,  9.90s/it]\n",
            "100% 25/25 [00:05<00:00,  4.74it/s]\u001b[A\n",
            "{'loss': 1.3592, 'grad_norm': 0.5056689977645874, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 412608}\n",
            "{'loss': 1.3009, 'grad_norm': 0.3940671682357788, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 554112}\n",
            " 74% 20/27 [03:24<01:11, 10.18s/it][INFO|trainer.py:3719] 2024-07-14 10:51:23,535 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:51:23,535 >>   Num examples = 50\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:51:23,535 >>   Batch size = 2\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:02,  9.14it/s]\u001b[A\n",
            " 12% 3/25 [00:00<00:03,  6.40it/s]\u001b[A\n",
            " 16% 4/25 [00:00<00:03,  5.45it/s]\u001b[A\n",
            " 20% 5/25 [00:00<00:03,  5.15it/s]\u001b[A\n",
            " 24% 6/25 [00:01<00:03,  4.98it/s]\u001b[A\n",
            " 28% 7/25 [00:01<00:03,  4.88it/s]\u001b[A\n",
            " 32% 8/25 [00:01<00:03,  4.74it/s]\u001b[A\n",
            " 36% 9/25 [00:01<00:03,  4.73it/s]\u001b[A\n",
            " 40% 10/25 [00:01<00:03,  4.67it/s]\u001b[A\n",
            " 44% 11/25 [00:02<00:03,  4.55it/s]\u001b[A\n",
            " 48% 12/25 [00:02<00:02,  4.61it/s]\u001b[A\n",
            " 52% 13/25 [00:02<00:02,  4.43it/s]\u001b[A\n",
            " 56% 14/25 [00:02<00:02,  4.53it/s]\u001b[A\n",
            " 60% 15/25 [00:03<00:02,  4.56it/s]\u001b[A\n",
            " 64% 16/25 [00:03<00:01,  4.57it/s]\u001b[A\n",
            " 68% 17/25 [00:03<00:01,  4.64it/s]\u001b[A\n",
            " 72% 18/25 [00:03<00:01,  4.65it/s]\u001b[A\n",
            " 76% 19/25 [00:03<00:01,  4.63it/s]\u001b[A\n",
            " 80% 20/25 [00:04<00:01,  4.65it/s]\u001b[A\n",
            " 84% 21/25 [00:04<00:00,  4.67it/s]\u001b[A\n",
            " 88% 22/25 [00:04<00:00,  4.67it/s]\u001b[A\n",
            " 92% 23/25 [00:04<00:00,  4.68it/s]\u001b[A\n",
            " 96% 24/25 [00:04<00:00,  4.73it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.160975456237793, 'eval_accuracy': 0.7848757253251332, 'eval_runtime': 5.4256, 'eval_samples_per_second': 9.216, 'eval_steps_per_second': 4.608, 'epoch': 2.13, 'num_input_tokens_seen': 554112}\n",
            " 74% 20/27 [03:30<01:11, 10.18s/it]\n",
            "100% 25/25 [00:05<00:00,  4.74it/s]\u001b[A\n",
            "{'loss': 1.2511, 'grad_norm': 0.4016122817993164, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 691632}\n",
            "100% 27/27 [04:39<00:00,  9.99s/it][INFO|trainer.py:2329] 2024-07-14 10:52:37,704 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 279.0789, 'train_samples_per_second': 4.837, 'train_steps_per_second': 0.097, 'train_loss': 1.3905307098671242, 'epoch': 2.88, 'num_input_tokens_seen': 745728}\n",
            "100% 27/27 [04:39<00:00, 10.34s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 10:52:37,709 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:52:37,899 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:52:37,900 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 10:52:38,102 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 10:52:38,106 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =     745728\n",
            "  total_flos               = 15564717GF\n",
            "  train_loss               =     1.3905\n",
            "  train_runtime            = 0:04:39.07\n",
            "  train_samples_per_second =      4.837\n",
            "  train_steps_per_second   =      0.097\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 10:52:38,653 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:52:38,653 >>   Num examples = 50\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:52:38,653 >>   Batch size = 2\n",
            "100% 25/25 [00:05<00:00,  4.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7818\n",
            "  eval_loss               =      1.157\n",
            "  eval_runtime            = 0:00:05.66\n",
            "  eval_samples_per_second =      8.832\n",
            "  eval_steps_per_second   =      4.416\n",
            "  num_input_tokens_seen   =     745728\n",
            "[INFO|modelcard.py:450] 2024-07-14 10:52:44,330 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7818263978702963}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 250 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples250 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtE_ZENzjOqe",
        "outputId": "b15ed13f-761d-4230-c0de-342acd9c9e13",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 10:52:49.203784: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 10:52:49.254275: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 10:52:49.254337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 10:52:49.255724: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 10:52:49.263117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 10:52:50.452711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 10:52:55 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 10:52:55 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:52:56,045 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:52:56,045 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:52:56,045 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:52:56,045 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 10:52:56,045 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 10:52:56,112 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 10:52:56 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 10:52:56 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 10:52:56 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "07/14/2024 10:52:56 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:52:56,632 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 10:52:56,722 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 10:52:56,723 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 10:52:56 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 10:52:56,801 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 10:52:56,802 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:52:56,802 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.45s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 10:53:02,001 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 10:53:02,001 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 10:53:02,050 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 10:53:02,050 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,qkv_proj,down_proj,gate_up_proj\n",
            "07/14/2024 10:53:02 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 10:53:02,395 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 10:53:03,455 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 10:53:03,455 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 10:53:03,455 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 10:53:03,455 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 10:53:03,455 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 10:53:03,455 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 10:53:03,455 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 10:53:03,459 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6781, 'grad_norm': 0.7267939448356628, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 138576}\n",
            "{'loss': 1.526, 'grad_norm': 0.6045851111412048, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:40<07:20, 10.01s/it][INFO|trainer.py:3719] 2024-07-14 10:54:44,080 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:54:44,081 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:54:44,081 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.75it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.70it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.55it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.24it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.07it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.91it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.67it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.55it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.59it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.40it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.58it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.55it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.68it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.65it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.67it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.55it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.58it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.64it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.67it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.69it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.58it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.60it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:04,  4.63it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.70it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.66it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.73it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.74it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.49it/s]\u001b[A\n",
            " 66% 33/50 [00:06<00:03,  4.53it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.46it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.47it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.48it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.51it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.59it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.54it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.59it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.61it/s]\u001b[A\n",
            " 84% 42/50 [00:08<00:01,  4.64it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.66it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.67it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.59it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.57it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.58it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.46it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.002673625946045, 'eval_accuracy': 0.7949593085729691, 'eval_runtime': 10.9267, 'eval_samples_per_second': 9.152, 'eval_steps_per_second': 4.576, 'epoch': 0.53, 'num_input_tokens_seen': 279120}\n",
            " 19% 10/54 [01:51<07:20, 10.01s/it]\n",
            "100% 50/50 [00:10<00:00,  4.56it/s]\u001b[A\n",
            "{'loss': 1.3339, 'grad_norm': 0.40261128544807434, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 414480}\n",
            "{'loss': 1.1732, 'grad_norm': 0.32505568861961365, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:28<05:38,  9.96s/it][INFO|trainer.py:3719] 2024-07-14 10:56:32,434 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:56:32,434 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:56:32,434 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.75it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.79it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.59it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.28it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.11it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.96it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.71it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:08,  4.58it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.60it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.61it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:07,  4.65it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.63it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.65it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.66it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.52it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.59it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.46it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.49it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.47it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.56it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.35it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.36it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.48it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.43it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.52it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.52it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.36it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8855045437812805, 'eval_accuracy': 0.8162709958368736, 'eval_runtime': 11.0779, 'eval_samples_per_second': 9.027, 'eval_steps_per_second': 4.513, 'epoch': 1.07, 'num_input_tokens_seen': 550176}\n",
            " 37% 20/54 [03:40<05:38,  9.96s/it]\n",
            "100% 50/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            "{'loss': 1.2438, 'grad_norm': 0.23605689406394958, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 683088}\n",
            "{'loss': 1.2461, 'grad_norm': 0.2343757450580597, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:17<03:59,  9.96s/it][INFO|trainer.py:3719] 2024-07-14 10:58:21,431 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 10:58:21,432 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 10:58:21,432 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:05,  9.45it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:07,  6.63it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.48it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.16it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  4.99it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.85it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.60it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.48it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.50it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.51it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.53it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.60it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.64it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.51it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.59it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.64it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.66it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.59it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.49it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.53it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.55it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.53it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.57it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.32it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.35it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.34it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.45it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.53it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.49it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.57it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.59it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.58it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.50it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.47it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.35it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8559126257896423, 'eval_accuracy': 0.8218364906014008, 'eval_runtime': 11.1006, 'eval_samples_per_second': 9.009, 'eval_steps_per_second': 4.504, 'epoch': 1.6, 'num_input_tokens_seen': 821712}\n",
            " 56% 30/54 [05:29<03:59,  9.96s/it]\n",
            "100% 50/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            "{'loss': 1.2306, 'grad_norm': 0.2232147753238678, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 962880}\n",
            "{'loss': 1.1996, 'grad_norm': 0.23049794137477875, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:08<02:19,  9.94s/it][INFO|trainer.py:3719] 2024-07-14 11:00:11,710 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:00:11,710 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:00:11,710 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.65it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.76it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.55it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.26it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.11it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.96it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:08,  4.69it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:08,  4.57it/s]\u001b[A\n",
            " 20% 10/50 [00:01<00:08,  4.57it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.52it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.55it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.60it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.55it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.58it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.62it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.63it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.64it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.49it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.53it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.54it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.56it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.57it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.56it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.48it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.47it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.54it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.52it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.58it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.59it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.37it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.38it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.42it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.51it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.43it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.52it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.57it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.49it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.48it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.50it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.37it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.38it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.8440226912498474, 'eval_accuracy': 0.8266674197812819, 'eval_runtime': 11.0787, 'eval_samples_per_second': 9.026, 'eval_steps_per_second': 4.513, 'epoch': 2.13, 'num_input_tokens_seen': 1098576}\n",
            " 74% 40/54 [07:19<02:19,  9.94s/it]\n",
            "100% 50/50 [00:10<00:00,  4.47it/s]\u001b[A\n",
            "{'loss': 1.1619, 'grad_norm': 0.3045008182525635, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1239168}\n",
            "{'loss': 1.1807, 'grad_norm': 0.2726382315158844, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [08:57<00:39,  9.76s/it][INFO|trainer.py:3719] 2024-07-14 11:02:00,771 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:02:00,771 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:02:00,771 >>   Batch size = 2\n",
            "\n",
            "  0% 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "  4% 2/50 [00:00<00:04,  9.67it/s]\u001b[A\n",
            "  6% 3/50 [00:00<00:06,  6.75it/s]\u001b[A\n",
            "  8% 4/50 [00:00<00:08,  5.54it/s]\u001b[A\n",
            " 10% 5/50 [00:00<00:08,  5.20it/s]\u001b[A\n",
            " 12% 6/50 [00:01<00:08,  5.03it/s]\u001b[A\n",
            " 14% 7/50 [00:01<00:08,  4.85it/s]\u001b[A\n",
            " 16% 8/50 [00:01<00:09,  4.58it/s]\u001b[A\n",
            " 18% 9/50 [00:01<00:09,  4.47it/s]\u001b[A\n",
            " 20% 10/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 22% 11/50 [00:02<00:08,  4.44it/s]\u001b[A\n",
            " 24% 12/50 [00:02<00:08,  4.48it/s]\u001b[A\n",
            " 26% 13/50 [00:02<00:08,  4.53it/s]\u001b[A\n",
            " 28% 14/50 [00:02<00:07,  4.51it/s]\u001b[A\n",
            " 30% 15/50 [00:03<00:07,  4.56it/s]\u001b[A\n",
            " 32% 16/50 [00:03<00:07,  4.61it/s]\u001b[A\n",
            " 34% 17/50 [00:03<00:07,  4.64it/s]\u001b[A\n",
            " 36% 18/50 [00:03<00:06,  4.65it/s]\u001b[A\n",
            " 38% 19/50 [00:03<00:06,  4.50it/s]\u001b[A\n",
            " 40% 20/50 [00:04<00:06,  4.55it/s]\u001b[A\n",
            " 42% 21/50 [00:04<00:06,  4.60it/s]\u001b[A\n",
            " 44% 22/50 [00:04<00:06,  4.61it/s]\u001b[A\n",
            " 46% 23/50 [00:04<00:05,  4.63it/s]\u001b[A\n",
            " 48% 24/50 [00:05<00:05,  4.62it/s]\u001b[A\n",
            " 50% 25/50 [00:05<00:05,  4.51it/s]\u001b[A\n",
            " 52% 26/50 [00:05<00:05,  4.54it/s]\u001b[A\n",
            " 54% 27/50 [00:05<00:05,  4.57it/s]\u001b[A\n",
            " 56% 28/50 [00:05<00:04,  4.62it/s]\u001b[A\n",
            " 58% 29/50 [00:06<00:04,  4.60it/s]\u001b[A\n",
            " 60% 30/50 [00:06<00:04,  4.67it/s]\u001b[A\n",
            " 62% 31/50 [00:06<00:04,  4.62it/s]\u001b[A\n",
            " 64% 32/50 [00:06<00:04,  4.38it/s]\u001b[A\n",
            " 66% 33/50 [00:07<00:03,  4.43it/s]\u001b[A\n",
            " 68% 34/50 [00:07<00:03,  4.39it/s]\u001b[A\n",
            " 70% 35/50 [00:07<00:03,  4.40it/s]\u001b[A\n",
            " 72% 36/50 [00:07<00:03,  4.41it/s]\u001b[A\n",
            " 74% 37/50 [00:07<00:02,  4.44it/s]\u001b[A\n",
            " 76% 38/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 78% 39/50 [00:08<00:02,  4.44it/s]\u001b[A\n",
            " 80% 40/50 [00:08<00:02,  4.50it/s]\u001b[A\n",
            " 82% 41/50 [00:08<00:01,  4.50it/s]\u001b[A\n",
            " 84% 42/50 [00:09<00:01,  4.53it/s]\u001b[A\n",
            " 86% 43/50 [00:09<00:01,  4.56it/s]\u001b[A\n",
            " 88% 44/50 [00:09<00:01,  4.54it/s]\u001b[A\n",
            " 90% 45/50 [00:09<00:01,  4.48it/s]\u001b[A\n",
            " 92% 46/50 [00:09<00:00,  4.46it/s]\u001b[A\n",
            " 94% 47/50 [00:10<00:00,  4.48it/s]\u001b[A\n",
            " 96% 48/50 [00:10<00:00,  4.36it/s]\u001b[A\n",
            " 98% 49/50 [00:10<00:00,  4.36it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 0.841320812702179, 'eval_accuracy': 0.8276372819487409, 'eval_runtime': 11.0871, 'eval_samples_per_second': 9.02, 'eval_steps_per_second': 4.51, 'epoch': 2.67, 'num_input_tokens_seen': 1371072}\n",
            " 93% 50/54 [09:08<00:39,  9.76s/it]\n",
            "100% 50/50 [00:10<00:00,  4.45it/s]\u001b[A\n",
            "100% 54/54 [09:48<00:00, 11.02s/it][INFO|trainer.py:2329] 2024-07-14 11:02:51,467 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 588.0084, 'train_samples_per_second': 4.592, 'train_steps_per_second': 0.092, 'train_loss': 1.2913152906629775, 'epoch': 2.88, 'num_input_tokens_seen': 1480992}\n",
            "100% 54/54 [09:48<00:00, 10.89s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 11:02:51,472 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 11:02:51,677 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 11:02:51,677 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 11:02:51,883 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 11:02:51,887 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1480992\n",
            "  total_flos               = 30911033GF\n",
            "  train_loss               =     1.2913\n",
            "  train_runtime            = 0:09:48.00\n",
            "  train_samples_per_second =      4.592\n",
            "  train_steps_per_second   =      0.092\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 11:02:52,390 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:02:52,390 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:02:52,390 >>   Batch size = 2\n",
            "100% 50/50 [00:10<00:00,  4.58it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.8237\n",
            "  eval_loss               =     0.8405\n",
            "  eval_runtime            = 0:00:11.14\n",
            "  eval_samples_per_second =      8.971\n",
            "  eval_steps_per_second   =      4.486\n",
            "  num_input_tokens_seen   =    1480992\n",
            "[INFO|modelcard.py:450] 2024-07-14 11:03:03,563 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8236597548501753}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples500 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJDoVmsLjOsS",
        "outputId": "e197a7a2-4899-496e-e6c3-0364ddfc2780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-14 11:03:08.335834: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 11:03:08.386411: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 11:03:08.386454: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 11:03:08.387853: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 11:03:08.395370: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 11:03:09.599670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 11:03:15 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "07/14/2024 11:03:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 11:03:15,209 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 11:03:15,209 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 11:03:15,209 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 11:03:15,209 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 11:03:15,209 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 11:03:15,276 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 11:03:15 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 11:03:15 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 11:03:15 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 900/900 [00:00<00:00, 3835.70 examples/s]\n",
            "07/14/2024 11:03:15 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Converting format of dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 4645.04 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1900/1900 [00:00<00:00, 2004.19 examples/s]\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 1317, 1670, 385, 3462, 3245, 7865, 310, 5293, 6254, 271, 520, 21203, 830, 14268, 3323, 1853, 29871, 29906, 29874, 1954, 24579, 1148, 391, 2878, 331, 6020, 3831, 1965, 304, 6254, 271, 520, 21203, 830, 14268, 2522, 524, 335, 1140, 29891, 501, 415, 1296, 297, 21099, 919, 292, 18406, 307, 5893, 459, 273, 1037, 2454, 2448, 2192, 355, 8415, 457, 323, 398, 272, 13291, 29973, 13, 6007, 4330, 29990, 9375, 29901, 739, 338, 9815, 3692, 21622, 11251, 1047, 271, 520, 21203, 337, 14268, 1014, 1853, 29871, 29906, 29874, 313, 29879, 303, 29906, 29874, 29897, 5198, 348, 1148, 391, 2878, 331, 6020, 313, 29902, 19127, 29897, 756, 5684, 995, 9401, 304, 1047, 271, 520, 21203, 337, 14268, 885, 524, 335, 1140, 29891, 313, 29903, 12445, 29897, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 2933, 304, 1236, 415, 680, 337, 14268, 17937, 1228, 27580, 773, 29871, 29896, 29955, 29955, 24126, 29899, 20082, 276, 327, 403, 313, 10593, 13079, 29897, 297, 22069, 411, 330, 23364, 5893, 459, 273, 1037, 2454, 452, 2192, 355, 8415, 457, 21622, 943, 313, 1692, 29925, 29899, 6006, 29879, 467, 450, 263, 9893, 310, 445, 6559, 892, 29901, 313, 29896, 29897, 304, 10127, 278, 19649, 310, 269, 303, 29906, 29874, 5198, 348, 459, 359, 24858, 297, 402, 15488, 29899, 6006, 11916, 310, 12089, 13079, 29899, 2484, 630, 22069, 29892, 313, 29906, 29897, 304, 8161, 278, 9443, 1546, 1900, 402, 15488, 29899, 6006, 2933, 773, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29871, 29896, 1629, 1156, 12089, 13079, 322, 21622, 11251, 269, 303, 29906, 29874, 306, 19127, 29892, 322, 313, 29941, 29897, 304, 7252, 21862, 310, 22069, 411, 269, 303, 29906, 29874, 306, 19127, 29899, 22198, 322, 448, 1066, 3321, 21622, 943, 29889, 2178, 29871, 29955, 29941, 18942, 22069, 892, 4629, 363, 12089, 13079, 2729, 373, 263, 6374, 317, 12445, 29889, 4957, 29875, 5996, 2933, 471, 15569, 5034, 304, 5195, 8426, 1254, 29871, 29896, 29889, 29900, 16614, 29889, 269, 303, 29906, 29874, 4660, 471, 17809, 373, 21622, 272, 11916, 491, 306, 19127, 29889, 512, 3001, 29892, 29871, 29929, 29941, 29995, 310, 402, 15488, 29899, 6006, 11916, 10018, 269, 303, 29906, 29874, 306, 19127, 13686, 2068, 29889, 1939, 12997, 1711, 7282, 9443, 471, 8900, 1546, 297, 13901, 307, 269, 303, 29906, 29874, 4603, 322, 297, 325, 4243, 1900, 402, 15488, 29899, 6006, 2933, 29871, 29896, 1629, 1156, 12089, 13079, 313, 29886, 353, 29871, 29900, 29889, 29946, 29955, 467, 21703, 29892, 7601, 21622, 272, 3268, 29892, 17135, 7408, 29892, 382, 6006, 29903, 323, 29940, 29924, 12965, 29892, 16540, 29899, 29953, 29955, 2380, 29892, 9939, 724, 398, 25173, 468, 661, 262, 29899, 29909, 3233, 29892, 322, 9939, 26808, 265, 29899, 14940, 427, 324, 559, 3233, 892, 451, 16951, 1422, 1546, 22069, 411, 8178, 322, 6374, 269, 303, 29906, 29874, 21622, 11251, 306, 19127, 411, 278, 3682, 310, 5046, 472, 24876, 19263, 313, 29886, 353, 29871, 29900, 29889, 29900, 29900, 29955, 467, 32007, 29871, 13, 32001, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Is There an Additional Value of Using Somatostatin Receptor Subtype 2a Immunohistochemistry Compared to Somatostatin Receptor Scintigraphy Uptake in Predicting Gastroenteropancreatic Neuroendocrine Tumor Response?\n",
            "CONTEXTS: It is unknown whether tumoral somatostatin receptor subtype 2a (sst2a) immunohistochemistry (IHC) has additional value compared to somatostatin receptor scintigraphy (SRS) uptake using OctreoScan® in predicting response to peptide receptor radiotherapy using 177Lu-octreotate (PRRT) in patients with gastroenteropancreatic neuroendocrine tumors (GEP-NETs). The aims of this study were: (1) to establish the percentage of sst2a immunopositivity in GEP-NET samples of PRRT-treated patients, (2) to determine the relationship between best GEP-NET response using RECIST 1.0 criteria 1 year after PRRT and tumoral sst2a IHC, and (3) to compare characteristics of patients with sst2a IHC-negative and -positive tumors. All 73 consecutive patients were selected for PRRT based on a positive SRS. Radiological response was scored according to RECIST 1.0 criteria. sst2a status was detected on tumor samples by IHC. In total, 93% of GEP-NET samples showed sst2a IHC positivity. No statistically significant relationship was observed between in vitro sst2a expression and in vivo best GEP-NET response 1 year after PRRT (p = 0.47). Sex, primary tumor site, disease stage, ENETS TNM classification, Ki-67 index, highest serum chromogranin-A level, and highest neuron-specific enolase level were not significantly different between patients with negative and positive sst2a tumoral IHC with the exception of age at diagnosis (p = 0.007).<|end|> \n",
            "<|assistant|> no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 694, 29889, 269, 303, 29906, 29874, 306, 19127, 310, 21622, 272, 11916, 756, 694, 5684, 995, 9401, 304, 317, 12445, 318, 415, 1296, 773, 4756, 276, 29877, 29083, 30342, 297, 8500, 292, 21622, 272, 2933, 1156, 12089, 13079, 29889, 32007]\n",
            "labels:\n",
            "no. sst2a IHC of tumor samples has no additional value compared to SRS uptake using OctreoScan® in predicting tumor response after PRRT.<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 11:03:17,888 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 11:03:17,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 11:03:17,979 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/14/2024 11:03:17 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit with bitsandbytes.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 11:03:18,078 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 11:03:18,079 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 11:03:18,080 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.48s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 11:03:23,340 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 11:03:23,340 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 11:03:23,394 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 11:03:23,395 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,gate_up_proj,down_proj,o_proj\n",
            "07/14/2024 11:03:23 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-14 11:03:23,746 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 11:03:24,800 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 11:03:24,800 >>   Num examples = 1,710\n",
            "[INFO|trainer.py:2080] 2024-07-14 11:03:24,800 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 11:03:24,800 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 11:03:24,800 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 11:03:24,800 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 11:03:24,800 >>   Total optimization steps = 105\n",
            "[INFO|trainer.py:2087] 2024-07-14 11:03:24,804 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/105 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6307, 'grad_norm': nan, 'learning_rate': 4.9821173158545936e-05, 'epoch': 0.14, 'num_input_tokens_seen': 134736}\n",
            "{'loss': 1.4951, 'grad_norm': 0.5975931882858276, 'learning_rate': 4.909907151739633e-05, 'epoch': 0.28, 'num_input_tokens_seen': 272400}\n",
            " 10% 10/105 [01:38<15:31,  9.80s/it][INFO|trainer.py:3719] 2024-07-14 11:05:03,642 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:05:03,642 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:05:03,642 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  8.58it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.25it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.59it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.23it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:17,  4.98it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.89it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:18,  4.66it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.69it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.71it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:17,  4.72it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.76it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.75it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.71it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:16,  4.73it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.62it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:16,  4.62it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:16,  4.59it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.65it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:15,  4.71it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:15,  4.72it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:15,  4.58it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.50it/s]\u001b[A\n",
            " 25% 24/95 [00:04<00:15,  4.51it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.60it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.55it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:14,  4.58it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:14,  4.52it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.57it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.56it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.43it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:13,  4.52it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.35it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.44it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.39it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.58it/s]\u001b[A\n",
            " 39% 37/95 [00:07<00:12,  4.62it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.59it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:11,  4.70it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.72it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.69it/s]\u001b[A\n",
            " 44% 42/95 [00:08<00:11,  4.67it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.68it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.57it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.47it/s]\u001b[A\n",
            " 48% 46/95 [00:09<00:10,  4.55it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.59it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.62it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:09,  4.66it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.68it/s]\u001b[A\n",
            " 54% 51/95 [00:10<00:09,  4.70it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.59it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.63it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.51it/s]\u001b[A\n",
            " 58% 55/95 [00:11<00:08,  4.48it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.50it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.54it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.33it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.43it/s]\u001b[A\n",
            " 63% 60/95 [00:12<00:07,  4.52it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.44it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.54it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:06,  4.62it/s]\u001b[A\n",
            " 67% 64/95 [00:13<00:06,  4.59it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.47it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.45it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.60it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:05,  4.65it/s]\u001b[A\n",
            " 73% 69/95 [00:14<00:05,  4.48it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.60it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.39it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.50it/s]\u001b[A\n",
            " 77% 73/95 [00:15<00:04,  4.57it/s]\u001b[A\n",
            " 78% 74/95 [00:15<00:04,  4.51it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.60it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.66it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.68it/s]\u001b[A\n",
            " 82% 78/95 [00:16<00:03,  4.67it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.67it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.62it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.64it/s]\u001b[A\n",
            " 86% 82/95 [00:17<00:02,  4.70it/s]\u001b[A\n",
            " 87% 83/95 [00:17<00:02,  4.74it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.77it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.78it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.77it/s]\u001b[A\n",
            " 92% 87/95 [00:18<00:01,  4.77it/s]\u001b[A\n",
            " 93% 88/95 [00:18<00:01,  4.72it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.69it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.70it/s]\u001b[A\n",
            " 96% 91/95 [00:19<00:00,  4.58it/s]\u001b[A\n",
            " 97% 92/95 [00:19<00:00,  4.58it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.65it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.66it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.173993706703186, 'eval_accuracy': 0.788399944049979, 'eval_runtime': 20.6883, 'eval_samples_per_second': 9.184, 'eval_steps_per_second': 4.592, 'epoch': 0.28, 'num_input_tokens_seen': 272400}\n",
            " 10% 10/105 [01:59<15:31,  9.80s/it]\n",
            "100% 95/95 [00:20<00:00,  4.65it/s]\u001b[A\n",
            "{'loss': 1.3795, 'grad_norm': 0.49776381254196167, 'learning_rate': 4.783863644106502e-05, 'epoch': 0.42, 'num_input_tokens_seen': 412896}\n",
            "{'loss': 1.1803, 'grad_norm': 0.31124112010002136, 'learning_rate': 4.606802396635098e-05, 'epoch': 0.56, 'num_input_tokens_seen': 551040}\n",
            " 19% 20/105 [03:39<14:37, 10.32s/it][INFO|trainer.py:3719] 2024-07-14 11:07:04,377 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:07:04,378 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:07:04,378 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.08it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.20it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.50it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.17it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.90it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.81it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:18,  4.59it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.64it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.62it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.65it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.70it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.67it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.65it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.68it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.57it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.57it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.52it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.58it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.63it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:15,  4.66it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.54it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.45it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:15,  4.46it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.55it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.50it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:14,  4.54it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:14,  4.49it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.54it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.50it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.36it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.45it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.25it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.36it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.33it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:13,  4.50it/s]\u001b[A\n",
            " 39% 37/95 [00:07<00:12,  4.51it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.48it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.57it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.59it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.58it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.48it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.39it/s]\u001b[A\n",
            " 48% 46/95 [00:09<00:10,  4.46it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.51it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.54it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.55it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.59it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.61it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.52it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.54it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.43it/s]\u001b[A\n",
            " 58% 55/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.43it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.47it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.26it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.36it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.46it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.38it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.47it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.54it/s]\u001b[A\n",
            " 67% 64/95 [00:13<00:06,  4.52it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.37it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.33it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.48it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:05,  4.51it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.37it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.50it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.31it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.43it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.46it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.40it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.50it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.56it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.58it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.58it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.58it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.55it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 86% 82/95 [00:17<00:02,  4.63it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.66it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.68it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.63it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.63it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.59it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.58it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.61it/s]\u001b[A\n",
            " 96% 91/95 [00:19<00:00,  4.47it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.46it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.53it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.52it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.0244983434677124, 'eval_accuracy': 0.8057194835782965, 'eval_runtime': 21.0665, 'eval_samples_per_second': 9.019, 'eval_steps_per_second': 4.51, 'epoch': 0.56, 'num_input_tokens_seen': 551040}\n",
            " 19% 20/105 [04:00<14:37, 10.32s/it]\n",
            "100% 95/95 [00:20<00:00,  4.53it/s]\u001b[A\n",
            "{'loss': 1.2235, 'grad_norm': 0.3148070275783539, 'learning_rate': 4.382678665009028e-05, 'epoch': 0.7, 'num_input_tokens_seen': 685584}\n",
            "{'loss': 1.2309, 'grad_norm': 0.2329755574464798, 'learning_rate': 4.116499003039499e-05, 'epoch': 0.84, 'num_input_tokens_seen': 825264}\n",
            " 29% 30/105 [05:39<12:55, 10.34s/it][INFO|trainer.py:3719] 2024-07-14 11:09:04,272 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:09:04,273 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:09:04,273 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.05it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.17it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.50it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.16it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.89it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.80it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:19,  4.58it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.62it/s]\u001b[A\n",
            " 11% 10/95 [00:02<00:18,  4.59it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.64it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.68it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.68it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.65it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.69it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.56it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.56it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.53it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.60it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.63it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:15,  4.63it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.50it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.43it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:15,  4.44it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.53it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.50it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.51it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:14,  4.47it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.53it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.51it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.38it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.48it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.30it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.42it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.40it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.59it/s]\u001b[A\n",
            " 39% 37/95 [00:07<00:12,  4.63it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.57it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.58it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.57it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.57it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.57it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.48it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.37it/s]\u001b[A\n",
            " 48% 46/95 [00:09<00:10,  4.47it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:11,  4.32it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.41it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.47it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.55it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.60it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.48it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.52it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.43it/s]\u001b[A\n",
            " 58% 55/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.42it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.46it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.25it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.37it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.47it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.39it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.49it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.56it/s]\u001b[A\n",
            " 67% 64/95 [00:13<00:06,  4.53it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.39it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.37it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.50it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:05,  4.54it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.41it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.52it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.32it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.45it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.49it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.39it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.47it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.54it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.58it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.53it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.61it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.63it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.65it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.67it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.66it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.65it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.62it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.62it/s]\u001b[A\n",
            " 96% 91/95 [00:19<00:00,  4.48it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.47it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.51it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.48it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9897189736366272, 'eval_accuracy': 0.808205041022257, 'eval_runtime': 21.0851, 'eval_samples_per_second': 9.011, 'eval_steps_per_second': 4.506, 'epoch': 0.84, 'num_input_tokens_seen': 825264}\n",
            " 29% 30/105 [06:00<12:55, 10.34s/it]\n",
            "100% 95/95 [00:20<00:00,  4.48it/s]\u001b[A\n",
            "{'loss': 1.0646, 'grad_norm': 0.26029911637306213, 'learning_rate': 3.814209424526262e-05, 'epoch': 0.98, 'num_input_tokens_seen': 954000}\n",
            "{'loss': 1.0961, 'grad_norm': 0.26300835609436035, 'learning_rate': 3.4825625791348096e-05, 'epoch': 1.12, 'num_input_tokens_seen': 1094160}\n",
            " 38% 40/105 [07:38<11:10, 10.31s/it][INFO|trainer.py:3719] 2024-07-14 11:11:02,908 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:11:02,909 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:11:02,909 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.29it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.30it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.57it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.21it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:17,  4.96it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.83it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:19,  4.57it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.62it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.61it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:19,  4.30it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:18,  4.44it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:18,  4.49it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.51it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.60it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.52it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.59it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.64it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.62it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.49it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.41it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.43it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.53it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.47it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.49it/s]\u001b[A\n",
            " 29% 28/95 [00:06<00:14,  4.48it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.55it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.52it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.38it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.48it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.30it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.41it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.38it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.57it/s]\u001b[A\n",
            " 39% 37/95 [00:08<00:12,  4.60it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.55it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.62it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.62it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.62it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.64it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.52it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.41it/s]\u001b[A\n",
            " 48% 46/95 [00:09<00:10,  4.50it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.54it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.56it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.58it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.60it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.64it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.54it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.55it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.43it/s]\u001b[A\n",
            " 58% 55/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.42it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.46it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.25it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.35it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.45it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.38it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.47it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.55it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.53it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.39it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.35it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.47it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:05,  4.50it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.37it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.49it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.31it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.45it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.50it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.43it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.52it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.58it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.59it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.59it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.60it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.59it/s]\u001b[A\n",
            " 86% 82/95 [00:17<00:02,  4.66it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.69it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.73it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.70it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.68it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.66it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.65it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.62it/s]\u001b[A\n",
            " 96% 91/95 [00:19<00:00,  4.47it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.46it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.52it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.53it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.974864661693573, 'eval_accuracy': 0.8128831646115947, 'eval_runtime': 21.0567, 'eval_samples_per_second': 9.023, 'eval_steps_per_second': 4.512, 'epoch': 1.12, 'num_input_tokens_seen': 1094160}\n",
            " 38% 40/105 [07:59<11:10, 10.31s/it]\n",
            "100% 95/95 [00:20<00:00,  4.53it/s]\u001b[A\n",
            "{'loss': 1.1795, 'grad_norm': 0.23659592866897583, 'learning_rate': 3.1289669093612714e-05, 'epoch': 1.26, 'num_input_tokens_seen': 1227792}\n",
            "{'loss': 1.0838, 'grad_norm': 0.2759784758090973, 'learning_rate': 2.761321158169134e-05, 'epoch': 1.4, 'num_input_tokens_seen': 1367808}\n",
            " 48% 50/105 [09:37<09:18, 10.16s/it][INFO|trainer.py:3719] 2024-07-14 11:13:02,530 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:13:02,530 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:13:02,530 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.22it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.27it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.53it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.19it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.94it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.84it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:18,  4.58it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.61it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.58it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.58it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.64it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.61it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.58it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.61it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.49it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.47it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.46it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.53it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.58it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.58it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.47it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.40it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.42it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.53it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:16,  4.17it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.29it/s]\u001b[A\n",
            " 29% 28/95 [00:06<00:15,  4.34it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.47it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.49it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.36it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.45it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.30it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.42it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.39it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.57it/s]\u001b[A\n",
            " 39% 37/95 [00:08<00:12,  4.57it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.53it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.62it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.62it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.61it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.63it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.65it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.52it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.42it/s]\u001b[A\n",
            " 48% 46/95 [00:10<00:10,  4.49it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.53it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.56it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.59it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.62it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.61it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.51it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.55it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.44it/s]\u001b[A\n",
            " 58% 55/95 [00:12<00:09,  4.43it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.46it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.49it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.29it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.38it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.49it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.40it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.51it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.55it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.51it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.36it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.33it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.44it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:06,  4.45it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:06,  4.30it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.42it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.21it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.36it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.44it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.41it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.50it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.56it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.58it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.58it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.61it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.64it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.65it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.68it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.68it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.66it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.63it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.63it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.63it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.65it/s]\u001b[A\n",
            " 96% 91/95 [00:20<00:00,  4.51it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.49it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.55it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.56it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9686406254768372, 'eval_accuracy': 0.813883548778765, 'eval_runtime': 21.1054, 'eval_samples_per_second': 9.002, 'eval_steps_per_second': 4.501, 'epoch': 1.4, 'num_input_tokens_seen': 1367808}\n",
            " 48% 50/105 [09:58<09:18, 10.16s/it]\n",
            "100% 95/95 [00:20<00:00,  4.54it/s]\u001b[A\n",
            "{'loss': 1.1874, 'grad_norm': 0.22769314050674438, 'learning_rate': 2.3878379241237136e-05, 'epoch': 1.54, 'num_input_tokens_seen': 1506144}\n",
            "{'loss': 1.1815, 'grad_norm': 0.21589413285255432, 'learning_rate': 2.0168602055111173e-05, 'epoch': 1.68, 'num_input_tokens_seen': 1640928}\n",
            " 57% 60/105 [11:37<07:26,  9.92s/it][INFO|trainer.py:3719] 2024-07-14 11:15:01,856 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:15:01,857 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:15:01,857 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.18it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.26it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.49it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.17it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.91it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.82it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:18,  4.58it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.61it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.61it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.63it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.65it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.61it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.60it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.65it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.53it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.53it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.56it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.56it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.53it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.40it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.32it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.35it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.46it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.39it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.41it/s]\u001b[A\n",
            " 29% 28/95 [00:06<00:15,  4.39it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.44it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.44it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.31it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.44it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.27it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.38it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.35it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:13,  4.53it/s]\u001b[A\n",
            " 39% 37/95 [00:08<00:12,  4.55it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.49it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.58it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.58it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:12,  4.23it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:12,  4.32it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.30it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.26it/s]\u001b[A\n",
            " 48% 46/95 [00:10<00:11,  4.36it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.44it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.49it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.53it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.54it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.59it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.50it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.52it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 58% 55/95 [00:12<00:09,  4.40it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.42it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.46it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.24it/s]\u001b[A\n",
            " 62% 59/95 [00:13<00:08,  4.34it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.44it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.37it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.45it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.51it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.50it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.35it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.32it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.44it/s]\u001b[A\n",
            " 72% 68/95 [00:15<00:06,  4.49it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.36it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.47it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.30it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.42it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.46it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.37it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.43it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.48it/s]\u001b[A\n",
            " 81% 77/95 [00:17<00:04,  4.48it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.49it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.50it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.43it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.46it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.50it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.52it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.58it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.58it/s]\u001b[A\n",
            " 91% 86/95 [00:19<00:01,  4.61it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.61it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.63it/s]\u001b[A\n",
            " 96% 91/95 [00:20<00:00,  4.50it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.51it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.53it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.58it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9644298553466797, 'eval_accuracy': 0.8129113921274225, 'eval_runtime': 21.2556, 'eval_samples_per_second': 8.939, 'eval_steps_per_second': 4.469, 'epoch': 1.68, 'num_input_tokens_seen': 1640928}\n",
            " 57% 60/105 [11:58<07:26,  9.92s/it]\n",
            "100% 95/95 [00:21<00:00,  4.57it/s]\u001b[A\n",
            "{'loss': 1.1424, 'grad_norm': 1.2487525939941406, 'learning_rate': 1.6566750315429254e-05, 'epoch': 1.82, 'num_input_tokens_seen': 1780368}\n",
            "{'loss': 1.1072, 'grad_norm': 0.20787164568901062, 'learning_rate': 1.3153283438175034e-05, 'epoch': 1.96, 'num_input_tokens_seen': 1921296}\n",
            " 67% 70/105 [13:38<06:03, 10.40s/it][INFO|trainer.py:3719] 2024-07-14 11:17:03,777 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:17:03,777 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:17:03,777 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.23it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.30it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.53it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.17it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.91it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.82it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:18,  4.61it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.65it/s]\u001b[A\n",
            " 11% 10/95 [00:01<00:18,  4.63it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.66it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.72it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.67it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.63it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.67it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.55it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.56it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.52it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.57it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.63it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:15,  4.63it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.49it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.41it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.41it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.47it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.44it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.47it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:15,  4.44it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.50it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.48it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.36it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.47it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.28it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.39it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.35it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.54it/s]\u001b[A\n",
            " 39% 37/95 [00:07<00:12,  4.56it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.52it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.61it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.61it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.58it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.59it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.49it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.38it/s]\u001b[A\n",
            " 48% 46/95 [00:10<00:11,  4.14it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:11,  4.25it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.36it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.44it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.50it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.56it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.47it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.51it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.42it/s]\u001b[A\n",
            " 58% 55/95 [00:12<00:09,  4.41it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.44it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.48it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.27it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.38it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.49it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.41it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.50it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.56it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.54it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.40it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.37it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.48it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:05,  4.52it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.39it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.50it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.32it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.45it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.51it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.43it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.50it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.56it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.56it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.59it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.49it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.50it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.56it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.58it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.62it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.61it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.63it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.62it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 96% 91/95 [00:20<00:00,  4.49it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.51it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.56it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.57it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9630878567695618, 'eval_accuracy': 0.8122007040075914, 'eval_runtime': 21.108, 'eval_samples_per_second': 9.001, 'eval_steps_per_second': 4.501, 'epoch': 1.96, 'num_input_tokens_seen': 1921296}\n",
            " 67% 70/105 [14:00<06:03, 10.40s/it]\n",
            "100% 95/95 [00:20<00:00,  4.57it/s]\u001b[A\n",
            "{'loss': 1.2424, 'grad_norm': 0.23914392292499542, 'learning_rate': 1.0004452632802158e-05, 'epoch': 2.11, 'num_input_tokens_seen': 2066208}\n",
            "{'loss': 1.1944, 'grad_norm': 0.23790641129016876, 'learning_rate': 7.190597576216385e-06, 'epoch': 2.25, 'num_input_tokens_seen': 2201712}\n",
            " 76% 80/105 [15:40<04:11, 10.08s/it][INFO|trainer.py:3719] 2024-07-14 11:19:05,626 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:19:05,626 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:19:05,626 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.29it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.36it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.58it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.17it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.86it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.77it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:19,  4.55it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.59it/s]\u001b[A\n",
            " 11% 10/95 [00:02<00:18,  4.56it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.60it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.66it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.65it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.61it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.63it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.49it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.52it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.55it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.56it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.44it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.36it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.39it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.48it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.39it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.42it/s]\u001b[A\n",
            " 29% 28/95 [00:06<00:15,  4.41it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.46it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.43it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.31it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.40it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.24it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:14,  4.34it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.32it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:13,  4.50it/s]\u001b[A\n",
            " 39% 37/95 [00:08<00:12,  4.52it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.48it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.56it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:12,  4.55it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.51it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.55it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.54it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.44it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.32it/s]\u001b[A\n",
            " 48% 46/95 [00:10<00:11,  4.41it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.46it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.50it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.51it/s]\u001b[A\n",
            " 53% 50/95 [00:11<00:10,  4.22it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:10,  4.36it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.36it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.43it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.36it/s]\u001b[A\n",
            " 58% 55/95 [00:12<00:09,  4.36it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.39it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.43it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.22it/s]\u001b[A\n",
            " 62% 59/95 [00:13<00:08,  4.34it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.45it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.37it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.45it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.53it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.50it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.35it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.31it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.42it/s]\u001b[A\n",
            " 72% 68/95 [00:15<00:06,  4.48it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.37it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.50it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.31it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.42it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.46it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.39it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.46it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.47it/s]\u001b[A\n",
            " 81% 77/95 [00:17<00:03,  4.50it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.51it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.53it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.48it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.48it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.54it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.54it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.58it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.59it/s]\u001b[A\n",
            " 91% 86/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.57it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.55it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.59it/s]\u001b[A\n",
            " 96% 91/95 [00:20<00:00,  4.45it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.44it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.50it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.50it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.96136873960495, 'eval_accuracy': 0.8128500036820786, 'eval_runtime': 21.2919, 'eval_samples_per_second': 8.924, 'eval_steps_per_second': 4.462, 'epoch': 2.25, 'num_input_tokens_seen': 2201712}\n",
            " 76% 80/105 [16:02<04:11, 10.08s/it]\n",
            "100% 95/95 [00:21<00:00,  4.50it/s]\u001b[A\n",
            "{'loss': 1.0749, 'grad_norm': 0.17606490850448608, 'learning_rate': 4.7745751406263165e-06, 'epoch': 2.39, 'num_input_tokens_seen': 2337744}\n",
            "{'loss': 1.1286, 'grad_norm': 0.20938628911972046, 'learning_rate': 2.8103552748861476e-06, 'epoch': 2.53, 'num_input_tokens_seen': 2470128}\n",
            " 86% 90/105 [17:39<02:28,  9.92s/it][INFO|trainer.py:3719] 2024-07-14 11:21:04,221 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:21:04,221 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:21:04,221 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.13it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.20it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.48it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.15it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.90it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.78it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:19,  4.54it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.59it/s]\u001b[A\n",
            " 11% 10/95 [00:02<00:18,  4.58it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.61it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.64it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.63it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.61it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.65it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.53it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.54it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:17,  4.51it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.59it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.62it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.60it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.46it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.39it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.41it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.49it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.46it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.49it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:15,  4.46it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.51it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.49it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.36it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.43it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.27it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.36it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.32it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:13,  4.50it/s]\u001b[A\n",
            " 39% 37/95 [00:08<00:12,  4.52it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.47it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.54it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:12,  4.55it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.53it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.54it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.40it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.36it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.30it/s]\u001b[A\n",
            " 48% 46/95 [00:10<00:11,  4.41it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.46it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.49it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.52it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.56it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.58it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.47it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.50it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 58% 55/95 [00:12<00:09,  4.39it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.40it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.42it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.22it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.34it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.44it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.36it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.46it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.52it/s]\u001b[A\n",
            " 67% 64/95 [00:14<00:06,  4.48it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.33it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.30it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.42it/s]\u001b[A\n",
            " 72% 68/95 [00:15<00:06,  4.46it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:06,  4.04it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.21it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.12it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.28it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:05,  4.34it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.31it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.42it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.48it/s]\u001b[A\n",
            " 81% 77/95 [00:17<00:03,  4.51it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.53it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.54it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.61it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.61it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.62it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.64it/s]\u001b[A\n",
            " 91% 86/95 [00:19<00:01,  4.62it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.60it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.58it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.54it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.56it/s]\u001b[A\n",
            " 96% 91/95 [00:20<00:00,  4.43it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.44it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.51it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.54it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.9632181525230408, 'eval_accuracy': 0.811825330420443, 'eval_runtime': 21.2769, 'eval_samples_per_second': 8.93, 'eval_steps_per_second': 4.465, 'epoch': 2.53, 'num_input_tokens_seen': 2470128}\n",
            " 86% 90/105 [18:00<02:28,  9.92s/it]\n",
            "100% 95/95 [00:21<00:00,  4.53it/s]\u001b[A\n",
            "{'loss': 1.0612, 'grad_norm': 0.1898968368768692, 'learning_rate': 1.3418154050208936e-06, 'epoch': 2.67, 'num_input_tokens_seen': 2607408}\n",
            "{'loss': 1.122, 'grad_norm': 0.2160274088382721, 'learning_rate': 4.0176028503425835e-07, 'epoch': 2.81, 'num_input_tokens_seen': 2742480}\n",
            " 95% 100/105 [19:39<00:50, 10.06s/it][INFO|trainer.py:3719] 2024-07-14 11:23:04,515 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:23:04,515 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:23:04,515 >>   Batch size = 2\n",
            "\n",
            "  0% 0/95 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/95 [00:00<00:10,  9.13it/s]\u001b[A\n",
            "  3% 3/95 [00:00<00:14,  6.28it/s]\u001b[A\n",
            "  4% 4/95 [00:00<00:16,  5.52it/s]\u001b[A\n",
            "  5% 5/95 [00:00<00:17,  5.15it/s]\u001b[A\n",
            "  6% 6/95 [00:01<00:18,  4.87it/s]\u001b[A\n",
            "  7% 7/95 [00:01<00:18,  4.79it/s]\u001b[A\n",
            "  8% 8/95 [00:01<00:19,  4.56it/s]\u001b[A\n",
            "  9% 9/95 [00:01<00:18,  4.60it/s]\u001b[A\n",
            " 11% 10/95 [00:02<00:18,  4.60it/s]\u001b[A\n",
            " 12% 11/95 [00:02<00:18,  4.66it/s]\u001b[A\n",
            " 13% 12/95 [00:02<00:17,  4.73it/s]\u001b[A\n",
            " 14% 13/95 [00:02<00:17,  4.72it/s]\u001b[A\n",
            " 15% 14/95 [00:02<00:17,  4.69it/s]\u001b[A\n",
            " 16% 15/95 [00:03<00:17,  4.70it/s]\u001b[A\n",
            " 17% 16/95 [00:03<00:17,  4.58it/s]\u001b[A\n",
            " 18% 17/95 [00:03<00:17,  4.56it/s]\u001b[A\n",
            " 19% 18/95 [00:03<00:16,  4.53it/s]\u001b[A\n",
            " 20% 19/95 [00:03<00:16,  4.60it/s]\u001b[A\n",
            " 21% 20/95 [00:04<00:16,  4.61it/s]\u001b[A\n",
            " 22% 21/95 [00:04<00:16,  4.62it/s]\u001b[A\n",
            " 23% 22/95 [00:04<00:16,  4.49it/s]\u001b[A\n",
            " 24% 23/95 [00:04<00:16,  4.41it/s]\u001b[A\n",
            " 25% 24/95 [00:05<00:16,  4.43it/s]\u001b[A\n",
            " 26% 25/95 [00:05<00:15,  4.53it/s]\u001b[A\n",
            " 27% 26/95 [00:05<00:15,  4.51it/s]\u001b[A\n",
            " 28% 27/95 [00:05<00:15,  4.52it/s]\u001b[A\n",
            " 29% 28/95 [00:05<00:14,  4.47it/s]\u001b[A\n",
            " 31% 29/95 [00:06<00:14,  4.54it/s]\u001b[A\n",
            " 32% 30/95 [00:06<00:14,  4.52it/s]\u001b[A\n",
            " 33% 31/95 [00:06<00:14,  4.37it/s]\u001b[A\n",
            " 34% 32/95 [00:06<00:14,  4.45it/s]\u001b[A\n",
            " 35% 33/95 [00:07<00:14,  4.27it/s]\u001b[A\n",
            " 36% 34/95 [00:07<00:13,  4.39it/s]\u001b[A\n",
            " 37% 35/95 [00:07<00:13,  4.36it/s]\u001b[A\n",
            " 38% 36/95 [00:07<00:12,  4.54it/s]\u001b[A\n",
            " 39% 37/95 [00:07<00:12,  4.56it/s]\u001b[A\n",
            " 40% 38/95 [00:08<00:12,  4.52it/s]\u001b[A\n",
            " 41% 39/95 [00:08<00:12,  4.61it/s]\u001b[A\n",
            " 42% 40/95 [00:08<00:11,  4.61it/s]\u001b[A\n",
            " 43% 41/95 [00:08<00:11,  4.59it/s]\u001b[A\n",
            " 44% 42/95 [00:09<00:11,  4.61it/s]\u001b[A\n",
            " 45% 43/95 [00:09<00:11,  4.61it/s]\u001b[A\n",
            " 46% 44/95 [00:09<00:11,  4.49it/s]\u001b[A\n",
            " 47% 45/95 [00:09<00:11,  4.38it/s]\u001b[A\n",
            " 48% 46/95 [00:09<00:10,  4.46it/s]\u001b[A\n",
            " 49% 47/95 [00:10<00:10,  4.47it/s]\u001b[A\n",
            " 51% 48/95 [00:10<00:10,  4.50it/s]\u001b[A\n",
            " 52% 49/95 [00:10<00:10,  4.53it/s]\u001b[A\n",
            " 53% 50/95 [00:10<00:09,  4.54it/s]\u001b[A\n",
            " 54% 51/95 [00:11<00:09,  4.57it/s]\u001b[A\n",
            " 55% 52/95 [00:11<00:09,  4.48it/s]\u001b[A\n",
            " 56% 53/95 [00:11<00:09,  4.50it/s]\u001b[A\n",
            " 57% 54/95 [00:11<00:09,  4.41it/s]\u001b[A\n",
            " 58% 55/95 [00:11<00:09,  4.40it/s]\u001b[A\n",
            " 59% 56/95 [00:12<00:08,  4.43it/s]\u001b[A\n",
            " 60% 57/95 [00:12<00:08,  4.46it/s]\u001b[A\n",
            " 61% 58/95 [00:12<00:08,  4.26it/s]\u001b[A\n",
            " 62% 59/95 [00:12<00:08,  4.35it/s]\u001b[A\n",
            " 63% 60/95 [00:13<00:07,  4.45it/s]\u001b[A\n",
            " 64% 61/95 [00:13<00:07,  4.38it/s]\u001b[A\n",
            " 65% 62/95 [00:13<00:07,  4.47it/s]\u001b[A\n",
            " 66% 63/95 [00:13<00:07,  4.53it/s]\u001b[A\n",
            " 67% 64/95 [00:13<00:06,  4.51it/s]\u001b[A\n",
            " 68% 65/95 [00:14<00:06,  4.38it/s]\u001b[A\n",
            " 69% 66/95 [00:14<00:06,  4.35it/s]\u001b[A\n",
            " 71% 67/95 [00:14<00:06,  4.45it/s]\u001b[A\n",
            " 72% 68/95 [00:14<00:06,  4.49it/s]\u001b[A\n",
            " 73% 69/95 [00:15<00:05,  4.36it/s]\u001b[A\n",
            " 74% 70/95 [00:15<00:05,  4.49it/s]\u001b[A\n",
            " 75% 71/95 [00:15<00:05,  4.31it/s]\u001b[A\n",
            " 76% 72/95 [00:15<00:05,  4.44it/s]\u001b[A\n",
            " 77% 73/95 [00:16<00:04,  4.48it/s]\u001b[A\n",
            " 78% 74/95 [00:16<00:04,  4.44it/s]\u001b[A\n",
            " 79% 75/95 [00:16<00:04,  4.52it/s]\u001b[A\n",
            " 80% 76/95 [00:16<00:04,  4.58it/s]\u001b[A\n",
            " 81% 77/95 [00:16<00:03,  4.58it/s]\u001b[A\n",
            " 82% 78/95 [00:17<00:03,  4.57it/s]\u001b[A\n",
            " 83% 79/95 [00:17<00:03,  4.60it/s]\u001b[A\n",
            " 84% 80/95 [00:17<00:03,  4.56it/s]\u001b[A\n",
            " 85% 81/95 [00:17<00:03,  4.23it/s]\u001b[A\n",
            " 86% 82/95 [00:18<00:02,  4.39it/s]\u001b[A\n",
            " 87% 83/95 [00:18<00:02,  4.49it/s]\u001b[A\n",
            " 88% 84/95 [00:18<00:02,  4.58it/s]\u001b[A\n",
            " 89% 85/95 [00:18<00:02,  4.64it/s]\u001b[A\n",
            " 91% 86/95 [00:18<00:01,  4.67it/s]\u001b[A\n",
            " 92% 87/95 [00:19<00:01,  4.67it/s]\u001b[A\n",
            " 93% 88/95 [00:19<00:01,  4.66it/s]\u001b[A\n",
            " 94% 89/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 95% 90/95 [00:19<00:01,  4.64it/s]\u001b[A\n",
            " 96% 91/95 [00:19<00:00,  4.48it/s]\u001b[A\n",
            " 97% 92/95 [00:20<00:00,  4.48it/s]\u001b[A\n",
            " 98% 93/95 [00:20<00:00,  4.52it/s]\u001b[A\n",
            " 99% 94/95 [00:20<00:00,  4.55it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.9616230130195618, 'eval_accuracy': 0.8118246211049532, 'eval_runtime': 21.0989, 'eval_samples_per_second': 9.005, 'eval_steps_per_second': 4.503, 'epoch': 2.81, 'num_input_tokens_seen': 2742480}\n",
            " 95% 100/105 [20:00<00:50, 10.06s/it]\n",
            "100% 95/95 [00:20<00:00,  4.55it/s]\u001b[A\n",
            "{'loss': 1.1387, 'grad_norm': 0.1939907670021057, 'learning_rate': 1.1189192912416934e-08, 'epoch': 2.95, 'num_input_tokens_seen': 2889360}\n",
            "100% 105/105 [20:52<00:00, 11.87s/it][INFO|trainer.py:2329] 2024-07-14 11:24:17,764 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1252.9602, 'train_samples_per_second': 4.094, 'train_steps_per_second': 0.084, 'train_loss': 1.1973726181756883, 'epoch': 2.95, 'num_input_tokens_seen': 2889360}\n",
            "100% 105/105 [20:52<00:00, 11.93s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 11:24:17,771 >> Saving model checkpoint to /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 11:24:17,955 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 11:24:17,956 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 11:24:18,152 >> tokenizer config file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 11:24:18,171 >> Special tokens file saved in /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9474\n",
            "  num_input_tokens_seen    =    2889360\n",
            "  total_flos               = 60306269GF\n",
            "  train_loss               =     1.1974\n",
            "  train_runtime            = 0:20:52.96\n",
            "  train_samples_per_second =      4.094\n",
            "  train_steps_per_second   =      0.084\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 11:24:18,669 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 11:24:18,669 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 11:24:18,669 >>   Batch size = 2\n",
            "100% 95/95 [00:21<00:00,  4.51it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9474\n",
            "  eval_accuracy           =     0.8083\n",
            "  eval_loss               =      0.962\n",
            "  eval_runtime            = 0:00:21.27\n",
            "  eval_samples_per_second =      8.932\n",
            "  eval_steps_per_second   =      4.466\n",
            "  num_input_tokens_seen   =    2889360\n",
            "[INFO|modelcard.py:450] 2024-07-14 11:24:40,038 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8083043157769453}]}\n"
          ]
        }
      ],
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 1000 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qlora_max_samples/train_max_samples1000 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnMkwBSKseSy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Akb-pTrSGfd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fOZBOdBbGfgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mLIj-GZUGfi2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "0uWU_msMDraQ",
        "bcu9FnPsD0rY",
        "mDGYy0nMD43r",
        "QmJL6iH7EECc",
        "38vqgBGiEMRb",
        "y1STRTswESZY",
        "7IGR_nEUIIaA",
        "0bAd0vQiIMn5",
        "zjFWkFPrIRVj",
        "NFLrduEOIYZi",
        "R4ENU1A6I0wY"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}