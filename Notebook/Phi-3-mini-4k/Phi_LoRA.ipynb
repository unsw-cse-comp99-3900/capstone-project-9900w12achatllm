{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "VrVvev-0hNs0",
        "outputId": "b72641a9-5ccb-4b2c-bb76-654c07ff9439"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-91874b305a32>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "3aehO6qPhaQt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc6a8310-93d3-4868-9515-a7fecf6c55c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15796, done.\u001b[K\n",
            "remote: Counting objects: 100% (6869/6869), done.\u001b[K\n",
            "remote: Compressing objects: 100% (491/491), done.\u001b[K\n",
            "remote: Total 15796 (delta 6501), reused 6421 (delta 6378), pack-reused 8927\u001b[K\n",
            "Receiving objects: 100% (15796/15796), 222.25 MiB | 11.63 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --quantization_bit 8 \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset PubMedQA_train,MedQA_train_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 1e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qLoRA_Rank/phi3/train_qLoRA_NF8_Rank16 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 2"
      ],
      "metadata": {
        "id": "bR3K2vVUsKEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 16 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8w60ecYpSos",
        "outputId": "cd72c56a-1607-4f47-b9dd-71ed3d0fa1ce",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-21 09:45:55.767790: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-21 09:45:55.767838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-21 09:45:55.769182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-21 09:45:55.776056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-21 09:45:56.984134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/21/2024 09:46:03 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 09:46:03,587 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 09:46:03,588 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 09:46:03,588 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 09:46:03,588 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 09:46:03,588 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-21 09:46:03,655 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/21/2024 09:46:03 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/21/2024 09:46:03 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/21/2024 09:46:03 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 13735.38 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2119.24 examples/s]\n",
            "07/21/2024 09:46:07 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:00, 1718.01 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2276.42 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1011.02 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 5.52MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 09:46:12,827 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 47.1MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 09:46:13,421 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 09:46:13,422 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 20.8MB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 63.6MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-21 09:46:14,727 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 10.5M/4.97G [00:00<01:17, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:00<01:08, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 31.5M/4.97G [00:00<01:06, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<01:05, 75.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:00<01:09, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 62.9M/4.97G [00:00<01:12, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/4.97G [00:01<01:09, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:01<01:10, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 94.4M/4.97G [00:01<01:07, 71.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:01<01:07, 71.6MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:01<01:07, 72.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 126M/4.97G [00:01<01:11, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 136M/4.97G [00:01<01:08, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:02<01:10, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 157M/4.97G [00:02<01:07, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:02<01:13, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 178M/4.97G [00:02<01:09, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:02<01:06, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 199M/4.97G [00:02<01:07, 70.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:02<01:06, 72.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:03<01:08, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:03<01:14, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 241M/4.97G [00:03<01:35, 49.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 252M/4.97G [00:03<01:26, 54.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:03<01:22, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:04<01:58, 39.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:04<01:52, 41.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 294M/4.97G [00:04<01:37, 48.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 304M/4.97G [00:04<01:29, 52.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:05<01:29, 52.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:05<01:26, 53.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 336M/4.97G [00:05<01:26, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:05<01:21, 56.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 357M/4.97G [00:05<01:17, 59.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:06<01:16, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:06<01:14, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:06<01:13, 62.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 398M/4.97G [00:06<01:08, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 409M/4.97G [00:06<01:07, 67.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:06<01:07, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:06<01:09, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 440M/4.97G [00:07<01:17, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:07<01:19, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 461M/4.97G [00:07<01:16, 59.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 472M/4.97G [00:07<01:11, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:07<01:10, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 493M/4.97G [00:07<01:07, 66.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 503M/4.97G [00:08<01:06, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:08<01:07, 66.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 524M/4.97G [00:08<01:07, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:08<01:06, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:08<01:07, 65.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 556M/4.97G [00:08<01:08, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 566M/4.97G [00:09<01:06, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:09<01:05, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:09<01:08, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 598M/4.97G [00:09<01:07, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:09<01:09, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 619M/4.97G [00:09<01:08, 63.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:10<01:06, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:10<01:13, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 650M/4.97G [00:10<01:09, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:10<01:06, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 671M/4.97G [00:10<01:03, 67.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:10<01:03, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:11<01:01, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 703M/4.97G [00:11<01:01, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:11<01:03, 66.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 724M/4.97G [00:11<01:05, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 734M/4.97G [00:11<01:19, 53.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:11<01:21, 51.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 755M/4.97G [00:12<01:45, 40.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 765M/4.97G [00:12<01:52, 37.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:12<01:35, 44.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 786M/4.97G [00:13<01:25, 48.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:13<01:18, 53.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:13<01:17, 53.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 818M/4.97G [00:13<01:15, 55.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 828M/4.97G [00:13<01:17, 53.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:13<01:20, 51.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:14<01:12, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 860M/4.97G [00:14<01:12, 56.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:14<01:07, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 881M/4.97G [00:14<01:29, 45.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 891M/4.97G [00:14<01:22, 49.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:15<01:15, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 912M/4.97G [00:15<01:14, 54.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 923M/4.97G [00:15<01:09, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:15<01:06, 60.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 944M/4.97G [00:15<01:04, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:15<01:02, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:16<00:59, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 975M/4.97G [00:16<00:57, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 986M/4.97G [00:16<00:58, 68.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:16<00:56, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:16<00:56, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.02G/4.97G [00:16<00:57, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:16<00:56, 70.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.04G/4.97G [00:17<00:54, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.05G/4.97G [00:17<00:53, 73.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:17<00:53, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.07G/4.97G [00:17<00:55, 69.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.08G/4.97G [00:17<00:56, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:17<00:56, 68.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.10G/4.97G [00:17<00:54, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:18<00:55, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:18<00:52, 73.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.13G/4.97G [00:18<00:51, 74.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:18<00:51, 74.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.15G/4.97G [00:18<00:51, 74.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:18<00:50, 75.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:18<00:51, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.18G/4.97G [00:19<00:57, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.20G/4.97G [00:19<00:58, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:19<00:58, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:19<00:57, 65.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:19<00:56, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.24G/4.97G [00:19<00:56, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:20<00:57, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [00:20<01:00, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:20<00:59, 62.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:20<00:59, 62.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.29G/4.97G [00:20<00:56, 64.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.30G/4.97G [00:20<00:56, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:21<00:57, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:21<00:57, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:21<00:54, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.34G/4.97G [00:21<00:54, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.35G/4.97G [00:21<00:54, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:21<00:53, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:22<00:53, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.38G/4.97G [00:22<00:52, 68.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:22<00:50, 71.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.41G/4.97G [00:22<00:48, 73.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.42G/4.97G [00:22<00:47, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:22<00:46, 76.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [00:22<00:45, 77.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.45G/4.97G [00:23<00:47, 74.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:23<00:49, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.47G/4.97G [00:23<00:50, 69.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:23<00:50, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:23<00:54, 63.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [00:23<00:53, 64.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.51G/4.97G [00:24<00:52, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:24<00:58, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:24<00:54, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.54G/4.97G [00:24<00:50, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:24<00:49, 69.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.56G/4.97G [00:24<00:51, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.57G/4.97G [00:24<00:50, 67.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:25<00:51, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.59G/4.97G [00:25<00:50, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.60G/4.97G [00:25<00:49, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:25<00:50, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.63G/4.97G [00:25<00:53, 62.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:25<00:54, 61.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:26<00:54, 61.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [00:26<00:54, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.67G/4.97G [00:26<00:50, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:26<00:49, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:26<00:48, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.70G/4.97G [00:26<00:49, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:27<00:51, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [00:27<00:58, 55.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:27<00:59, 54.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:27<00:56, 57.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:27<00:54, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [00:28<00:55, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:28<00:50, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.78G/4.97G [00:28<00:49, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:28<00:48, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.80G/4.97G [00:28<01:01, 51.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [00:28<00:54, 58.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.82G/4.97G [00:29<00:50, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:29<00:48, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:29<00:46, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:29<00:45, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.87G/4.97G [00:29<00:45, 68.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:29<00:45, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.89G/4.97G [00:29<00:46, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:30<00:45, 68.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [00:30<00:42, 71.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:30<00:41, 74.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.93G/4.97G [00:30<00:40, 75.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [00:30<00:39, 75.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:30<00:40, 74.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:30<00:40, 73.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [00:31<00:42, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:31<00:45, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.99G/4.97G [00:31<00:44, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [00:31<00:46, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:31<00:44, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.02G/4.97G [00:31<00:45, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:32<00:43, 67.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.04G/4.97G [00:32<00:44, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:32<00:45, 64.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [00:32<00:45, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [00:32<00:43, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:32<00:43, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [00:33<00:43, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.11G/4.97G [00:33<00:43, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:33<00:42, 67.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.13G/4.97G [00:33<00:44, 63.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.14G/4.97G [00:33<00:49, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:33<00:47, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.16G/4.97G [00:34<00:46, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:34<00:48, 57.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:34<00:44, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.19G/4.97G [00:34<00:45, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.20G/4.97G [00:34<00:44, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:34<00:46, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:35<00:48, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [00:35<00:46, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:35<00:43, 63.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.25G/4.97G [00:35<00:41, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.26G/4.97G [00:35<00:43, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:35<00:42, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.29G/4.97G [00:36<00:43, 61.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.30G/4.97G [00:36<00:43, 61.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:36<00:41, 64.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.32G/4.97G [00:36<00:39, 66.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [00:36<00:40, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:36<00:43, 61.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [00:37<00:49, 53.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [00:37<00:45, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [00:37<00:44, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:37<00:44, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.39G/4.97G [00:37<00:41, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.40G/4.97G [00:38<00:40, 64.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [00:38<00:39, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.42G/4.97G [00:38<00:40, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:38<00:41, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.44G/4.97G [00:38<00:40, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:38<00:39, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.46G/4.97G [00:38<00:36, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:39<00:35, 71.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [00:39<00:37, 66.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.50G/4.97G [00:39<00:37, 66.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:39<00:36, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.52G/4.97G [00:39<00:35, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.53G/4.97G [00:39<00:34, 71.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:40<00:33, 72.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.55G/4.97G [00:40<00:35, 68.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.56G/4.97G [00:40<00:36, 66.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [00:40<00:35, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [00:40<00:33, 70.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:40<00:33, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.60G/4.97G [00:40<00:33, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:41<00:34, 68.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.62G/4.97G [00:41<00:36, 63.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [00:41<00:39, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:41<00:40, 57.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:41<00:39, 59.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.66G/4.97G [00:42<00:37, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:42<00:35, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.68G/4.97G [00:42<00:36, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:42<00:36, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.71G/4.97G [00:42<00:36, 62.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [00:42<00:34, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:42<00:33, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:43<00:36, 61.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.75G/4.97G [00:43<00:36, 60.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [00:43<00:38, 57.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.77G/4.97G [00:43<00:37, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.78G/4.97G [00:43<00:36, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:44<00:37, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.80G/4.97G [00:44<00:36, 59.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.81G/4.97G [00:44<00:35, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:44<00:46, 45.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [00:45<00:39, 54.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:45<00:39, 54.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.86G/4.97G [00:45<00:36, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:45<00:34, 61.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:45<00:33, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.89G/4.97G [00:45<00:33, 62.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.90G/4.97G [00:46<00:32, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:46<00:34, 60.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.93G/4.97G [00:46<00:35, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:46<00:34, 59.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [00:46<00:32, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.96G/4.97G [00:46<00:31, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [00:47<00:30, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:47<00:37, 52.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.99G/4.97G [00:47<00:47, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 3.00G/4.97G [00:48<00:48, 40.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:48<00:50, 39.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:48<00:52, 37.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [00:48<00:55, 35.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:49<00:54, 35.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.05G/4.97G [00:49<00:48, 39.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [00:49<00:41, 45.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:49<00:37, 50.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.08G/4.97G [00:49<00:34, 54.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.09G/4.97G [00:50<00:31, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:50<00:33, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [00:50<00:31, 58.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.12G/4.97G [00:50<00:29, 62.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:50<00:29, 62.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.15G/4.97G [00:50<00:31, 58.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:51<00:30, 60.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:51<00:29, 61.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.18G/4.97G [00:51<00:27, 64.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.19G/4.97G [00:51<00:27, 65.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:51<00:27, 63.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:51<00:26, 65.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.22G/4.97G [00:52<00:28, 61.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:52<00:27, 62.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.24G/4.97G [00:52<00:28, 61.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.25G/4.97G [00:52<00:28, 60.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:52<00:27, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [00:52<00:27, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.28G/4.97G [00:53<00:31, 53.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:53<00:29, 57.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.30G/4.97G [00:53<00:27, 60.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:53<00:26, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:53<00:26, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.33G/4.97G [00:54<00:29, 56.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [00:54<00:30, 53.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [00:54<00:28, 56.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:54<00:26, 60.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.38G/4.97G [00:54<00:25, 62.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:54<00:26, 60.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.40G/4.97G [00:55<00:25, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.41G/4.97G [00:55<00:24, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:55<00:23, 66.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.43G/4.97G [00:55<00:22, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.44G/4.97G [00:55<00:22, 68.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [00:55<00:22, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.46G/4.97G [00:55<00:22, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [00:56<00:23, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [00:56<00:24, 61.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.49G/4.97G [00:56<00:24, 61.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.50G/4.97G [00:56<00:24, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [00:56<00:23, 61.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:56<00:23, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.53G/4.97G [00:57<00:21, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:57<00:21, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.55G/4.97G [00:57<00:20, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [00:57<00:20, 69.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:57<00:20, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.59G/4.97G [00:57<00:20, 67.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:58<00:22, 61.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [00:58<00:22, 60.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [00:58<00:24, 56.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:58<00:22, 59.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.64G/4.97G [00:58<00:28, 47.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.66G/4.97G [00:59<00:19, 67.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [00:59<00:18, 69.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:59<00:19, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [00:59<00:18, 68.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.70G/4.97G [00:59<00:18, 69.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:59<00:18, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.72G/4.97G [01:00<00:19, 65.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [01:00<00:19, 63.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.74G/4.97G [01:00<00:18, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [01:00<00:18, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [01:00<00:17, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.77G/4.97G [01:00<00:17, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [01:00<00:17, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [01:01<00:17, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [01:01<00:17, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.82G/4.97G [01:01<00:17, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [01:01<00:19, 58.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [01:02<00:23, 47.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [01:02<00:21, 53.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.86G/4.97G [01:02<00:19, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [01:02<00:19, 57.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.88G/4.97G [01:02<00:18, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [01:02<00:17, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.90G/4.97G [01:02<00:16, 64.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [01:03<00:15, 67.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.92G/4.97G [01:03<00:16, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [01:03<00:15, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [01:03<00:15, 67.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [01:03<00:14, 70.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.96G/4.97G [01:03<00:14, 72.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [01:03<00:14, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.98G/4.97G [01:04<00:13, 71.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [01:04<00:14, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.01G/4.97G [01:04<00:14, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [01:04<00:14, 66.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.03G/4.97G [01:04<00:14, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [01:04<00:14, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.05G/4.97G [01:05<00:13, 68.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [01:05<00:13, 66.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [01:05<00:13, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [01:05<00:13, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.09G/4.97G [01:05<00:13, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [01:05<00:13, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.11G/4.97G [01:06<00:13, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [01:06<00:13, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [01:06<00:13, 63.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [01:06<00:12, 65.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.15G/4.97G [01:06<00:13, 62.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [01:06<00:12, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.17G/4.97G [01:07<00:12, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [01:07<00:11, 66.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [01:07<00:12, 61.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.20G/4.97G [01:07<00:12, 63.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.22G/4.97G [01:07<00:12, 62.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [01:07<00:11, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [01:08<00:12, 59.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [01:08<00:11, 61.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [01:08<00:11, 62.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [01:08<00:10, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [01:08<00:10, 64.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.29G/4.97G [01:08<00:10, 66.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.30G/4.97G [01:09<00:10, 66.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [01:09<00:09, 68.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [01:09<00:09, 68.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [01:09<00:09, 64.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.34G/4.97G [01:09<00:09, 67.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [01:09<00:09, 66.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [01:09<00:08, 69.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.37G/4.97G [01:10<00:08, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [01:10<00:08, 69.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.39G/4.97G [01:10<00:08, 69.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [01:10<00:07, 71.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [01:10<00:08, 65.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.42G/4.97G [01:11<00:10, 50.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [01:11<00:10, 53.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [01:11<00:09, 56.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.46G/4.97G [01:11<00:08, 60.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [01:11<00:08, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.48G/4.97G [01:11<00:08, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [01:12<00:08, 55.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [01:12<00:08, 58.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [01:12<00:07, 62.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.52G/4.97G [01:12<00:07, 62.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.53G/4.97G [01:12<00:09, 48.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [01:13<00:06, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.56G/4.97G [01:13<00:06, 66.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [01:13<00:06, 65.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.58G/4.97G [01:13<00:05, 67.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [01:13<00:05, 65.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.60G/4.97G [01:13<00:05, 62.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [01:14<00:05, 62.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.62G/4.97G [01:14<00:05, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.63G/4.97G [01:14<00:05, 60.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [01:14<00:05, 60.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [01:14<00:05, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.67G/4.97G [01:14<00:04, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [01:15<00:04, 65.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.69G/4.97G [01:15<00:04, 66.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [01:15<00:04, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [01:15<00:04, 60.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.72G/4.97G [01:15<00:04, 63.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [01:15<00:03, 64.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [01:16<00:03, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [01:16<00:03, 64.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.76G/4.97G [01:16<00:03, 64.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [01:16<00:03, 63.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.78G/4.97G [01:16<00:02, 67.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.79G/4.97G [01:16<00:02, 67.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [01:16<00:02, 69.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.81G/4.97G [01:17<00:02, 70.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.82G/4.97G [01:17<00:02, 71.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [01:17<00:01, 71.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [01:17<00:01, 71.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [01:17<00:01, 72.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [01:17<00:01, 71.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.88G/4.97G [01:17<00:01, 70.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.89G/4.97G [01:18<00:01, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [01:18<00:01, 66.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [01:18<00:00, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.92G/4.97G [01:18<00:00, 65.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.97G [01:18<00:00, 68.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.94G/4.97G [01:18<00:00, 69.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.95G/4.97G [01:19<00:00, 65.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.96G/4.97G [01:19<00:00, 64.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [01:19<00:00, 62.6MB/s]\n",
            "Downloading shards:  50% 1/2 [01:19<01:19, 79.92s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 10.5M/2.67G [00:01<05:46, 7.68MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:01<03:12, 13.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 31.5M/2.67G [00:02<02:41, 16.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:02<02:11, 19.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 52.4M/2.67G [00:02<01:54, 22.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:03<01:44, 25.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 73.4M/2.67G [00:03<01:38, 26.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:04<01:44, 24.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:04<01:37, 26.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 105M/2.67G [00:04<01:33, 27.5MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 115M/2.67G [00:05<01:29, 28.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:05<01:27, 29.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:05<01:25, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 147M/2.67G [00:06<01:24, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:06<01:23, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 168M/2.67G [00:06<01:22, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:07<01:21, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:07<01:21, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:07<01:20, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 210M/2.67G [00:08<01:20, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 220M/2.67G [00:08<01:19, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:08<01:19, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:09<01:19, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 252M/2.67G [00:09<01:18, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:10<01:27, 27.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 273M/2.67G [00:10<01:24, 28.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:10<01:22, 29.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:11<01:20, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:11<01:19, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 315M/2.67G [00:11<01:18, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:12<01:17, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 336M/2.67G [00:12<01:16, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:12<01:16, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 357M/2.67G [00:13<01:15, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:13<01:15, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 377M/2.67G [00:13<01:14, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 388M/2.67G [00:14<01:14, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:14<01:13, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 409M/2.67G [00:14<01:13, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:15<01:13, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 430M/2.67G [00:15<01:12, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:15<01:12, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:16<01:12, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:16<01:11, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 472M/2.67G [00:16<01:11, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:17<01:11, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 493M/2.67G [00:17<01:11, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:17<01:10, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 514M/2.67G [00:18<01:17, 27.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:18<01:15, 28.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 535M/2.67G [00:19<01:13, 29.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:19<01:12, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:19<01:10, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:20<01:09, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 577M/2.67G [00:20<01:09, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:20<01:08, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 598M/2.67G [00:21<01:07, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:21<01:08, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:21<01:06, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 629M/2.67G [00:22<01:06, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 640M/2.67G [00:22<01:06, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:22<01:05, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:23<01:05, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:23<01:04, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 682M/2.67G [00:23<01:05, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:24<01:04, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 703M/2.67G [00:24<01:04, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:24<01:10, 27.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 724M/2.67G [00:25<01:08, 28.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:25<01:06, 29.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 744M/2.67G [00:25<01:05, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:26<01:04, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:26<01:03, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:27<01:02, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 786M/2.67G [00:27<01:01, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:27<01:01, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 807M/2.67G [00:28<01:01, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:28<01:00, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:28<01:01, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 839M/2.67G [00:29<00:59, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 849M/2.67G [00:29<00:59, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:29<00:59, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:30<00:58, 30.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:30<00:58, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 891M/2.67G [00:30<00:57, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:31<00:57, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 912M/2.67G [00:31<00:57, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:31<00:57, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 933M/2.67G [00:32<00:56, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:32<00:56, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 954M/2.67G [00:32<00:56, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:33<00:55, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:33<00:55, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:33<00:55, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 996M/2.67G [00:34<00:59, 28.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.01G/2.67G [00:34<00:57, 28.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:34<00:56, 29.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:35<00:55, 29.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.04G/2.67G [00:35<00:54, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:36<00:54, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.06G/2.67G [00:36<00:52, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.07G/2.67G [00:36<00:52, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:37<00:52, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:37<00:51, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.10G/2.67G [00:37<00:51, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:38<00:51, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.12G/2.67G [00:38<00:50, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:38<00:50, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.14G/2.67G [00:39<00:50, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:39<00:49, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.16G/2.67G [00:39<00:49, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:40<00:48, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:40<00:48, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:40<00:48, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.21G/2.67G [00:41<00:47, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.22G/2.67G [00:41<00:47, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:41<00:47, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:42<00:46, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.25G/2.67G [00:42<00:46, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:42<00:46, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:43<00:45, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.28G/2.67G [00:43<00:45, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:43<00:46, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.30G/2.67G [00:44<00:47, 29.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.31G/2.67G [00:44<00:46, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:44<00:45, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.33G/2.67G [00:45<00:44, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:45<00:44, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:46<00:43, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.36G/2.67G [00:46<00:43, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.37G/2.67G [00:46<00:42, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:47<00:42, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:47<00:41, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.41G/2.67G [00:47<00:41, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:48<00:41, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.43G/2.67G [00:48<00:40, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [00:48<00:40, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:49<00:39, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.46G/2.67G [00:49<00:39, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.47G/2.67G [00:49<00:40, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:50<00:38, 30.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.49G/2.67G [00:50<00:38, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:50<00:38, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:51<00:37, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [00:51<00:37, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.53G/2.67G [00:51<00:37, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:52<00:37, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:52<00:36, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.56G/2.67G [00:52<00:36, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:53<00:35, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.58G/2.67G [00:53<00:35, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.59G/2.67G [00:53<00:36, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:54<00:36, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [00:54<00:36, 29.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.63G/2.67G [00:55<00:35, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:55<00:34, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.65G/2.67G [00:55<00:33, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:56<00:33, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:56<00:33, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [00:56<00:32, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.69G/2.67G [00:57<00:32, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:57<00:31, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:57<00:31, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.72G/2.67G [00:58<00:31, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:58<00:30, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [00:58<00:30, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:59<00:29, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:59<00:29, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:59<00:29, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.78G/2.67G [01:00<00:46, 19.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [01:01<00:39, 22.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [01:01<00:35, 24.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [01:01<00:32, 25.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.82G/2.67G [01:02<00:31, 27.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [01:02<00:30, 27.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.85G/2.67G [01:02<00:28, 28.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.86G/2.67G [01:03<00:27, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [01:03<00:27, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.88G/2.67G [01:03<00:26, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.89G/2.67G [01:04<00:27, 28.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [01:04<00:26, 29.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.91G/2.67G [01:04<00:25, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [01:05<00:25, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [01:05<00:24, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.94G/2.67G [01:05<00:24, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.95G/2.67G [01:06<00:23, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [01:06<00:23, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [01:07<00:22, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.98G/2.67G [01:07<00:22, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [01:07<00:22, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.00G/2.67G [01:08<00:22, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 2.01G/2.67G [01:08<00:21, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [01:08<00:21, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.03G/2.67G [01:09<00:20, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.04G/2.67G [01:09<00:20, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [01:09<00:20, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.07G/2.67G [01:10<00:20, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [01:10<00:20, 29.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [01:10<00:20, 28.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.10G/2.67G [01:11<00:19, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.11G/2.67G [01:11<00:18, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [01:11<00:18, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [01:12<00:17, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [01:12<00:17, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.15G/2.67G [01:12<00:17, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [01:13<00:16, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.17G/2.67G [01:13<00:16, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [01:13<00:15, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [01:14<00:17, 27.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [01:14<00:16, 28.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.21G/2.67G [01:15<00:15, 28.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [01:15<00:15, 28.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [01:15<00:15, 27.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [01:16<00:15, 27.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.25G/2.67G [01:16<00:14, 28.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [01:17<00:14, 28.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.28G/2.67G [01:17<00:13, 28.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [01:17<00:13, 29.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [01:18<00:12, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.31G/2.67G [01:18<00:12, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [01:18<00:11, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.33G/2.67G [01:19<00:11, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [01:19<00:11, 29.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [01:19<00:10, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.36G/2.67G [01:20<00:10, 29.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.37G/2.67G [01:20<00:10, 29.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [01:20<00:09, 30.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [01:21<00:09, 30.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.40G/2.67G [01:21<00:08, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [01:21<00:08, 30.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [01:22<00:08, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [01:22<00:07, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [01:22<00:07, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [01:23<00:07, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.46G/2.67G [01:23<00:06, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [01:23<00:06, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.49G/2.67G [01:24<00:06, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [01:24<00:05, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.51G/2.67G [01:24<00:05, 30.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [01:25<00:05, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.53G/2.67G [01:25<00:04, 29.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [01:26<00:04, 29.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [01:26<00:04, 29.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.56G/2.67G [01:26<00:03, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [01:27<00:03, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.58G/2.67G [01:27<00:02, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [01:27<00:02, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [01:28<00:02, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [01:28<00:01, 30.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.62G/2.67G [01:28<00:01, 30.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [01:29<00:01, 30.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.64G/2.67G [01:29<00:00, 30.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.65G/2.67G [01:29<00:00, 30.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.66G/2.67G [01:30<00:00, 29.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [01:30<00:00, 29.5MB/s]\n",
            "Downloading shards: 100% 2/2 [02:50<00:00, 85.36s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-21 09:49:05,438 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 09:49:05,439 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.56s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-21 09:49:10,643 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-21 09:49:10,643 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.25MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-21 09:49:11,052 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 09:49:11,052 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,qkv_proj,o_proj,gate_up_proj\n",
            "07/21/2024 09:49:11 - INFO - llamafactory.model.loader - trainable params: 25,165,824 || all params: 3,846,245,376 || trainable%: 0.6543\n",
            "[INFO|trainer.py:642] 2024-07-21 09:49:11,498 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-21 09:49:11,960 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-21 09:49:11,960 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-21 09:49:11,960 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:2131] 2024-07-21 09:49:11,960 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-21 09:49:11,960 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-21 09:49:11,960 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-21 09:49:11,960 >>   Total optimization steps = 90\n",
            "[INFO|trainer.py:2137] 2024-07-21 09:49:11,963 >>   Number of trainable parameters = 25,165,824\n",
            "  0% 0/90 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.686, 'grad_norm': 0.4680123031139374, 'learning_rate': 4.962019382530521e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.4651, 'grad_norm': 0.3944858908653259, 'learning_rate': 4.849231551964771e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 11% 10/90 [07:58<1:06:02, 49.54s/it][INFO|trainer.py:3788] 2024-07-21 09:57:10,442 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 09:57:10,442 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 09:57:10,442 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.34it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.02it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.12s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.02it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.20s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:21,  1.18s/it]\u001b[A\n",
            " 32% 8/25 [00:09<00:20,  1.23s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.24s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.30s/it]\u001b[A\n",
            " 44% 11/25 [00:13<00:18,  1.31s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.30s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n",
            " 60% 15/25 [00:18<00:12,  1.23s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.28s/it]\u001b[A\n",
            " 72% 18/25 [00:22<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.17s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.21s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:27<00:03,  1.29s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.25s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.32s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.32550847530365, 'eval_runtime': 32.6138, 'eval_samples_per_second': 3.066, 'eval_steps_per_second': 0.767, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 11% 10/90 [08:31<1:06:02, 49.54s/it]\n",
            "100% 25/25 [00:31<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 1.2359, 'grad_norm': 0.28750118613243103, 'learning_rate': 4.665063509461097e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.2735, 'grad_norm': 0.17972883582115173, 'learning_rate': 4.415111107797445e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 22% 20/90 [16:52<59:23, 50.90s/it][INFO|trainer.py:3788] 2024-07-21 10:06:04,107 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:06:04,108 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:06:04,108 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.03it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.03it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.19s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:20,  1.16s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.22s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.23s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.28s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.30s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.29s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.23s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.27s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.18s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.22s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.29s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.25s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.32s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1724551916122437, 'eval_runtime': 32.4841, 'eval_samples_per_second': 3.078, 'eval_steps_per_second': 0.77, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 22% 20/90 [17:24<59:23, 50.90s/it]\n",
            "100% 25/25 [00:30<00:00,  1.37s/it]\u001b[A\n",
            "{'loss': 1.2489, 'grad_norm': 0.2177177369594574, 'learning_rate': 4.1069690242163484e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.1288, 'grad_norm': 0.18796928226947784, 'learning_rate': 3.7500000000000003e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 33% 30/90 [25:53<51:05, 51.10s/it][INFO|trainer.py:3788] 2024-07-21 10:15:05,345 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:15:05,346 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:15:05,346 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:16,  1.38it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.04it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:22,  1.10s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.05it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.17s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:20,  1.15s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.20s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.21s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:18,  1.26s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:17,  1.28s/it]\u001b[A\n",
            " 48% 12/25 [00:13<00:16,  1.27s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.27s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.28s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.20s/it]\u001b[A\n",
            " 64% 16/25 [00:18<00:11,  1.24s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.25s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:08,  1.27s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:06,  1.15s/it]\u001b[A\n",
            " 80% 20/25 [00:23<00:05,  1.19s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.33s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.27s/it]\u001b[A\n",
            " 92% 23/25 [00:27<00:02,  1.23s/it]\u001b[A\n",
            " 96% 24/25 [00:28<00:01,  1.29s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1330538988113403, 'eval_runtime': 31.9062, 'eval_samples_per_second': 3.134, 'eval_steps_per_second': 0.784, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 33% 30/90 [26:25<51:05, 51.10s/it]\n",
            "100% 25/25 [00:30<00:00,  1.34s/it]\u001b[A\n",
            "{'loss': 1.1292, 'grad_norm': 0.1771353781223297, 'learning_rate': 3.355050358314172e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.2409, 'grad_norm': 0.19067160785198212, 'learning_rate': 2.9341204441673266e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 44% 40/90 [34:41<41:14, 49.50s/it][INFO|trainer.py:3788] 2024-07-21 10:23:53,250 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:23:53,250 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:23:53,250 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.03it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.03it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.19s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:21,  1.17s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.22s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.24s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.29s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.30s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.29s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.30s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.23s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.27s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.18s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.22s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.30s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.25s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.32s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.117126226425171, 'eval_runtime': 32.5262, 'eval_samples_per_second': 3.074, 'eval_steps_per_second': 0.769, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 44% 40/90 [35:13<41:14, 49.50s/it]\n",
            "100% 25/25 [00:31<00:00,  1.37s/it]\u001b[A\n",
            "{'loss': 1.1508, 'grad_norm': 0.16371974349021912, 'learning_rate': 2.5e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 1.1603, 'grad_norm': 0.14723652601242065, 'learning_rate': 2.0658795558326743e-05, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 56% 50/90 [43:46<35:11, 52.80s/it][INFO|trainer.py:3788] 2024-07-21 10:32:58,344 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:32:58,344 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:32:58,344 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.02it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.03it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.19s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:20,  1.17s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.22s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.24s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.29s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.30s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.29s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.23s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.27s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.17s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.22s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.29s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.25s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.32s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1112174987792969, 'eval_runtime': 32.4917, 'eval_samples_per_second': 3.078, 'eval_steps_per_second': 0.769, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 56% 50/90 [44:18<35:11, 52.80s/it]\n",
            "100% 25/25 [00:31<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 1.1178, 'grad_norm': 0.15496285259723663, 'learning_rate': 1.6449496416858284e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1502976}\n",
            "{'loss': 1.1142, 'grad_norm': 0.15833614766597748, 'learning_rate': 1.2500000000000006e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1640112}\n",
            " 67% 60/90 [52:38<24:44, 49.47s/it][INFO|trainer.py:3788] 2024-07-21 10:41:50,557 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:41:50,557 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:41:50,557 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.02it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.03it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.20s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:21,  1.17s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.22s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.24s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.29s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.30s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.30s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.23s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.27s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.18s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.22s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.30s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.26s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.32s/it]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1083428859710693, 'eval_runtime': 32.5718, 'eval_samples_per_second': 3.07, 'eval_steps_per_second': 0.768, 'epoch': 3.2, 'num_input_tokens_seen': 1640112}\n",
            " 67% 60/90 [53:11<24:44, 49.47s/it]\n",
            "100% 25/25 [00:31<00:00,  1.37s/it]\u001b[A\n",
            "{'loss': 1.1782, 'grad_norm': 0.13881336152553558, 'learning_rate': 8.930309757836517e-06, 'epoch': 3.47, 'num_input_tokens_seen': 1777824}\n",
            "{'loss': 1.1559, 'grad_norm': 0.14840523898601532, 'learning_rate': 5.848888922025553e-06, 'epoch': 3.73, 'num_input_tokens_seen': 1913328}\n",
            " 78% 70/90 [1:01:31<16:42, 50.15s/it][INFO|trainer.py:3788] 2024-07-21 10:50:43,064 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 10:50:43,065 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 10:50:43,065 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:16,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.03it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.04it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.19s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:20,  1.16s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.21s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.23s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.28s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.29s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.29s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.28s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.29s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.22s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.26s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.27s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.30s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.17s/it]\u001b[A\n",
            " 80% 20/25 [00:23<00:06,  1.21s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.35s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.29s/it]\u001b[A\n",
            " 92% 23/25 [00:27<00:02,  1.25s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.31s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1065882444381714, 'eval_runtime': 32.35, 'eval_samples_per_second': 3.091, 'eval_steps_per_second': 0.773, 'epoch': 3.73, 'num_input_tokens_seen': 1913328}\n",
            " 78% 70/90 [1:02:03<16:42, 50.15s/it]\n",
            "100% 25/25 [00:30<00:00,  1.36s/it]\u001b[A\n",
            "{'loss': 1.206, 'grad_norm': 0.16619716584682465, 'learning_rate': 3.3493649053890326e-06, 'epoch': 4.0, 'num_input_tokens_seen': 2059008}\n",
            "{'loss': 1.1239, 'grad_norm': 0.14411623775959015, 'learning_rate': 1.5076844803522922e-06, 'epoch': 4.27, 'num_input_tokens_seen': 2201712}\n",
            " 89% 80/90 [1:10:52<09:02, 54.24s/it][INFO|trainer.py:3788] 2024-07-21 11:00:04,352 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 11:00:04,352 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 11:00:04,352 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:17,  1.35it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:21,  1.02it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:23,  1.11s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.03it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.19s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:21,  1.17s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.22s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.24s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:19,  1.29s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:18,  1.30s/it]\u001b[A\n",
            " 48% 12/25 [00:14<00:16,  1.30s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.30s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.31s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.24s/it]\u001b[A\n",
            " 64% 16/25 [00:19<00:11,  1.27s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.28s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:09,  1.32s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:07,  1.18s/it]\u001b[A\n",
            " 80% 20/25 [00:24<00:06,  1.23s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.36s/it]\u001b[A\n",
            " 88% 22/25 [00:27<00:03,  1.30s/it]\u001b[A\n",
            " 92% 23/25 [00:28<00:02,  1.26s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.33s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1063615083694458, 'eval_runtime': 32.6604, 'eval_samples_per_second': 3.062, 'eval_steps_per_second': 0.765, 'epoch': 4.27, 'num_input_tokens_seen': 2201712}\n",
            " 89% 80/90 [1:11:25<09:02, 54.24s/it]\n",
            "100% 25/25 [00:31<00:00,  1.37s/it]\u001b[A\n",
            "{'loss': 1.1053, 'grad_norm': 0.15064576268196106, 'learning_rate': 3.7980617469479953e-07, 'epoch': 4.53, 'num_input_tokens_seen': 2334768}\n",
            "{'loss': 1.1049, 'grad_norm': 0.1687917560338974, 'learning_rate': 0.0, 'epoch': 4.8, 'num_input_tokens_seen': 2472912}\n",
            "100% 90/90 [1:19:41<00:00, 50.72s/it][INFO|trainer.py:3788] 2024-07-21 11:08:53,251 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 11:08:53,251 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 11:08:53,251 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:01<00:16,  1.39it/s]\u001b[A\n",
            " 12% 3/25 [00:02<00:20,  1.05it/s]\u001b[A\n",
            " 16% 4/25 [00:04<00:22,  1.09s/it]\u001b[A\n",
            " 20% 5/25 [00:04<00:19,  1.05it/s]\u001b[A\n",
            " 24% 6/25 [00:06<00:22,  1.17s/it]\u001b[A\n",
            " 28% 7/25 [00:07<00:20,  1.14s/it]\u001b[A\n",
            " 32% 8/25 [00:08<00:20,  1.20s/it]\u001b[A\n",
            " 36% 9/25 [00:10<00:19,  1.21s/it]\u001b[A\n",
            " 40% 10/25 [00:11<00:18,  1.26s/it]\u001b[A\n",
            " 44% 11/25 [00:12<00:17,  1.27s/it]\u001b[A\n",
            " 48% 12/25 [00:13<00:16,  1.27s/it]\u001b[A\n",
            " 52% 13/25 [00:15<00:15,  1.27s/it]\u001b[A\n",
            " 56% 14/25 [00:16<00:14,  1.28s/it]\u001b[A\n",
            " 60% 15/25 [00:17<00:12,  1.21s/it]\u001b[A\n",
            " 64% 16/25 [00:18<00:11,  1.24s/it]\u001b[A\n",
            " 68% 17/25 [00:20<00:10,  1.26s/it]\u001b[A\n",
            " 72% 18/25 [00:21<00:08,  1.28s/it]\u001b[A\n",
            " 76% 19/25 [00:22<00:06,  1.16s/it]\u001b[A\n",
            " 80% 20/25 [00:23<00:05,  1.20s/it]\u001b[A\n",
            " 84% 21/25 [00:25<00:05,  1.33s/it]\u001b[A\n",
            " 88% 22/25 [00:26<00:03,  1.28s/it]\u001b[A\n",
            " 92% 23/25 [00:27<00:02,  1.24s/it]\u001b[A\n",
            " 96% 24/25 [00:29<00:01,  1.30s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1063123941421509, 'eval_runtime': 31.9436, 'eval_samples_per_second': 3.131, 'eval_steps_per_second': 0.783, 'epoch': 4.8, 'num_input_tokens_seen': 2472912}\n",
            "100% 90/90 [1:20:13<00:00, 50.72s/it]\n",
            "100% 25/25 [00:30<00:00,  1.35s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:3478] 2024-07-21 11:09:25,198 >> Saving model checkpoint to /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/checkpoint-90\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:09:25,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 11:09:25,671 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 11:09:26,039 >> tokenizer config file saved in /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/checkpoint-90/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 11:09:26,042 >> Special tokens file saved in /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/checkpoint-90/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-21 11:09:26,736 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4814.7727, 'train_samples_per_second': 0.935, 'train_steps_per_second': 0.019, 'train_loss': 1.2125409444173176, 'epoch': 4.8, 'num_input_tokens_seen': 2472912}\n",
            "100% 90/90 [1:20:14<00:00, 53.50s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-21 11:09:26,741 >> Saving model checkpoint to /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:09:27,166 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 11:09:27,167 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-21 11:09:27,524 >> tokenizer config file saved in /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-21 11:09:27,528 >> Special tokens file saved in /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        4.8\n",
            "  num_input_tokens_seen    =    2472912\n",
            "  total_flos               = 51788108GF\n",
            "  train_loss               =     1.2125\n",
            "  train_runtime            = 1:20:14.77\n",
            "  train_samples_per_second =      0.935\n",
            "  train_steps_per_second   =      0.019\n",
            "Figure saved at: /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16/training_eval_loss.png\n",
            "07/21/2024 11:09:27 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-21 11:09:27,978 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 11:09:27,978 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 11:09:27,978 >>   Batch size = 4\n",
            "100% 25/25 [00:30<00:00,  1.21s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        4.8\n",
            "  eval_loss               =     1.1063\n",
            "  eval_runtime            = 0:00:31.76\n",
            "  eval_samples_per_second =      3.149\n",
            "  eval_steps_per_second   =      0.787\n",
            "  num_input_tokens_seen   =    2472912\n",
            "[INFO|modelcard.py:449] 2024-07-21 11:09:59,754 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA_Rank/phi3/eval_LoRA_Rank16_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3iC6t3qpSqz",
        "outputId": "0f65a6f7-a554-4127-c641-fae6f7d01310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-21 11:22:02.653337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-21 11:22:02.653394: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-21 11:22:02.654842: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-21 11:22:02.663102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-21 11:22:03.938905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/21/2024 11:22:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:22:10,912 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:22:10,912 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:22:10,912 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:22:10,912 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:22:10,912 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-21 11:22:10,980 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/21/2024 11:22:10 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/21/2024 11:22:10 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/21/2024 11:22:10 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "Generating train split: 100 examples [00:00, 4197.91 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 381.62 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 106.24 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:22:14,430 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:22:14,822 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 11:22:14,823 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/21/2024 11:22:14 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-21 11:22:15,052 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-21 11:22:15,052 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 11:22:15,054 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.69s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-21 11:22:18,492 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-21 11:22:18,492 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-21 11:22:18,690 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 11:22:18,691 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/21/2024 11:22:18 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/21/2024 11:22:19 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/21/2024 11:22:19 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16\n",
            "07/21/2024 11:22:19 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-21 11:22:19,445 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 11:22:19,445 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 11:22:19,445 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-21 11:22:19,609 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [17:18<00:00, 17.72s/it]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.703 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [17:19<00:00, 20.80s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    28.8717\n",
            "  predict_rouge-1            =    35.1309\n",
            "  predict_rouge-2            =    13.7397\n",
            "  predict_rouge-l            =    26.7806\n",
            "  predict_runtime            = 0:17:35.54\n",
            "  predict_samples_per_second =      0.095\n",
            "  predict_steps_per_second   =      0.047\n",
            "07/21/2024 11:39:54 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/LoRA_Rank/phi3/eval_LoRA_Rank16_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16"
      ],
      "metadata": {
        "id": "tl4GY6TZugbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA_Rank/phi3/eval_LoRA_Rank16_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThRn2l1ypSs7",
        "outputId": "4bd151a8-bc27-43eb-a076-379432ed2c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-21 11:41:50.739426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-21 11:41:50.739472: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-21 11:41:50.740818: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-21 11:41:50.748132: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-21 11:41:51.938726: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/21/2024 11:41:58 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:41:58,555 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:41:58,555 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:41:58,555 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:41:58,555 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-21 11:41:58,555 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-21 11:41:58,619 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/21/2024 11:41:58 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/21/2024 11:41:58 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/21/2024 11:41:58 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 122.80 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:42:00,608 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-21 11:42:01,025 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-21 11:42:01,026 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/21/2024 11:42:01 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-21 11:42:01,252 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-21 11:42:01,253 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 11:42:01,254 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.46s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-21 11:42:04,245 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-21 11:42:04,245 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-21 11:42:04,443 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-21 11:42:04,443 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/21/2024 11:42:04 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/21/2024 11:42:05 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/21/2024 11:42:05 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/LoRA_Rank/phi3/train_LoRA_Rank16\n",
            "07/21/2024 11:42:05 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-21 11:42:05,186 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-21 11:42:05,186 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-21 11:42:05,186 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-21 11:42:05,364 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 34/34 [05:17<00:00,  7.21s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.676 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [05:17<00:00,  9.35s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =     63.797\n",
            "  predict_rouge-1            =    68.4906\n",
            "  predict_rouge-2            =    57.4162\n",
            "  predict_rouge-l            =    68.3217\n",
            "  predict_runtime            = 0:05:29.39\n",
            "  predict_samples_per_second =      0.304\n",
            "  predict_steps_per_second   =      0.103\n",
            "07/21/2024 11:47:34 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/LoRA_Rank/phi3/eval_LoRA_Rank16_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wmw_ay6YpSwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qwen2/05b/eval_MedQA_test_US \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "s4JMQcKwBPG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qwen2/15b/eval_MedQA_test_US \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "4zwLazAlBRca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-1.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qwen2/15b/eval_PubMedQA_val \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "Syk6d8wJBSV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/qwen2/05b/eval_PubMedQA_val \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "0bRBmKVoBWSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K2iJqng9Q5-",
        "outputId": "25580dac-bcbe-4e0e-c9cf-6218b9aaa606",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-19 17:35:30.956819: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-19 17:35:31.008300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-19 17:35:31.008347: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-19 17:35:31.009950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-19 17:35:31.017757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-19 17:35:32.269003: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/19/2024 17:35:38 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 17:35:38,970 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 17:35:38,970 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 17:35:38,970 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 17:35:38,970 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 17:35:38,970 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-19 17:35:39,037 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/19/2024 17:35:39 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/19/2024 17:35:39 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/19/2024 17:35:39 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:01, 8555.86 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2235.08 examples/s]\n",
            "07/19/2024 17:35:44 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:01, 853.60 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2450.36 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:00<00:00, 1104.69 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 6.39MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 17:35:51,697 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 40.5MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 17:35:52,412 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-19 17:35:52,413 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 35.5MB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 57.8MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-19 17:35:53,977 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 41.9M/4.97G [00:00<00:11, 415MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 105M/4.97G [00:00<00:09, 499MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:00<00:09, 504MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 220M/4.97G [00:00<00:09, 484MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 273M/4.97G [00:00<00:09, 479MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 325M/4.97G [00:00<00:10, 464MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 377M/4.97G [00:00<00:10, 459MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 430M/4.97G [00:00<00:09, 471MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:01<00:09, 484MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 535M/4.97G [00:01<00:08, 495MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 587M/4.97G [00:01<00:08, 491MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 640M/4.97G [00:01<00:08, 487MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 692M/4.97G [00:01<00:08, 487MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:01<00:08, 472MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 797M/4.97G [00:01<00:08, 465MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 849M/4.97G [00:01<00:09, 440MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:01<00:09, 434MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 954M/4.97G [00:02<00:09, 425MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.01G/4.97G [00:02<00:09, 421MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:02<00:09, 422MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.11G/4.97G [00:02<00:08, 437MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:02<00:08, 455MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [00:02<00:08, 460MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.27G/4.97G [00:02<00:07, 465MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.32G/4.97G [00:02<00:07, 462MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.37G/4.97G [00:02<00:08, 450MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:03<00:08, 443MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.48G/4.97G [00:03<00:07, 446MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.53G/4.97G [00:03<00:07, 449MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:03<00:07, 451MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [00:03<00:07, 461MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.69G/4.97G [00:03<00:07, 462MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.74G/4.97G [00:03<00:07, 461MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:03<00:06, 467MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [00:04<00:06, 461MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:04<00:06, 470MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.95G/4.97G [00:04<00:06, 482MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.01G/4.97G [00:04<00:05, 500MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.07G/4.97G [00:04<00:05, 501MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:04<00:05, 482MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.17G/4.97G [00:04<00:05, 477MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.22G/4.97G [00:04<00:05, 477MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:04<00:05, 470MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.33G/4.97G [00:05<00:05, 464MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:05<00:05, 469MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:05<00:05, 465MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [00:05<00:05, 466MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:05<00:05, 460MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:05<00:05, 431MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.64G/4.97G [00:05<00:05, 420MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:05<00:05, 405MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [00:06<00:05, 382MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:06<00:05, 399MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.83G/4.97G [00:06<00:05, 380MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.87G/4.97G [00:08<00:32, 63.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.94G/4.97G [00:08<00:21, 94.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:08<00:16, 118MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.02G/4.97G [00:08<00:13, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.06G/4.97G [00:08<00:10, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:08<00:08, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.16G/4.97G [00:09<00:07, 258MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.21G/4.97G [00:09<00:05, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:09<00:05, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.31G/4.97G [00:09<00:04, 374MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.37G/4.97G [00:09<00:03, 409MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:09<00:03, 433MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.47G/4.97G [00:09<00:03, 440MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.52G/4.97G [00:09<00:03, 453MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.58G/4.97G [00:09<00:03, 464MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.63G/4.97G [00:09<00:02, 463MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [00:10<00:03, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [00:10<00:03, 355MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:10<00:03, 392MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.84G/4.97G [00:10<00:02, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:10<00:02, 417MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [00:10<00:02, 418MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [00:10<00:02, 403MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:11<00:02, 395MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:11<00:03, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.13G/4.97G [00:11<00:02, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.18G/4.97G [00:11<00:02, 352MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:11<00:02, 339MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.27G/4.97G [00:11<00:02, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.32G/4.97G [00:12<00:01, 359MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.36G/4.97G [00:12<00:01, 373MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.40G/4.97G [00:12<00:01, 356MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [00:12<00:01, 361MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.50G/4.97G [00:12<00:01, 379MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [00:12<00:01, 388MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [00:12<00:00, 404MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [00:12<00:00, 430MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.70G/4.97G [00:12<00:00, 444MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.75G/4.97G [00:13<00:00, 457MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [00:13<00:00, 455MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.85G/4.97G [00:13<00:00, 446MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [00:13<00:00, 439MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:13<00:00, 368MB/s]\n",
            "Downloading shards:  50% 1/2 [00:14<00:14, 14.01s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:06, 378MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 83.9M/2.67G [00:00<00:06, 391MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 136M/2.67G [00:00<00:05, 427MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 189M/2.67G [00:00<00:05, 447MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 241M/2.67G [00:00<00:05, 460MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 294M/2.67G [00:00<00:05, 453MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:00<00:05, 441MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:00<00:05, 444MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 451M/2.67G [00:01<00:05, 442MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:01<00:04, 446MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 556M/2.67G [00:01<00:04, 452MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 608M/2.67G [00:01<00:04, 453MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 661M/2.67G [00:01<00:04, 460MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:01<00:04, 462MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 765M/2.67G [00:01<00:04, 469MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 818M/2.67G [00:01<00:03, 475MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:01<00:03, 480MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:02<00:03, 486MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 975M/2.67G [00:02<00:03, 488MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.03G/2.67G [00:02<00:03, 488MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:02<00:03, 467MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:02<00:03, 471MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.18G/2.67G [00:02<00:03, 469MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.24G/2.67G [00:02<00:03, 467MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:02<00:03, 433MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [00:02<00:03, 434MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.39G/2.67G [00:03<00:02, 442MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:03<00:02, 448MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [00:03<00:02, 450MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [00:03<00:02, 450MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:03<00:02, 454MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.66G/2.67G [00:03<00:02, 455MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.71G/2.67G [00:03<00:02, 458MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.76G/2.67G [00:03<00:01, 458MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:03<00:01, 455MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:04<00:01, 454MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [00:04<00:01, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 1.97G/2.67G [00:04<00:01, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:04<00:01, 459MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.08G/2.67G [00:04<00:01, 460MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.13G/2.67G [00:04<00:01, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:04<00:01, 459MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.23G/2.67G [00:04<00:00, 458MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:05<00:00, 462MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.34G/2.67G [00:05<00:00, 459MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.39G/2.67G [00:05<00:00, 457MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.44G/2.67G [00:05<00:00, 456MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:05<00:00, 462MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.55G/2.67G [00:05<00:00, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.60G/2.67G [00:05<00:00, 461MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:05<00:00, 456MB/s]\n",
            "Downloading shards: 100% 2/2 [00:20<00:00, 10.37s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-19 17:36:14,722 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 17:36:14,724 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.34s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-19 17:36:19,581 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-19 17:36:19,581 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.28MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-19 17:36:20,128 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 17:36:20,128 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,gate_up_proj,o_proj,qkv_proj\n",
            "07/19/2024 17:36:20 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:642] 2024-07-19 17:36:20,597 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-19 17:36:21,041 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-19 17:36:21,041 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-19 17:36:21,041 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-19 17:36:21,041 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2134] 2024-07-19 17:36:21,041 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2135] 2024-07-19 17:36:21,041 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-19 17:36:21,041 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2137] 2024-07-19 17:36:21,044 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6831, 'grad_norm': 0.6707862019538879, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.4627, 'grad_norm': 0.5632836222648621, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [04:56<21:58, 29.95s/it][INFO|trainer.py:3788] 2024-07-19 17:41:17,466 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 17:41:17,466 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 17:41:17,466 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:09,  2.49it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:11,  1.98it/s]\u001b[A\n",
            " 16% 4/25 [00:02<00:12,  1.74it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.00it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.63it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:10,  1.69it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.61it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:09,  1.51it/s]\u001b[A\n",
            " 44% 11/25 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 48% 12/25 [00:07<00:08,  1.54it/s]\u001b[A\n",
            " 52% 13/25 [00:07<00:07,  1.55it/s]\u001b[A\n",
            " 56% 14/25 [00:08<00:07,  1.54it/s]\u001b[A\n",
            " 60% 15/25 [00:09<00:06,  1.66it/s]\u001b[A\n",
            " 64% 16/25 [00:09<00:05,  1.60it/s]\u001b[A\n",
            " 68% 17/25 [00:10<00:05,  1.58it/s]\u001b[A\n",
            " 72% 18/25 [00:11<00:04,  1.52it/s]\u001b[A\n",
            " 76% 19/25 [00:11<00:03,  1.69it/s]\u001b[A\n",
            " 80% 20/25 [00:12<00:03,  1.66it/s]\u001b[A\n",
            " 84% 21/25 [00:13<00:02,  1.44it/s]\u001b[A\n",
            " 88% 22/25 [00:13<00:01,  1.51it/s]\u001b[A\n",
            " 92% 23/25 [00:14<00:01,  1.56it/s]\u001b[A\n",
            " 96% 24/25 [00:15<00:00,  1.45it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3256181478500366, 'eval_runtime': 16.6261, 'eval_samples_per_second': 6.015, 'eval_steps_per_second': 1.504, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [05:13<21:58, 29.95s/it]\n",
            "100% 25/25 [00:15<00:00,  1.39it/s]\u001b[A\n",
            "{'loss': 1.2385, 'grad_norm': 0.4336055815219879, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.2793, 'grad_norm': 0.25713416934013367, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [10:09<17:04, 30.12s/it][INFO|trainer.py:3788] 2024-07-19 17:46:30,520 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 17:46:30,520 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 17:46:30,520 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:09,  2.50it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:11,  1.99it/s]\u001b[A\n",
            " 16% 4/25 [00:02<00:12,  1.75it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.01it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.63it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:10,  1.68it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.61it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:09,  1.51it/s]\u001b[A\n",
            " 44% 11/25 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 48% 12/25 [00:07<00:08,  1.54it/s]\u001b[A\n",
            " 52% 13/25 [00:07<00:07,  1.55it/s]\u001b[A\n",
            " 56% 14/25 [00:08<00:07,  1.54it/s]\u001b[A\n",
            " 60% 15/25 [00:09<00:06,  1.66it/s]\u001b[A\n",
            " 64% 16/25 [00:09<00:05,  1.59it/s]\u001b[A\n",
            " 68% 17/25 [00:10<00:05,  1.58it/s]\u001b[A\n",
            " 72% 18/25 [00:11<00:04,  1.52it/s]\u001b[A\n",
            " 76% 19/25 [00:11<00:03,  1.69it/s]\u001b[A\n",
            " 80% 20/25 [00:12<00:03,  1.66it/s]\u001b[A\n",
            " 84% 21/25 [00:13<00:02,  1.44it/s]\u001b[A\n",
            " 88% 22/25 [00:13<00:01,  1.51it/s]\u001b[A\n",
            " 92% 23/25 [00:14<00:01,  1.56it/s]\u001b[A\n",
            " 96% 24/25 [00:15<00:00,  1.45it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1795086860656738, 'eval_runtime': 16.618, 'eval_samples_per_second': 6.018, 'eval_steps_per_second': 1.504, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [10:26<17:04, 30.12s/it]\n",
            "100% 25/25 [00:15<00:00,  1.39it/s]\u001b[A\n",
            "{'loss': 1.2565, 'grad_norm': 0.29544028639793396, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.1384, 'grad_norm': 0.22938717901706696, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [15:28<12:15, 30.65s/it][INFO|trainer.py:3788] 2024-07-19 17:51:49,648 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 17:51:49,648 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 17:51:49,648 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:09,  2.49it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:11,  1.97it/s]\u001b[A\n",
            " 16% 4/25 [00:02<00:12,  1.74it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.00it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.62it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:10,  1.69it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.61it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:09,  1.51it/s]\u001b[A\n",
            " 44% 11/25 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 48% 12/25 [00:07<00:08,  1.54it/s]\u001b[A\n",
            " 52% 13/25 [00:07<00:07,  1.55it/s]\u001b[A\n",
            " 56% 14/25 [00:08<00:07,  1.54it/s]\u001b[A\n",
            " 60% 15/25 [00:09<00:06,  1.66it/s]\u001b[A\n",
            " 64% 16/25 [00:09<00:05,  1.59it/s]\u001b[A\n",
            " 68% 17/25 [00:10<00:05,  1.58it/s]\u001b[A\n",
            " 72% 18/25 [00:11<00:04,  1.52it/s]\u001b[A\n",
            " 76% 19/25 [00:11<00:03,  1.69it/s]\u001b[A\n",
            " 80% 20/25 [00:12<00:03,  1.65it/s]\u001b[A\n",
            " 84% 21/25 [00:13<00:02,  1.44it/s]\u001b[A\n",
            " 88% 22/25 [00:13<00:01,  1.51it/s]\u001b[A\n",
            " 92% 23/25 [00:14<00:01,  1.55it/s]\u001b[A\n",
            " 96% 24/25 [00:15<00:00,  1.45it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1444720029830933, 'eval_runtime': 16.6333, 'eval_samples_per_second': 6.012, 'eval_steps_per_second': 1.503, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [15:45<12:15, 30.65s/it]\n",
            "100% 25/25 [00:15<00:00,  1.39it/s]\u001b[A\n",
            "{'loss': 1.1415, 'grad_norm': 0.33522868156433105, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.2568, 'grad_norm': 0.27331992983818054, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [20:37<06:48, 29.17s/it][INFO|trainer.py:3788] 2024-07-19 17:56:58,667 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 17:56:58,667 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 17:56:58,667 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:09,  2.49it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:11,  1.98it/s]\u001b[A\n",
            " 16% 4/25 [00:02<00:12,  1.75it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.00it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.63it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:10,  1.68it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.60it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:09,  1.51it/s]\u001b[A\n",
            " 44% 11/25 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 48% 12/25 [00:07<00:08,  1.54it/s]\u001b[A\n",
            " 52% 13/25 [00:07<00:07,  1.55it/s]\u001b[A\n",
            " 56% 14/25 [00:08<00:07,  1.54it/s]\u001b[A\n",
            " 60% 15/25 [00:09<00:06,  1.66it/s]\u001b[A\n",
            " 64% 16/25 [00:09<00:05,  1.59it/s]\u001b[A\n",
            " 68% 17/25 [00:10<00:05,  1.58it/s]\u001b[A\n",
            " 72% 18/25 [00:11<00:04,  1.52it/s]\u001b[A\n",
            " 76% 19/25 [00:11<00:03,  1.70it/s]\u001b[A\n",
            " 80% 20/25 [00:12<00:03,  1.66it/s]\u001b[A\n",
            " 84% 21/25 [00:13<00:02,  1.44it/s]\u001b[A\n",
            " 88% 22/25 [00:13<00:01,  1.51it/s]\u001b[A\n",
            " 92% 23/25 [00:14<00:01,  1.56it/s]\u001b[A\n",
            " 96% 24/25 [00:15<00:00,  1.46it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1326597929000854, 'eval_runtime': 16.6218, 'eval_samples_per_second': 6.016, 'eval_steps_per_second': 1.504, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [20:54<06:48, 29.17s/it]\n",
            "100% 25/25 [00:15<00:00,  1.39it/s]\u001b[A\n",
            "{'loss': 1.1707, 'grad_norm': 0.228803351521492, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 1.1805, 'grad_norm': 0.20731474459171295, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [25:57<02:05, 31.30s/it][INFO|trainer.py:3788] 2024-07-19 18:02:18,661 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 18:02:18,661 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 18:02:18,661 >>   Batch size = 4\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 2/25 [00:00<00:09,  2.50it/s]\u001b[A\n",
            " 12% 3/25 [00:01<00:11,  1.99it/s]\u001b[A\n",
            " 16% 4/25 [00:02<00:11,  1.75it/s]\u001b[A\n",
            " 20% 5/25 [00:02<00:09,  2.01it/s]\u001b[A\n",
            " 24% 6/25 [00:03<00:11,  1.63it/s]\u001b[A\n",
            " 28% 7/25 [00:03<00:10,  1.69it/s]\u001b[A\n",
            " 32% 8/25 [00:04<00:10,  1.61it/s]\u001b[A\n",
            " 36% 9/25 [00:05<00:09,  1.61it/s]\u001b[A\n",
            " 40% 10/25 [00:05<00:09,  1.51it/s]\u001b[A\n",
            " 44% 11/25 [00:06<00:09,  1.52it/s]\u001b[A\n",
            " 48% 12/25 [00:07<00:08,  1.54it/s]\u001b[A\n",
            " 52% 13/25 [00:07<00:07,  1.55it/s]\u001b[A\n",
            " 56% 14/25 [00:08<00:07,  1.54it/s]\u001b[A\n",
            " 60% 15/25 [00:09<00:06,  1.66it/s]\u001b[A\n",
            " 64% 16/25 [00:09<00:05,  1.59it/s]\u001b[A\n",
            " 68% 17/25 [00:10<00:05,  1.58it/s]\u001b[A\n",
            " 72% 18/25 [00:11<00:04,  1.52it/s]\u001b[A\n",
            " 76% 19/25 [00:11<00:03,  1.69it/s]\u001b[A\n",
            " 80% 20/25 [00:12<00:03,  1.65it/s]\u001b[A\n",
            " 84% 21/25 [00:13<00:02,  1.44it/s]\u001b[A\n",
            " 88% 22/25 [00:13<00:01,  1.51it/s]\u001b[A\n",
            " 92% 23/25 [00:14<00:01,  1.55it/s]\u001b[A\n",
            " 96% 24/25 [00:15<00:00,  1.45it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.1301426887512207, 'eval_runtime': 16.619, 'eval_samples_per_second': 6.017, 'eval_steps_per_second': 1.504, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [26:14<02:05, 31.30s/it]\n",
            "100% 25/25 [00:15<00:00,  1.39it/s]\u001b[A\n",
            "100% 54/54 [28:05<00:00, 30.11s/it][INFO|trainer.py:3478] 2024-07-19 18:04:26,290 >> Saving model checkpoint to /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/checkpoint-54\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:04:26,868 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-19 18:04:26,869 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-19 18:04:27,064 >> tokenizer config file saved in /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/checkpoint-54/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-19 18:04:27,068 >> Special tokens file saved in /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/checkpoint-54/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-19 18:04:27,459 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1686.4146, 'train_samples_per_second': 1.601, 'train_steps_per_second': 0.032, 'train_loss': 1.2711060930181433, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [28:06<00:00, 31.23s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-19 18:04:27,462 >> Saving model checkpoint to /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:04:27,946 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-19 18:04:27,947 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-19 18:04:28,142 >> tokenizer config file saved in /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-19 18:04:28,146 >> Special tokens file saved in /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30757750GF\n",
            "  train_loss               =     1.2711\n",
            "  train_runtime            = 0:28:06.41\n",
            "  train_samples_per_second =      1.601\n",
            "  train_steps_per_second   =      0.032\n",
            "Figure saved at: /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train/training_eval_loss.png\n",
            "07/19/2024 18:04:28 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-19 18:04:28,551 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 18:04:28,551 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 18:04:28,551 >>   Batch size = 4\n",
            "100% 25/25 [00:15<00:00,  1.58it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_loss               =       1.13\n",
            "  eval_runtime            = 0:00:16.61\n",
            "  eval_samples_per_second =       6.02\n",
            "  eval_steps_per_second   =      1.505\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:449] 2024-07-19 18:04:45,177 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;lllamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_test_US,PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir saves\\Phi3-4B-4k-Chat\\lora\\eval_2024-07-17-23-32-25 \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "TEwt2jak5wfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type full \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US,PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/lora_lr/Qwen2-0.5B-Instruct/eval full \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "N5gUhkpFJb3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA/Phi_LoRA_PubMedQA_val \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train"
      ],
      "metadata": {
        "id": "G5OWis8TZnxd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff790adf-9e70-4100-9e9a-8a065029c561",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-19 18:11:59.658676: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-19 18:11:59.709598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-19 18:11:59.709646: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-19 18:11:59.711149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-19 18:11:59.718652: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-19 18:12:00.967164: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/19/2024 18:12:07 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:12:07,803 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:12:07,803 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:12:07,803 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:12:07,803 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:12:07,803 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-19 18:12:07,871 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/19/2024 18:12:07 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/19/2024 18:12:07 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/19/2024 18:12:07 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "Generating train split: 100 examples [00:00, 5813.96 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 440.14 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 134.76 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:12:11,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:12:11,530 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-19 18:12:11,532 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/19/2024 18:12:11 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-19 18:12:11,813 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-19 18:12:11,814 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 18:12:11,815 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.70s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-19 18:12:15,271 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-19 18:12:15,271 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-19 18:12:15,508 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 18:12:15,508 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/19/2024 18:12:15 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/19/2024 18:12:16 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/19/2024 18:12:16 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train\n",
            "07/19/2024 18:12:16 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-19 18:12:16,171 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 18:12:16,171 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 18:12:16,171 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-19 18:12:16,263 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [03:55<00:00,  6.34s/it]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.718 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [03:56<00:00,  4.73s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    19.2167\n",
            "  predict_rouge-1            =    26.7918\n",
            "  predict_rouge-2            =     9.9104\n",
            "  predict_rouge-l            =    19.8682\n",
            "  predict_runtime            = 0:03:58.39\n",
            "  predict_samples_per_second =      0.419\n",
            "  predict_steps_per_second   =       0.21\n",
            "07/19/2024 18:16:14 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/LoRA/Phi_LoRA_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 2 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/LoRA/Phi_LoRA_MedQA_test_US \\\n",
        "    --do_predict True \\\n",
        "    --adapter_name_or_path /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8np8jXfMVwY",
        "outputId": "90fe1c7c-ba51-48ae-ec7e-db8d9ed86bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-19 18:16:22.391497: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-19 18:16:22.443084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-19 18:16:22.443135: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-19 18:16:22.444629: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-19 18:16:22.452606: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-19 18:16:23.660415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/19/2024 18:16:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:16:30,530 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:16:30,531 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:16:30,531 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:16:30,531 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-19 18:16:30,531 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-19 18:16:30,600 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/19/2024 18:16:30 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/19/2024 18:16:30 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/19/2024 18:16:30 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "Generating train split: 1273 examples [00:00, 1789.81 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 445.36 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 135.87 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:16:37,357 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-19 18:16:37,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-19 18:16:37,847 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/19/2024 18:16:37 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-19 18:16:38,119 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-19 18:16:38,119 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 18:16:38,121 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.48s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-19 18:16:41,140 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-19 18:16:41,140 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-19 18:16:41,373 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-19 18:16:41,373 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/19/2024 18:16:41 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/19/2024 18:16:42 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\n",
            "07/19/2024 18:16:42 - INFO - llamafactory.model.adapter - Loaded adapter(s): /content/drive/MyDrive/9900/LoRA/Phi_LoRA_train\n",
            "07/19/2024 18:16:42 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-19 18:16:42,078 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-19 18:16:42,079 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-19 18:16:42,079 >>   Batch size = 2\n",
            "[WARNING|logging.py:328] 2024-07-19 18:16:42,178 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 50/50 [00:43<00:00,  1.29it/s]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.736 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 50/50 [00:44<00:00,  1.13it/s]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    61.3252\n",
            "  predict_rouge-1            =    66.3724\n",
            "  predict_rouge-2            =    55.1208\n",
            "  predict_rouge-l            =    66.1614\n",
            "  predict_runtime            = 0:00:46.37\n",
            "  predict_samples_per_second =      2.156\n",
            "  predict_steps_per_second   =      1.078\n",
            "07/19/2024 18:17:28 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/LoRA/Phi_LoRA_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/lora_lr/train_lr_test \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6zXWc2HQhcJs",
        "outputId": "f47678c8-c6c2-478b-eedf-ee3bc13a4588"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-07-16 05:07:07.665634: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-16 05:07:07.665685: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-16 05:07:07.666994: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-16 05:07:07.674521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-16 05:07:08.869375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/16/2024 05:07:14 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-16 05:07:14,508 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-16 05:07:14,508 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-16 05:07:14,508 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-16 05:07:14,508 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-16 05:07:14,508 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-16 05:07:14,583 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/16/2024 05:07:14 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/16/2024 05:07:14 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/16/2024 05:07:14 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 10972.07 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2162.18 examples/s]\n",
            "07/16/2024 05:07:17 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:00, 1946.48 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 2431.39 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:01<00:00, 974.40 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-16 05:07:21,331 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-16 05:07:21,530 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-16 05:07:21,531 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-16 05:07:21,663 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-16 05:07:21,664 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-16 05:07:21,665 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.87s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-16 05:07:27,462 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-16 05:07:27,462 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-16 05:07:27,568 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-16 05:07:27,569 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.model_utils.misc - Found linear modules: o_proj,qkv_proj,gate_up_proj,down_proj\n",
            "07/16/2024 05:07:27 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-16 05:07:27,879 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-16 05:07:29,039 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-16 05:07:29,039 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-16 05:07:29,039 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-16 05:07:29,039 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-16 05:07:29,039 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-16 05:07:29,039 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-16 05:07:29,039 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-16 05:07:29,042 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/54 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "  2% 1/54 [00:43<38:49, 43.95s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type lora \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/lora_epoch/train_epoch_test \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0 \\\n",
        "    --lora_target all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGV6cYk3hcLp",
        "outputId": "bc4c99ff-944f-4ede-b6b0-e8142902c7d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-17 13:47:55.068382: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-17 13:47:55.068463: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-17 13:47:55.070020: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-17 13:47:55.078587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-17 13:47:56.639045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
            "07/17/2024 13:48:03 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-17 13:48:03,875 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-17 13:48:03,876 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-17 13:48:03,876 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-17 13:48:03,876 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-17 13:48:03,876 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-17 13:48:03,970 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/17/2024 13:48:03 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/17/2024 13:48:03 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/17/2024 13:48:03 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/17/2024 13:48:04 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-17 13:48:04,356 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-17 13:48:04,429 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-17 13:48:04,430 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-17 13:48:04,504 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-17 13:48:04,505 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-17 13:48:04,506 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:03<00:00,  1.81s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-17 13:48:08,190 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-17 13:48:08,190 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-17 13:48:08,222 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-17 13:48:08,223 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.model_utils.misc - Found linear modules: qkv_proj,o_proj,gate_up_proj,down_proj\n",
            "07/17/2024 13:48:08 - INFO - llamafactory.model.loader - trainable params: 12,582,912 || all params: 3,833,662,464 || trainable%: 0.3282\n",
            "[INFO|trainer.py:641] 2024-07-17 13:48:08,550 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-17 13:48:09,862 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-17 13:48:09,862 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-17 13:48:09,863 >>   Num Epochs = 5\n",
            "[INFO|trainer.py:2081] 2024-07-17 13:48:09,863 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-17 13:48:09,863 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-17 13:48:09,863 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-17 13:48:09,863 >>   Total optimization steps = 90\n",
            "[INFO|trainer.py:2087] 2024-07-17 13:48:09,867 >>   Number of trainable parameters = 12,582,912\n",
            "  0% 0/90 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1FKI956hcNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XjOZfizMhcPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5iB6QJeVhcRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jDN-MTKohcTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}