{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9lrN_JqJZr1",
        "outputId": "1b2c154e-5f16-4f97-b4cc-4d34d9c6ce5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nw8EZgyoH87b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4539ede3-ff67-44ec-8fde-1393c8305b35",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 15858, done.\u001b[K\n",
            "remote: Counting objects: 100% (6887/6887), done.\u001b[K\n",
            "remote: Compressing objects: 100% (521/521), done.\u001b[K\n",
            "remote: Total 15858 (delta 6504), reused 6419 (delta 6363), pack-reused 8971\u001b[K\n",
            "Receiving objects: 100% (15858/15858), 222.27 MiB | 18.22 MiB/s, done.\n",
            "Resolving deltas: 100% (11695/11695), done.\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.41.2 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (4.42.4)\n",
            "Collecting datasets>=2.16.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate>=0.30.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.32.1)\n",
            "Collecting peft>=0.11.1 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl>=0.8.6 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting gradio>=4.0.0 (from llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio-4.39.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.11.4)\n",
            "Collecting einops (from llamafactory==0.8.4.dev0)\n",
            "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.1.99)\n",
            "Collecting tiktoken (from llamafactory==0.8.4.dev0)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.20.3)\n",
            "Collecting uvicorn (from llamafactory==0.8.4.dev0)\n",
            "  Downloading uvicorn-0.30.3-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Collecting fastapi (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fastapi-0.111.1-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.8.4.dev0)\n",
            "  Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting fire (from llamafactory==0.8.4.dev0)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (24.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (6.0.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (1.25.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (3.8.1)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (0.42.1)\n",
            "Collecting rouge-chinese (from llamafactory==0.8.4.dev0)\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.8.4.dev0) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.23.5)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.30.1->llamafactory==0.8.4.dev0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.66.4)\n",
            "Collecting xxhash (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets>=2.16.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.4.dev0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7.1)\n",
            "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==1.1.1 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading gradio_client-1.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting httpx>=0.24.1 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (6.4.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (9.4.0)\n",
            "Collecting pydub (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading ruff-0.5.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.12.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.0.7)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.1->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.8.4.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.8.4.dev0) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->llamafactory==0.8.4.dev0) (2.20.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (3.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->llamafactory==0.8.4.dev0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->llamafactory==0.8.4.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.41.2->llamafactory==0.8.4.dev0) (0.19.1)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.8.4.dev0) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->llamafactory==0.8.4.dev0)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting fastapi-cli>=0.0.2 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting email_validator>=2.0.0 (from fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.8.4.dev0) (2.4.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->llamafactory==0.8.4.dev0) (1.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.2.2)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.4.dev0) (4.0.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.4.dev0)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->llamafactory==0.8.4.dev0) (3.3.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (13.7.1)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.4.dev0)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.12.0->fastapi->llamafactory==0.8.4.dev0)\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->llamafactory==0.8.4.dev0) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.4.dev0) (0.1.2)\n",
            "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.39.0-py3-none-any.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.1.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m388.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.30.3-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.111.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.5.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m113.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, ffmpy\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.8.4.dev0-0.editable-py3-none-any.whl size=20629 sha256=205ec82afded6572a7aaa32af6ffef5114fbbb535c0aabe43ef1d1da106ac67e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lz8yhfs0/wheels/de/aa/c5/27b5682c5592b7c0eecc3e208f176dedf6b11a61cf2a910b85\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=9504cec7838146706563c7af198b6fb104f84108749b7c835f81e93ae490128e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=28253978b939de9c9ba29029be90733c3cad549993ce7ba3dd6ca6952e116754\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built llamafactory fire ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, uvloop, tomlkit, shtab, semantic-version, ruff, rouge-chinese, requests, python-multipart, python-dotenv, pyarrow, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, httptools, h11, fire, einops, dnspython, dill, aiofiles, watchfiles, uvicorn, tiktoken, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, email_validator, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, gradio-client, fastapi-cli, datasets, fastapi, trl, peft, gradio, llamafactory\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 datasets-2.20.0 dill-0.3.8 dnspython-2.6.1 einops-0.8.0 email_validator-2.2.0 fastapi-0.111.1 fastapi-cli-0.0.4 ffmpy-0.3.2 fire-0.6.0 gradio-4.39.0 gradio-client-1.1.1 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 llamafactory-0.8.4.dev0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 orjson-3.10.6 peft-0.12.0 pyarrow-17.0.0 pydub-0.25.1 python-dotenv-1.0.1 python-multipart-0.0.9 requests-2.32.3 rouge-chinese-1.0.3 ruff-0.5.4 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 starlette-0.37.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 tyro-0.8.5 uvicorn-0.30.3 uvloop-0.19.0 watchfiles-0.22.0 websockets-11.0.3 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 2 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/phi3/Freeze/train_batchz \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmKz_vqXP8dW",
        "outputId": "d0b01308-b8b1-43f7-8298-795dc86188e4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-25 14:10:24.521747: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-25 14:10:24.521803: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-25 14:10:24.645851: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-25 14:10:24.896509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-25 14:10:26.347745: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/25/2024 14:10:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 3.44k/3.44k [00:00<00:00, 19.5MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 14.8MB/s]\n",
            "tokenizer.json: 100% 1.94M/1.94M [00:00<00:00, 14.0MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.06MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 4.29MB/s]\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:10:36,639 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:10:36,639 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:10:36,639 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:10:36,639 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-25 14:10:36,639 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-25 14:10:36,723 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/25/2024 14:10:36 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/25/2024 14:10:36 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/25/2024 14:10:36 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "Generating train split: 10178 examples [00:00, 17042.13 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 905.41 examples/s] \n",
            "07/25/2024 14:10:39 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "Generating train split: 900 examples [00:00, 3131.02 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 500/500 [00:00<00:00, 1093.19 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 1000/1000 [00:03<00:00, 327.68 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "config.json: 100% 967/967 [00:00<00:00, 4.79MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 14:10:46,007 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "configuration_phi3.py: 100% 11.2k/11.2k [00:00<00:00, 34.0MB/s]\n",
            "[INFO|configuration_utils.py:733] 2024-07-25 14:10:46,291 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-25 14:10:46,292 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "modeling_phi3.py: 100% 73.2k/73.2k [00:00<00:00, 143MB/s]\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "model.safetensors.index.json: 100% 16.5k/16.5k [00:00<00:00, 51.9MB/s]\n",
            "[INFO|modeling_utils.py:3556] 2024-07-25 14:10:47,008 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 21.0M/4.97G [00:00<00:24, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 52.4M/4.97G [00:00<00:22, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 83.9M/4.97G [00:00<00:22, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 115M/4.97G [00:00<00:21, 223MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 147M/4.97G [00:00<00:23, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 168M/4.97G [00:00<00:24, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 189M/4.97G [00:00<00:24, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 210M/4.97G [00:01<00:24, 192MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 231M/4.97G [00:01<00:24, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 262M/4.97G [00:01<00:23, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 283M/4.97G [00:01<00:24, 193MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 315M/4.97G [00:01<00:23, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 346M/4.97G [00:01<00:23, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 367M/4.97G [00:01<00:23, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 388M/4.97G [00:01<00:23, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 419M/4.97G [00:02<00:21, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 451M/4.97G [00:02<00:21, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 482M/4.97G [00:05<02:47, 26.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 514M/4.97G [00:05<01:59, 37.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 545M/4.97G [00:05<01:26, 50.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 577M/4.97G [00:05<01:05, 67.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 608M/4.97G [00:06<00:52, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 629M/4.97G [00:06<00:45, 95.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 661M/4.97G [00:06<00:36, 117MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 682M/4.97G [00:06<00:33, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 713M/4.97G [00:06<00:28, 151MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 744M/4.97G [00:06<00:25, 165MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 776M/4.97G [00:06<00:23, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 807M/4.97G [00:06<00:21, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 839M/4.97G [00:07<00:20, 202MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 870M/4.97G [00:07<00:19, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 902M/4.97G [00:07<00:18, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 933M/4.97G [00:07<00:17, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 965M/4.97G [00:07<00:17, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 996M/4.97G [00:07<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.03G/4.97G [00:07<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:08<00:18, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:08<00:18, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.12G/4.97G [00:09<00:53, 71.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 1.14G/4.97G [00:09<00:45, 83.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.17G/4.97G [00:09<00:35, 106MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 1.21G/4.97G [00:09<00:29, 129MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.23G/4.97G [00:09<00:26, 140MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 1.25G/4.97G [00:09<00:24, 152MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.28G/4.97G [00:10<00:21, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [00:10<00:20, 182MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.33G/4.97G [00:10<00:19, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 1.36G/4.97G [00:10<00:18, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 1.39G/4.97G [00:10<00:17, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.43G/4.97G [00:10<00:17, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 1.46G/4.97G [00:10<00:16, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 1.49G/4.97G [00:11<00:15, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.52G/4.97G [00:11<00:15, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [00:11<00:15, 227MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.58G/4.97G [00:11<00:14, 231MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 1.61G/4.97G [00:11<00:14, 229MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 1.65G/4.97G [00:11<00:15, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.68G/4.97G [00:11<00:15, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 1.71G/4.97G [00:12<00:15, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.73G/4.97G [00:12<00:16, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 1.75G/4.97G [00:13<00:52, 61.0MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.77G/4.97G [00:13<00:43, 72.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [00:13<00:36, 86.2MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 1.81G/4.97G [00:13<00:31, 99.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.84G/4.97G [00:14<00:43, 71.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 1.86G/4.97G [00:14<00:35, 87.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.88G/4.97G [00:14<00:31, 97.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 1.90G/4.97G [00:14<00:26, 115MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.92G/4.97G [00:14<00:23, 131MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.94G/4.97G [00:14<00:21, 141MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 1.96G/4.97G [00:14<00:20, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 1.98G/4.97G [00:14<00:18, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 2.00G/4.97G [00:15<00:17, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [00:15<00:15, 187MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 2.06G/4.97G [00:15<00:15, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 2.09G/4.97G [00:15<00:14, 199MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.12G/4.97G [00:15<00:13, 205MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [00:15<00:13, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.18G/4.97G [00:15<00:13, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [00:16<00:12, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 2.24G/4.97G [00:16<00:12, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [00:16<00:17, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 2.31G/4.97G [00:16<00:15, 169MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [00:16<00:14, 184MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 2.36G/4.97G [00:16<00:14, 186MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 2.38G/4.97G [00:17<00:13, 185MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.41G/4.97G [00:17<00:13, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.43G/4.97G [00:17<00:12, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 2.45G/4.97G [00:17<00:12, 200MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [00:19<01:28, 28.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [00:20<00:59, 41.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 2.54G/4.97G [00:20<00:41, 58.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.57G/4.97G [00:20<00:31, 77.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 2.59G/4.97G [00:20<00:26, 90.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.61G/4.97G [00:20<00:22, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [00:20<00:19, 119MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 2.65G/4.97G [00:20<00:17, 134MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.67G/4.97G [00:20<00:15, 148MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 2.69G/4.97G [00:20<00:14, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.73G/4.97G [00:21<00:12, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 2.76G/4.97G [00:21<00:11, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 2.79G/4.97G [00:21<00:10, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.82G/4.97G [00:21<00:10, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 2.85G/4.97G [00:21<00:10, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 2.88G/4.97G [00:21<00:09, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.92G/4.97G [00:21<00:09, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 2.95G/4.97G [00:22<00:09, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 2.98G/4.97G [00:22<00:09, 210MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.01G/4.97G [00:22<00:09, 213MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 3.04G/4.97G [00:22<00:08, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.07G/4.97G [00:22<00:08, 211MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 3.10G/4.97G [00:22<00:08, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 3.14G/4.97G [00:22<00:08, 219MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.17G/4.97G [00:23<00:08, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [00:23<00:07, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 3.23G/4.97G [00:23<00:07, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.26G/4.97G [00:23<00:07, 223MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 3.29G/4.97G [00:23<00:07, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.32G/4.97G [00:23<00:07, 225MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 3.36G/4.97G [00:23<00:07, 222MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [00:24<00:07, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [00:24<00:07, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 3.45G/4.97G [00:24<00:06, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [00:24<00:06, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.51G/4.97G [00:24<00:06, 212MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [00:24<00:06, 209MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.57G/4.97G [00:24<00:06, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 3.60G/4.97G [00:25<00:06, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.62G/4.97G [00:25<00:06, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 3.65G/4.97G [00:25<00:06, 208MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.67G/4.97G [00:25<00:06, 206MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 3.69G/4.97G [00:25<00:06, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.71G/4.97G [00:25<00:06, 196MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 3.73G/4.97G [00:25<00:06, 198MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.76G/4.97G [00:25<00:05, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 3.79G/4.97G [00:26<00:05, 203MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.81G/4.97G [00:26<00:05, 195MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.83G/4.97G [00:26<00:06, 178MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 3.85G/4.97G [00:26<00:06, 174MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [00:26<00:06, 173MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 3.89G/4.97G [00:26<00:06, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.91G/4.97G [00:26<00:06, 171MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 3.93G/4.97G [00:27<00:08, 117MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.95G/4.97G [00:27<00:08, 127MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 3.97G/4.97G [00:27<00:07, 139MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [00:27<00:06, 146MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.02G/4.97G [00:27<00:06, 150MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 4.04G/4.97G [00:27<00:05, 158MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.06G/4.97G [00:27<00:05, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.08G/4.97G [00:27<00:05, 160MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 4.10G/4.97G [00:28<00:05, 168MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [00:29<00:17, 48.1MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 4.14G/4.97G [00:29<00:13, 60.3MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [00:29<00:10, 75.6MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [00:29<00:07, 103MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.23G/4.97G [00:29<00:05, 130MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 4.25G/4.97G [00:29<00:05, 143MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 4.28G/4.97G [00:30<00:04, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [00:30<00:03, 177MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 4.33G/4.97G [00:31<00:10, 58.5MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.35G/4.97G [00:31<00:09, 68.9MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [00:31<00:06, 91.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.41G/4.97G [00:31<00:04, 116MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 4.45G/4.97G [00:31<00:03, 137MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.47G/4.97G [00:33<00:10, 48.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 4.49G/4.97G [00:33<00:08, 58.7MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [00:33<00:06, 72.4MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 4.54G/4.97G [00:33<00:04, 96.8MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.57G/4.97G [00:33<00:03, 121MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 4.59G/4.97G [00:33<00:02, 132MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.61G/4.97G [00:33<00:02, 142MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 4.65G/4.97G [00:34<00:02, 162MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 4.68G/4.97G [00:34<00:01, 176MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.71G/4.97G [00:34<00:01, 191MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 4.74G/4.97G [00:34<00:01, 201MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 4.77G/4.97G [00:34<00:00, 204MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.80G/4.97G [00:34<00:00, 207MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 4.83G/4.97G [00:34<00:00, 214MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.87G/4.97G [00:35<00:00, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 4.90G/4.97G [00:35<00:00, 220MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 4.93G/4.97G [00:35<00:00, 224MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [00:35<00:00, 140MB/s]\n",
            "Downloading shards:  50% 1/2 [00:35<00:35, 35.67s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 21.0M/2.67G [00:00<00:13, 194MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 41.9M/2.67G [00:00<00:19, 135MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 62.9M/2.67G [00:00<00:16, 153MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 94.4M/2.67G [00:00<00:14, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 126M/2.67G [00:00<00:13, 193MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 157M/2.67G [00:00<00:12, 201MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 178M/2.67G [00:02<01:00, 40.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 199M/2.67G [00:02<00:47, 52.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 231M/2.67G [00:02<00:33, 72.3MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 262M/2.67G [00:02<00:25, 94.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 283M/2.67G [00:03<00:22, 107MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 304M/2.67G [00:03<00:19, 120MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 325M/2.67G [00:03<00:17, 132MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 346M/2.67G [00:03<00:16, 142MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 367M/2.67G [00:03<00:15, 150MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 398M/2.67G [00:03<00:13, 163MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 419M/2.67G [00:03<00:13, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 440M/2.67G [00:03<00:12, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 461M/2.67G [00:04<00:12, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 482M/2.67G [00:04<00:12, 176MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 503M/2.67G [00:04<00:11, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 524M/2.67G [00:04<00:11, 184MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 545M/2.67G [00:04<00:11, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 566M/2.67G [00:04<00:11, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 587M/2.67G [00:04<00:11, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 619M/2.67G [00:04<00:10, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 650M/2.67G [00:04<00:09, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 671M/2.67G [00:05<00:09, 206MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 692M/2.67G [00:05<00:09, 200MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 713M/2.67G [00:05<00:10, 191MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 734M/2.67G [00:05<00:11, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 755M/2.67G [00:05<00:10, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 776M/2.67G [00:05<00:10, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 797M/2.67G [00:05<00:09, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 828M/2.67G [00:05<00:09, 198MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 860M/2.67G [00:06<00:08, 202MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 881M/2.67G [00:06<00:13, 134MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 902M/2.67G [00:06<00:12, 139MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 923M/2.67G [00:06<00:11, 146MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 944M/2.67G [00:06<00:11, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 965M/2.67G [00:06<00:10, 160MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 986M/2.67G [00:08<00:41, 40.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.02G/2.67G [00:08<00:28, 58.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.05G/2.67G [00:08<00:20, 79.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.08G/2.67G [00:08<00:15, 101MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.11G/2.67G [00:08<00:12, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:09<00:11, 136MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.15G/2.67G [00:09<00:10, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.17G/2.67G [00:09<00:09, 160MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 1.20G/2.67G [00:09<00:08, 169MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 1.23G/2.67G [00:09<00:07, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 1.26G/2.67G [00:09<00:07, 196MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.29G/2.67G [00:09<00:06, 205MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 1.32G/2.67G [00:09<00:06, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 1.35G/2.67G [00:10<00:06, 216MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 1.38G/2.67G [00:10<00:05, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.42G/2.67G [00:10<00:05, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.45G/2.67G [00:10<00:05, 228MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 1.48G/2.67G [00:10<00:05, 224MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.51G/2.67G [00:10<00:05, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.54G/2.67G [00:10<00:05, 212MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 1.57G/2.67G [00:11<00:05, 213MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.60G/2.67G [00:14<00:42, 25.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 1.64G/2.67G [00:15<00:30, 34.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 1.67G/2.67G [00:15<00:21, 46.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 1.70G/2.67G [00:15<00:15, 60.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.73G/2.67G [00:15<00:12, 76.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.75G/2.67G [00:15<00:10, 87.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 1.77G/2.67G [00:15<00:08, 102MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 1.79G/2.67G [00:15<00:07, 117MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.81G/2.67G [00:15<00:06, 131MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 1.84G/2.67G [00:15<00:05, 146MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [00:16<00:04, 165MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 1.90G/2.67G [00:16<00:04, 183MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.93G/2.67G [00:16<00:03, 192MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 1.96G/2.67G [00:16<00:03, 204MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [00:16<00:03, 214MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 2.02G/2.67G [00:16<00:02, 220MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [00:16<00:02, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 2.09G/2.67G [00:17<00:02, 209MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 2.12G/2.67G [00:17<00:02, 203MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.14G/2.67G [00:18<00:07, 72.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 2.16G/2.67G [00:18<00:05, 85.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.18G/2.67G [00:18<00:04, 99.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.20G/2.67G [00:18<00:04, 110MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 2.22G/2.67G [00:18<00:03, 123MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [00:18<00:03, 139MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 2.26G/2.67G [00:18<00:02, 148MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.29G/2.67G [00:19<00:02, 149MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 2.32G/2.67G [00:19<00:02, 167MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [00:19<00:01, 182MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 2.38G/2.67G [00:19<00:01, 187MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 2.41G/2.67G [00:19<00:01, 192MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.43G/2.67G [00:19<00:01, 185MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 2.45G/2.67G [00:19<00:01, 181MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.47G/2.67G [00:20<00:01, 177MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.50G/2.67G [00:20<00:00, 176MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 2.52G/2.67G [00:20<00:00, 179MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 2.54G/2.67G [00:20<00:00, 178MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.57G/2.67G [00:20<00:00, 191MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 2.59G/2.67G [00:20<00:00, 184MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [00:20<00:00, 174MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 2.63G/2.67G [00:20<00:00, 175MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [00:21<00:00, 127MB/s]\n",
            "Downloading shards: 100% 2/2 [00:56<00:00, 28.48s/it]\n",
            "[INFO|modeling_utils.py:1531] 2024-07-25 14:11:43,979 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-25 14:11:43,981 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:37<00:00, 18.81s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-25 14:12:21,741 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-25 14:12:21,741 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.04MB/s]\n",
            "[INFO|configuration_utils.py:955] 2024-07-25 14:12:21,920 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-25 14:12:21,921 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/25/2024 14:12:21 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/25/2024 14:12:21 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/25/2024 14:12:21 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/25/2024 14:12:21 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/25/2024 14:12:22 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/25/2024 14:12:22 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:642] 2024-07-25 14:12:22,051 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2128] 2024-07-25 14:12:22,656 >> ***** Running training *****\n",
            "[INFO|trainer.py:2129] 2024-07-25 14:12:22,657 >>   Num examples = 900\n",
            "[INFO|trainer.py:2130] 2024-07-25 14:12:22,657 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2131] 2024-07-25 14:12:22,657 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2134] 2024-07-25 14:12:22,657 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2135] 2024-07-25 14:12:22,657 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2136] 2024-07-25 14:12:22,657 >>   Total optimization steps = 168\n",
            "[INFO|trainer.py:2137] 2024-07-25 14:12:22,659 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/168 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.338, 'grad_norm': 1.6455178260803223, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 36416}\n",
            "{'loss': 1.1238, 'grad_norm': 1.5470126867294312, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 73536}\n",
            "  6% 10/168 [00:45<11:42,  4.45s/it][INFO|trainer.py:3788] 2024-07-25 14:13:07,729 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:13:07,729 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:13:07,729 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:13,  2.36it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:12,  2.42it/s]\u001b[A\n",
            " 12% 4/34 [00:01<00:16,  1.87it/s]\u001b[A\n",
            " 15% 5/34 [00:02<00:17,  1.68it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:18,  1.54it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:18,  1.48it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:19,  1.35it/s]\u001b[A\n",
            " 26% 9/34 [00:05<00:17,  1.39it/s]\u001b[A\n",
            " 29% 10/34 [00:06<00:17,  1.37it/s]\u001b[A\n",
            " 32% 11/34 [00:06<00:15,  1.52it/s]\u001b[A\n",
            " 35% 12/34 [00:07<00:14,  1.49it/s]\u001b[A\n",
            " 38% 13/34 [00:08<00:15,  1.38it/s]\u001b[A\n",
            " 41% 14/34 [00:09<00:14,  1.41it/s]\u001b[A\n",
            " 44% 15/34 [00:09<00:13,  1.38it/s]\u001b[A\n",
            " 47% 16/34 [00:10<00:12,  1.49it/s]\u001b[A\n",
            " 50% 17/34 [00:11<00:11,  1.45it/s]\u001b[A\n",
            " 53% 18/34 [00:11<00:11,  1.44it/s]\u001b[A\n",
            " 56% 19/34 [00:12<00:10,  1.41it/s]\u001b[A\n",
            " 59% 20/34 [00:13<00:09,  1.47it/s]\u001b[A\n",
            " 62% 21/34 [00:14<00:09,  1.42it/s]\u001b[A\n",
            " 65% 22/34 [00:14<00:08,  1.40it/s]\u001b[A\n",
            " 68% 23/34 [00:15<00:07,  1.38it/s]\u001b[A\n",
            " 71% 24/34 [00:16<00:07,  1.33it/s]\u001b[A\n",
            " 74% 25/34 [00:16<00:06,  1.46it/s]\u001b[A\n",
            " 76% 26/34 [00:17<00:05,  1.43it/s]\u001b[A\n",
            " 79% 27/34 [00:18<00:05,  1.26it/s]\u001b[A\n",
            " 82% 28/34 [00:19<00:04,  1.32it/s]\u001b[A\n",
            " 85% 29/34 [00:19<00:03,  1.37it/s]\u001b[A\n",
            " 88% 30/34 [00:20<00:02,  1.40it/s]\u001b[A\n",
            " 91% 31/34 [00:21<00:02,  1.33it/s]\u001b[A\n",
            " 94% 32/34 [00:22<00:01,  1.26it/s]\u001b[A\n",
            " 97% 33/34 [00:23<00:00,  1.32it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.3134056329727173, 'eval_runtime': 24.2687, 'eval_samples_per_second': 4.121, 'eval_steps_per_second': 1.401, 'epoch': 0.18, 'num_input_tokens_seen': 73536}\n",
            "  6% 10/168 [01:09<11:42,  4.45s/it]\n",
            "100% 34/34 [00:23<00:00,  1.59it/s]\u001b[A\n",
            "{'loss': 1.2392, 'grad_norm': 1.6297719478607178, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.27, 'num_input_tokens_seen': 110288}\n",
            "{'loss': 1.0649, 'grad_norm': 1.375707745552063, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 146544}\n",
            " 12% 20/168 [01:55<12:15,  4.97s/it][INFO|trainer.py:3788] 2024-07-25 14:14:18,038 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:14:18,038 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:14:18,038 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.08it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.19it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.68it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.48it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.38it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.27it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.24it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.38it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.35it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.25it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.28it/s]\u001b[A\n",
            " 44% 15/34 [00:11<00:15,  1.25it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.35it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.31it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.29it/s]\u001b[A\n",
            " 56% 19/34 [00:14<00:11,  1.26it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.32it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.28it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.26it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.24it/s]\u001b[A\n",
            " 71% 24/34 [00:18<00:08,  1.20it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.33it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.30it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.14it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.21it/s]\u001b[A\n",
            " 85% 29/34 [00:22<00:03,  1.26it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.29it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.22it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.16it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.23it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2805347442626953, 'eval_runtime': 26.7488, 'eval_samples_per_second': 3.738, 'eval_steps_per_second': 1.271, 'epoch': 0.36, 'num_input_tokens_seen': 146544}\n",
            " 12% 20/168 [02:22<12:15,  4.97s/it]\n",
            "100% 34/34 [00:25<00:00,  1.47it/s]\u001b[A\n",
            "{'loss': 0.9949, 'grad_norm': 1.3323090076446533, 'learning_rate': 4.731745523109029e-05, 'epoch': 0.44, 'num_input_tokens_seen': 182368}\n",
            "{'loss': 0.9974, 'grad_norm': 1.4027363061904907, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 218928}\n",
            " 18% 30/168 [03:08<11:26,  4.98s/it][INFO|trainer.py:3788] 2024-07-25 14:15:31,505 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:15:31,505 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:15:31,505 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.12it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:13,  2.23it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.71it/s]\u001b[A\n",
            " 15% 5/34 [00:02<00:19,  1.51it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.40it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.23it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.26it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.40it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.30it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:14,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.30it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.23it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.17it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2657008171081543, 'eval_runtime': 26.3888, 'eval_samples_per_second': 3.789, 'eval_steps_per_second': 1.288, 'epoch': 0.53, 'num_input_tokens_seen': 218928}\n",
            " 18% 30/168 [03:35<11:26,  4.98s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 1.2015, 'grad_norm': 1.6760056018829346, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 256848}\n",
            "{'loss': 0.9272, 'grad_norm': 0.9133765697479248, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 290352}\n",
            " 24% 40/168 [04:21<10:11,  4.77s/it][INFO|trainer.py:3788] 2024-07-25 14:16:44,450 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:16:44,450 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:16:44,450 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.10it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.21it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.70it/s]\u001b[A\n",
            " 15% 5/34 [00:02<00:19,  1.51it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.40it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.23it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.29it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.26it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.40it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.30it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:14,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.29it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:16<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.24it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:24<00:00,  1.25it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2625815868377686, 'eval_runtime': 26.3074, 'eval_samples_per_second': 3.801, 'eval_steps_per_second': 1.292, 'epoch': 0.71, 'num_input_tokens_seen': 290352}\n",
            " 24% 40/168 [04:48<10:11,  4.77s/it]\n",
            "100% 34/34 [00:25<00:00,  1.51it/s]\u001b[A\n",
            "{'loss': 0.9292, 'grad_norm': 1.41203773021698, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.8, 'num_input_tokens_seen': 326320}\n",
            "{'loss': 1.0497, 'grad_norm': 1.2333866357803345, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 366016}\n",
            " 30% 50/168 [05:37<10:25,  5.30s/it][INFO|trainer.py:3788] 2024-07-25 14:17:59,747 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:17:59,748 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:17:59,748 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.09it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.21it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.68it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.50it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.23it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.29it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.26it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.40it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:14,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.26it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2556499242782593, 'eval_runtime': 26.3682, 'eval_samples_per_second': 3.792, 'eval_steps_per_second': 1.289, 'epoch': 0.89, 'num_input_tokens_seen': 366016}\n",
            " 30% 50/168 [06:03<10:25,  5.30s/it]\n",
            "100% 34/34 [00:25<00:00,  1.51it/s]\u001b[A\n",
            "{'loss': 1.134, 'grad_norm': 2.099759817123413, 'learning_rate': 3.790265684518767e-05, 'epoch': 0.98, 'num_input_tokens_seen': 404672}\n",
            "{'loss': 1.0449, 'grad_norm': 1.279213547706604, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 441392}\n",
            " 36% 60/168 [06:52<09:14,  5.13s/it][INFO|trainer.py:3788] 2024-07-25 14:19:14,861 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:19:14,861 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:19:14,861 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.11it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:13,  2.22it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.70it/s]\u001b[A\n",
            " 15% 5/34 [00:02<00:19,  1.50it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.23it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.26it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.40it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.30it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:14,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.29it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.35it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:16<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.24it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.25it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2510876655578613, 'eval_runtime': 26.3328, 'eval_samples_per_second': 3.798, 'eval_steps_per_second': 1.291, 'epoch': 1.07, 'num_input_tokens_seen': 441392}\n",
            " 36% 60/168 [07:18<09:14,  5.13s/it]\n",
            "100% 34/34 [00:25<00:00,  1.50it/s]\u001b[A\n",
            "{'loss': 0.984, 'grad_norm': 1.149806261062622, 'learning_rate': 3.369677161463068e-05, 'epoch': 1.16, 'num_input_tokens_seen': 480400}\n",
            "{'loss': 0.9054, 'grad_norm': 1.241377592086792, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 515072}\n",
            " 42% 70/168 [08:06<08:02,  4.92s/it][INFO|trainer.py:3788] 2024-07-25 14:20:28,925 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:20:28,925 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:20:28,925 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.10it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.20it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.30it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:14,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.29it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.35it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2569583654403687, 'eval_runtime': 26.3846, 'eval_samples_per_second': 3.79, 'eval_steps_per_second': 1.289, 'epoch': 1.24, 'num_input_tokens_seen': 515072}\n",
            " 42% 70/168 [08:32<08:02,  4.92s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 0.9651, 'grad_norm': 1.0622308254241943, 'learning_rate': 2.918765558261841e-05, 'epoch': 1.33, 'num_input_tokens_seen': 553440}\n",
            "{'loss': 0.8597, 'grad_norm': 1.3648861646652222, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 591776}\n",
            " 48% 80/168 [09:22<07:36,  5.19s/it][INFO|trainer.py:3788] 2024-07-25 14:21:45,055 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:21:45,055 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:21:45,055 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.09it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:13,  2.22it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.35it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.36it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.17it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.25it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.266488790512085, 'eval_runtime': 26.3655, 'eval_samples_per_second': 3.793, 'eval_steps_per_second': 1.29, 'epoch': 1.42, 'num_input_tokens_seen': 591776}\n",
            " 48% 80/168 [09:48<07:36,  5.19s/it]\n",
            "100% 34/34 [00:25<00:00,  1.50it/s]\u001b[A\n",
            "{'loss': 0.844, 'grad_norm': 1.1912882328033447, 'learning_rate': 2.4532528339227452e-05, 'epoch': 1.51, 'num_input_tokens_seen': 626912}\n",
            "{'loss': 0.9294, 'grad_norm': 1.8234949111938477, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 664064}\n",
            " 54% 90/168 [10:35<06:44,  5.18s/it][INFO|trainer.py:3788] 2024-07-25 14:22:58,573 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:22:58,573 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:22:58,573 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.10it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.21it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.68it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.31it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.25it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2674221992492676, 'eval_runtime': 26.4236, 'eval_samples_per_second': 3.784, 'eval_steps_per_second': 1.287, 'epoch': 1.6, 'num_input_tokens_seen': 664064}\n",
            " 54% 90/168 [11:02<06:44,  5.18s/it]\n",
            "100% 34/34 [00:25<00:00,  1.50it/s]\u001b[A\n",
            "{'loss': 0.7762, 'grad_norm': 0.9178082346916199, 'learning_rate': 1.9893700455257996e-05, 'epoch': 1.69, 'num_input_tokens_seen': 699680}\n",
            "{'loss': 0.8642, 'grad_norm': 1.0760326385498047, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 735152}\n",
            " 60% 100/168 [11:48<05:37,  4.96s/it][INFO|trainer.py:3788] 2024-07-25 14:24:11,291 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:24:11,291 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:24:11,291 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.09it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.21it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.26it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.36it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.31it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.27it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.17it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2728904485702515, 'eval_runtime': 26.4641, 'eval_samples_per_second': 3.779, 'eval_steps_per_second': 1.285, 'epoch': 1.78, 'num_input_tokens_seen': 735152}\n",
            " 60% 100/168 [12:15<05:37,  4.96s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 0.8293, 'grad_norm': 1.4197683334350586, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.87, 'num_input_tokens_seen': 771168}\n",
            "{'loss': 1.0139, 'grad_norm': 1.1446846723556519, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 809888}\n",
            " 65% 110/168 [13:03<05:00,  5.18s/it][INFO|trainer.py:3788] 2024-07-25 14:25:26,426 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:25:26,426 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:25:26,426 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.09it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.20it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.50it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.27it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.26it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.36it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.32it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.31it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.31it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.17it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.25it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2767528295516968, 'eval_runtime': 26.426, 'eval_samples_per_second': 3.784, 'eval_steps_per_second': 1.287, 'epoch': 1.96, 'num_input_tokens_seen': 809888}\n",
            " 65% 110/168 [13:30<05:00,  5.18s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 0.8837, 'grad_norm': 1.101771593093872, 'learning_rate': 1.130570401955322e-05, 'epoch': 2.04, 'num_input_tokens_seen': 848368}\n",
            "{'loss': 0.7937, 'grad_norm': 1.2375128269195557, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 883504}\n",
            " 71% 120/168 [14:18<03:55,  4.91s/it][INFO|trainer.py:3788] 2024-07-25 14:26:40,828 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:26:40,828 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:26:40,828 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.08it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.20it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.50it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.29it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.31it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.27it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.30it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2827203273773193, 'eval_runtime': 26.4343, 'eval_samples_per_second': 3.783, 'eval_steps_per_second': 1.286, 'epoch': 2.13, 'num_input_tokens_seen': 883504}\n",
            " 71% 120/168 [14:44<03:55,  4.91s/it]\n",
            "100% 34/34 [00:25<00:00,  1.50it/s]\u001b[A\n",
            "{'loss': 0.6542, 'grad_norm': 1.0853264331817627, 'learning_rate': 7.65597359928646e-06, 'epoch': 2.22, 'num_input_tokens_seen': 921904}\n",
            "{'loss': 0.7963, 'grad_norm': 1.0298423767089844, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 958336}\n",
            " 77% 130/168 [15:32<03:06,  4.91s/it][INFO|trainer.py:3788] 2024-07-25 14:27:55,634 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:27:55,634 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:27:55,634 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.11it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:13,  2.22it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.27it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.35it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.27it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.2994905710220337, 'eval_runtime': 26.4306, 'eval_samples_per_second': 3.783, 'eval_steps_per_second': 1.286, 'epoch': 2.31, 'num_input_tokens_seen': 958336}\n",
            " 77% 130/168 [15:59<03:06,  4.91s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 0.8114, 'grad_norm': 1.1130821704864502, 'learning_rate': 4.610978276018496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 995088}\n",
            "{'loss': 0.7946, 'grad_norm': 1.1300190687179565, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1031200}\n",
            " 83% 140/168 [16:46<02:24,  5.16s/it][INFO|trainer.py:3788] 2024-07-25 14:29:09,420 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:29:09,421 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:29:09,421 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.10it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.21it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.26it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.36it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.32it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.31it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.31it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.24it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.29it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.32it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.25it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.309090256690979, 'eval_runtime': 26.4116, 'eval_samples_per_second': 3.786, 'eval_steps_per_second': 1.287, 'epoch': 2.49, 'num_input_tokens_seen': 1031200}\n",
            " 83% 140/168 [17:13<02:24,  5.16s/it]\n",
            "100% 34/34 [00:25<00:00,  1.50it/s]\u001b[A\n",
            "{'loss': 0.6676, 'grad_norm': 1.201778531074524, 'learning_rate': 2.2768880646947268e-06, 'epoch': 2.58, 'num_input_tokens_seen': 1068752}\n",
            "{'loss': 0.8968, 'grad_norm': 1.0454155206680298, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1109232}\n",
            " 89% 150/168 [18:03<01:40,  5.60s/it][INFO|trainer.py:3788] 2024-07-25 14:30:26,464 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:30:26,464 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:30:26,464 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.09it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:14,  2.20it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.38it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.33it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.22it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.28it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.25it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.39it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.36it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.26it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.26it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.36it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.32it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.31it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.28it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.34it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.29it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.31it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.23it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.30it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.23it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.17it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.24it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.3139034509658813, 'eval_runtime': 26.4789, 'eval_samples_per_second': 3.777, 'eval_steps_per_second': 1.284, 'epoch': 2.67, 'num_input_tokens_seen': 1109232}\n",
            " 89% 150/168 [18:30<01:40,  5.60s/it]\n",
            "100% 34/34 [00:25<00:00,  1.49it/s]\u001b[A\n",
            "{'loss': 0.8447, 'grad_norm': 0.999687135219574, 'learning_rate': 7.350858136652261e-07, 'epoch': 2.76, 'num_input_tokens_seen': 1146272}\n",
            "{'loss': 0.5892, 'grad_norm': 0.8382247686386108, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1179248}\n",
            " 95% 160/168 [19:16<00:37,  4.67s/it][INFO|trainer.py:3788] 2024-07-25 14:31:38,774 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:31:38,774 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:31:38,774 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:15,  2.11it/s]\u001b[A\n",
            "  9% 3/34 [00:01<00:13,  2.22it/s]\u001b[A\n",
            " 12% 4/34 [00:02<00:17,  1.69it/s]\u001b[A\n",
            " 15% 5/34 [00:03<00:19,  1.49it/s]\u001b[A\n",
            " 18% 6/34 [00:03<00:20,  1.39it/s]\u001b[A\n",
            " 21% 7/34 [00:04<00:20,  1.34it/s]\u001b[A\n",
            " 24% 8/34 [00:05<00:21,  1.23it/s]\u001b[A\n",
            " 26% 9/34 [00:06<00:19,  1.29it/s]\u001b[A\n",
            " 29% 10/34 [00:07<00:19,  1.26it/s]\u001b[A\n",
            " 32% 11/34 [00:07<00:16,  1.40it/s]\u001b[A\n",
            " 35% 12/34 [00:08<00:16,  1.37it/s]\u001b[A\n",
            " 38% 13/34 [00:09<00:16,  1.27it/s]\u001b[A\n",
            " 41% 14/34 [00:10<00:15,  1.29it/s]\u001b[A\n",
            " 44% 15/34 [00:10<00:15,  1.27it/s]\u001b[A\n",
            " 47% 16/34 [00:11<00:13,  1.37it/s]\u001b[A\n",
            " 50% 17/34 [00:12<00:12,  1.33it/s]\u001b[A\n",
            " 53% 18/34 [00:13<00:12,  1.32it/s]\u001b[A\n",
            " 56% 19/34 [00:13<00:11,  1.29it/s]\u001b[A\n",
            " 59% 20/34 [00:14<00:10,  1.35it/s]\u001b[A\n",
            " 62% 21/34 [00:15<00:10,  1.30it/s]\u001b[A\n",
            " 65% 22/34 [00:16<00:09,  1.28it/s]\u001b[A\n",
            " 68% 23/34 [00:17<00:08,  1.26it/s]\u001b[A\n",
            " 71% 24/34 [00:17<00:08,  1.22it/s]\u001b[A\n",
            " 74% 25/34 [00:18<00:06,  1.35it/s]\u001b[A\n",
            " 76% 26/34 [00:19<00:06,  1.32it/s]\u001b[A\n",
            " 79% 27/34 [00:20<00:06,  1.16it/s]\u001b[A\n",
            " 82% 28/34 [00:21<00:04,  1.24it/s]\u001b[A\n",
            " 85% 29/34 [00:21<00:03,  1.28it/s]\u001b[A\n",
            " 88% 30/34 [00:22<00:03,  1.31it/s]\u001b[A\n",
            " 91% 31/34 [00:23<00:02,  1.24it/s]\u001b[A\n",
            " 94% 32/34 [00:24<00:01,  1.18it/s]\u001b[A\n",
            " 97% 33/34 [00:25<00:00,  1.26it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.315045714378357, 'eval_runtime': 26.3314, 'eval_samples_per_second': 3.798, 'eval_steps_per_second': 1.291, 'epoch': 2.84, 'num_input_tokens_seen': 1179248}\n",
            " 95% 160/168 [19:42<00:37,  4.67s/it]\n",
            "100% 34/34 [00:25<00:00,  1.51it/s]\u001b[A\n",
            "{'loss': 0.6915, 'grad_norm': 1.169007658958435, 'learning_rate': 3.9329624554584884e-08, 'epoch': 2.93, 'num_input_tokens_seen': 1218128}\n",
            "100% 168/168 [20:22<00:00,  5.80s/it][INFO|trainer.py:3478] 2024-07-25 14:32:45,447 >> Saving model checkpoint to /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168\n",
            "[INFO|configuration_utils.py:472] 2024-07-25 14:32:45,479 >> Configuration saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168/config.json\n",
            "[INFO|configuration_utils.py:769] 2024-07-25 14:32:45,485 >> Configuration saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168/generation_config.json\n",
            "[INFO|modeling_utils.py:2698] 2024-07-25 14:34:20,811 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-25 14:34:21,215 >> tokenizer config file saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-25 14:34:21,229 >> Special tokens file saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/checkpoint-168/special_tokens_map.json\n",
            "[INFO|trainer.py:2383] 2024-07-25 14:34:42,586 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1339.9325, 'train_samples_per_second': 2.015, 'train_steps_per_second': 0.125, 'train_loss': 0.9229277216252827, 'epoch': 2.99, 'num_input_tokens_seen': 1242048}\n",
            "100% 168/168 [22:19<00:00,  7.98s/it]\n",
            "[INFO|trainer.py:3478] 2024-07-25 14:34:42,616 >> Saving model checkpoint to /content/drive/MyDrive/9900/phi3/Freeze/train_batchz\n",
            "[INFO|configuration_utils.py:472] 2024-07-25 14:34:42,908 >> Configuration saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/config.json\n",
            "[INFO|configuration_utils.py:769] 2024-07-25 14:34:42,920 >> Configuration saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/generation_config.json\n",
            "[INFO|modeling_utils.py:2698] 2024-07-25 14:37:31,792 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2574] 2024-07-25 14:37:31,813 >> tokenizer config file saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2583] 2024-07-25 14:37:31,827 >> Special tokens file saved in /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9867\n",
            "  num_input_tokens_seen    =    1242048\n",
            "  total_flos               = 25836499GF\n",
            "  train_loss               =     0.9229\n",
            "  train_runtime            = 0:22:19.93\n",
            "  train_samples_per_second =      2.015\n",
            "  train_steps_per_second   =      0.125\n",
            "Figure saved at: /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/phi3/Freeze/train_batchz/training_eval_loss.png\n",
            "07/25/2024 14:37:33 - WARNING - llamafactory.extras.ploting - No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:3788] 2024-07-25 14:37:33,439 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:3790] 2024-07-25 14:37:33,439 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-25 14:37:33,439 >>   Batch size = 3\n",
            "100% 34/34 [00:23<00:00,  1.42it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9867\n",
            "  eval_loss               =     1.3153\n",
            "  eval_runtime            = 0:00:24.82\n",
            "  eval_samples_per_second =      4.028\n",
            "  eval_steps_per_second   =      1.369\n",
            "  num_input_tokens_seen   =    1242048\n",
            "[INFO|modelcard.py:449] 2024-07-25 14:37:58,306 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path /content/drive/MyDrive/9900/Freeze/train \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Freeze/eval_PubMedQA_val \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "id": "nNA5TpIt-g-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "si8ETglo017A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path /content/drive/MyDrive/9900/Freeze/train \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir saves/Phi3-4B-4k-Chat/Freeze/eval_MedQA_test_US \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "auWY5KYGDSW7",
        "outputId": "dfc5df2d-6c36-4b3a-b839-33fcccb8d5c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-18 04:38:17.218607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-18 04:38:17.218660: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-18 04:38:17.219941: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-18 04:38:17.227149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 04:38:18.426592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/18/2024 04:38:24 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2159] 2024-07-18 04:38:24,729 >> loading file tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2159] 2024-07-18 04:38:24,729 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2159] 2024-07-18 04:38:24,729 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2159] 2024-07-18 04:38:24,729 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2159] 2024-07-18 04:38:24,729 >> loading file tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-18 04:38:24,797 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/18/2024 04:38:24 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/18/2024 04:38:24 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "Generating train split: 1273 examples [00:00, 3392.88 examples/s]\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 427.29 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 123.93 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:731] 2024-07-18 04:38:28,312 >> loading configuration file /content/drive/MyDrive/9900/Freeze/train/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-18 04:38:28,318 >> loading configuration file /content/drive/MyDrive/9900/Freeze/train/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-18 04:38:28,319 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/9900/Freeze/train\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_phi3.Phi3Config\",\n",
            "    \"AutoModel\": \"modeling_phi3.Phi3ForCausalLM\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": false,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/18/2024 04:38:28 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3553] 2024-07-18 04:38:28,662 >> loading weights file /content/drive/MyDrive/9900/Freeze/train/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-18 04:38:28,665 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 04:38:28,666 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:07<00:00,  3.54s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-18 04:38:35,810 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-18 04:38:35,810 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at /content/drive/MyDrive/9900/Freeze/train.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:953] 2024-07-18 04:38:35,815 >> loading configuration file /content/drive/MyDrive/9900/Freeze/train/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 04:38:35,815 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/18/2024 04:38:35 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/18/2024 04:38:35 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-18 04:38:35,888 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-18 04:38:35,889 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-18 04:38:35,889 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-18 04:38:35,949 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 34/34 [00:57<00:00,  1.34s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.622 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [00:58<00:00,  1.71s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    63.6434\n",
            "  predict_rouge-1            =    67.6818\n",
            "  predict_rouge-2            =    56.6668\n",
            "  predict_rouge-l            =    67.3775\n",
            "  predict_runtime            = 0:01:01.24\n",
            "  predict_samples_per_second =      1.633\n",
            "  predict_steps_per_second   =      0.555\n",
            "07/18/2024 04:39:37 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Phi3-4B-4k-Chat/Freeze/eval_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset MedQA_test_US \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Original_phi/eval_MedQA_test_US \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "W6ytzUWhD1g6",
        "outputId": "dbc9dbaa-a343-4a02-d195-cdc1efc30a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-18 04:47:51.973510: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-18 04:47:51.973557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-18 04:47:51.974867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-18 04:47:51.981913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 04:47:53.120273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/18/2024 04:47:59 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 04:47:59,436 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 04:47:59,436 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 04:47:59,436 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 04:47:59,436 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 04:47:59,436 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-18 04:47:59,512 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/18/2024 04:47:59 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/18/2024 04:47:59 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/18/2024 04:47:59 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/test.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 119.78 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 20183, 14219, 459, 29874, 7486, 25300, 708, 20201, 338, 1614, 1259, 263, 1559, 7830, 26086, 26032, 411, 278, 14311, 28942, 408, 278, 1098, 2548, 4824, 8910, 29889, 7133, 278, 1206, 29892, 278, 20201, 297, 328, 1765, 2705, 5700, 29879, 263, 8525, 272, 10331, 265, 29889, 450, 10331, 265, 338, 1634, 29874, 2859, 1728, 752, 1414, 29889, 450, 1098, 2548, 10603, 278, 20201, 393, 278, 16500, 674, 437, 2691, 29892, 322, 727, 338, 694, 817, 304, 3461, 445, 9461, 752, 1414, 393, 674, 451, 10311, 278, 16500, 29892, 408, 540, 947, 451, 864, 304, 1207, 278, 16500, 15982, 17128, 6275, 29889, 940, 10603, 278, 20201, 304, 5967, 445, 752, 1414, 714, 310, 278, 1751, 1230, 3461, 29889, 8449, 310, 278, 1494, 338, 278, 1959, 2446, 3158, 363, 278, 20201, 304, 2125, 14579, 319, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 541, 5967, 372, 714, 310, 278, 1751, 1230, 3461, 29892, 350, 29901, 3295, 5358, 278, 1059, 304, 278, 16500, 322, 1925, 372, 297, 278, 1751, 1230, 3461, 29892, 315, 29901, 24948, 278, 1098, 2548, 393, 540, 2609, 4418, 304, 2313, 2226, 445, 10171, 29892, 360, 29901, 13969, 278, 4824, 8910, 304, 278, 11314, 1199, 21118, 29892, 382, 29901, 9897, 1509, 304, 9657, 403, 278, 1751, 1230, 3461, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A junior orthopaedic surgery resident is completing a carpal tunnel repair with the department chairman as the attending physician. During the case, the resident inadvertently cuts a flexor tendon. The tendon is repaired without complication. The attending tells the resident that the patient will do fine, and there is no need to report this minor complication that will not harm the patient, as he does not want to make the patient worry unnecessarily. He tells the resident to leave this complication out of the operative report. Which of the following is the correct next action for the resident to take?, A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-18 04:48:01,150 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-18 04:48:01,430 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-18 04:48:01,431 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/18/2024 04:48:01 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-18 04:48:01,628 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-18 04:48:01,629 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 04:48:01,630 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:32<00:00, 16.27s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-18 04:48:34,231 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-18 04:48:34,231 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-18 04:48:34,334 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 04:48:34,334 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/18/2024 04:48:34 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/18/2024 04:48:34 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-18 04:48:34,405 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-18 04:48:34,405 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-18 04:48:34,405 >>   Batch size = 3\n",
            "[WARNING|logging.py:328] 2024-07-18 04:48:34,482 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 34/34 [41:13<00:00, 53.94s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.634 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 34/34 [41:14<00:00, 72.78s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =     3.3428\n",
            "  predict_rouge-1            =     10.414\n",
            "  predict_rouge-2            =     4.9317\n",
            "  predict_rouge-l            =     6.3776\n",
            "  predict_runtime            = 0:42:44.24\n",
            "  predict_samples_per_second =      0.039\n",
            "  predict_steps_per_second   =      0.013\n",
            "07/18/2024 05:31:18 - INFO - llamafactory.train.sft.trainer - Saving prediction results to saves/Phi3-4B-4k-Chat/Original_phi/eval_MedQA_test_US/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --eval_dataset PubMedQA_val \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --max_samples 100 \\\n",
        "    --per_device_eval_batch_size 8 \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --top_p 0.7 \\\n",
        "    --temperature 0.95 \\\n",
        "    --output_dir /content/drive/MyDrive/9900/Original_phi/eval_PubMedQA_val \\\n",
        "    --do_predict True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpziaMTED1i9",
        "outputId": "04e58cb1-92c3-4742-8589-fb081b23a044",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-18 05:36:14.057678: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-18 05:36:14.057724: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-18 05:36:14.058953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-18 05:36:14.065575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 05:36:15.209082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/18/2024 05:36:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 05:36:21,512 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 05:36:21,512 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 05:36:21,512 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 05:36:21,512 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2161] 2024-07-18 05:36:21,512 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:313] 2024-07-18 05:36:21,576 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/18/2024 05:36:21 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/18/2024 05:36:21 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/18/2024 05:36:21 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_test_set.json...\n",
            "/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "Converting format of dataset (num_proc=16): 100% 100/100 [00:00<00:00, 408.82 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 100/100 [00:00<00:00, 118.00 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 4768, 27067, 936, 5925, 1139, 2183, 278, 1139, 322, 3030, 29879, 13, 14130, 2725, 29901, 5538, 278, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 7853, 2175, 9736, 275, 9085, 23351, 10794, 29973, 13, 6007, 4330, 29990, 9375, 29901, 450, 3086, 5827, 2667, 310, 15202, 624, 10946, 2522, 744, 313, 12916, 29950, 1799, 29897, 338, 263, 2854, 29892, 9483, 15520, 6287, 393, 15366, 452, 2192, 1188, 936, 822, 293, 277, 29889, 4587, 29871, 29946, 29906, 1950, 3291, 29892, 29871, 29955, 3291, 526, 4153, 4475, 304, 20039, 310, 4086, 9401, 411, 871, 29871, 29906, 3291, 4475, 304, 22851, 29889, 1334, 4392, 1312, 278, 2058, 833, 5075, 310, 278, 405, 1177, 8452, 260, 29899, 7228, 19782, 14260, 304, 1243, 278, 20051, 393, 278, 3001, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 1492, 9736, 275, 9085, 23351, 10794, 723, 367, 7621, 1135, 278, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 297, 22069, 411, 2175, 9736, 275, 9085, 23351, 10794, 1058, 505, 2788, 405, 29902, 29950, 1799, 19435, 29889, 450, 7977, 310, 19782, 471, 10087, 491, 6601, 1891, 1967, 7418, 310, 26637, 12298, 322, 26637, 4558, 6087, 373, 6601, 260, 4085, 322, 27070, 766, 2039, 29889, 315, 4003, 29899, 4632, 13852, 310, 966, 291, 7977, 471, 8560, 363, 1269, 26637, 29889, 4103, 15628, 966, 291, 7977, 471, 29537, 287, 297, 263, 1480, 4695, 17855, 1904, 304, 8500, 7977, 310, 19782, 491, 405, 29902, 29950, 1799, 8158, 363, 1269, 9736, 275, 9085, 29889, 5013, 279, 1171, 7115, 19869, 471, 1304, 304, 8161, 278, 8220, 1546, 278, 405, 29902, 29950, 1799, 8158, 322, 966, 291, 7977, 29889, 450, 7977, 363, 1492, 9736, 275, 9085, 19782, 471, 12997, 1711, 7621, 1135, 278, 7977, 363, 2175, 9736, 275, 9085, 23351, 10794, 29892, 10365, 292, 363, 278, 2362, 5570, 405, 29902, 29950, 1799, 313, 29925, 29966, 29900, 29889, 29871, 29900, 29900, 29896, 467, 1152, 1269, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 8158, 29966, 29906, 29900, 29892, 278, 19194, 7977, 310, 1492, 9736, 275, 9085, 23351, 10794, 471, 14235, 3765, 278, 19194, 7977, 310, 2175, 9736, 275, 9085, 23351, 10794, 29889, 1152, 1342, 29892, 363, 22069, 411, 263, 2175, 9736, 275, 9085, 19782, 322, 263, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 310, 29871, 29896, 29953, 304, 29871, 29906, 29900, 29892, 278, 19194, 7977, 310, 274, 406, 1182, 284, 3041, 279, 428, 471, 29871, 29946, 29947, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29896, 29946, 304, 29871, 29896, 29896, 29896, 286, 29931, 29897, 408, 9401, 411, 29871, 29896, 29941, 29941, 286, 29931, 313, 1639, 339, 442, 488, 3464, 29871, 29947, 29896, 304, 29871, 29906, 29900, 29947, 286, 29931, 29897, 363, 22069, 411, 263, 1492, 9736, 275, 9085, 19782, 313, 29925, 29966, 29900, 29889, 29900, 29900, 29896, 467, 450, 19194, 7977, 310, 263, 1492, 9736, 275, 9085, 19782, 471, 20928, 5186, 304, 278, 19194, 7977, 310, 263, 2175, 9736, 275, 9085, 19782, 297, 278, 2446, 9939, 29871, 29945, 29899, 3149, 7663, 310, 278, 405, 29902, 29950, 1799, 29889, 450, 5013, 279, 1171, 7115, 19869, 1546, 278, 29871, 29906, 29946, 29899, 18721, 405, 29902, 29950, 1799, 8158, 322, 29871, 29941, 29899, 10874, 966, 291, 7977, 471, 29871, 29900, 29889, 29955, 29906, 363, 22069, 411, 2175, 9736, 275, 9085, 19782, 322, 29871, 29900, 29889, 29955, 29896, 363, 22069, 411, 1492, 9736, 275, 9085, 19782, 29889, 32007, 29871, 13, 32001]\n",
            "inputs:\n",
            "<s><|user|> Answer the biomedical research question given the question and contexts\n",
            "QUESTION: Does the National Institutes of Health Stroke Scale favor left hemisphere strokes?\n",
            "CONTEXTS: The National Institutes of Health Stroke Scale (NIHSS) is a valid, reproducible scale that measures neurological deficit. Of 42 possible points, 7 points are directly related to measurement of language compared with only 2 points related to neglect. We examined the placebo arm of the NINDS t-PA stroke trial to test the hypothesis that the total volume of cerebral infarction in patients with right hemisphere strokes would be greater than the volume of cerebral infarction in patients with left hemisphere strokes who have similar NIHSS scores. The volume of stroke was determined by computerized image analysis of CT films and CT images stored on computer tape and optical disks. Cube-root transformation of lesion volume was performed for each CT. Transformed lesion volume was analyzed in a logistic regression model to predict volume of stroke by NIHSS score for each hemisphere. Spearman rank correlation was used to determine the relation between the NIHSS score and lesion volume. The volume for right hemisphere stroke was statistically greater than the volume for left hemisphere strokes, adjusting for the baseline NIHSS (P<0. 001). For each 5-point category of the NIHSS score<20, the median volume of right hemisphere strokes was approximately double the median volume of left hemisphere strokes. For example, for patients with a left hemisphere stroke and a 24-hour NIHSS score of 16 to 20, the median volume of cerebral infarction was 48 mL (interquartile range 14 to 111 mL) as compared with 133 mL (interquartile range 81 to 208 mL) for patients with a right hemisphere stroke (P<0.001). The median volume of a right hemisphere stroke was roughly equal to the median volume of a left hemisphere stroke in the next highest 5-point category of the NIHSS. The Spearman rank correlation between the 24-hour NIHSS score and 3-month lesion volume was 0.72 for patients with left hemisphere stroke and 0.71 for patients with right hemisphere stroke.<|end|> \n",
            "<|assistant|>\n",
            "[INFO|configuration_utils.py:733] 2024-07-18 05:36:23,744 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-18 05:36:24,555 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:800] 2024-07-18 05:36:24,557 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.42.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "07/18/2024 05:36:24 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3556] 2024-07-18 05:36:24,692 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1531] 2024-07-18 05:36:24,692 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 05:36:24,693 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.50s/it]\n",
            "[INFO|modeling_utils.py:4364] 2024-07-18 05:36:27,750 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4372] 2024-07-18 05:36:27,750 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:955] 2024-07-18 05:36:27,852 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:1000] 2024-07-18 05:36:27,853 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/18/2024 05:36:27 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/18/2024 05:36:27 - INFO - llamafactory.model.loader - all params: 3,821,079,552\n",
            "[INFO|trainer.py:3788] 2024-07-18 05:36:27,923 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:3790] 2024-07-18 05:36:27,923 >>   Num examples = 100\n",
            "[INFO|trainer.py:3793] 2024-07-18 05:36:27,923 >>   Batch size = 8\n",
            "[WARNING|logging.py:328] 2024-07-18 05:36:28,001 >> The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "100% 13/13 [28:10<00:00, 115.86s/it]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.634 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "100% 13/13 [28:12<00:00, 130.20s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4             =    20.0794\n",
            "  predict_rouge-1            =    29.0438\n",
            "  predict_rouge-2            =     8.8005\n",
            "  predict_rouge-l            =    15.9426\n",
            "  predict_runtime            = 0:30:51.50\n",
            "  predict_samples_per_second =      0.054\n",
            "  predict_steps_per_second   =      0.007\n",
            "07/18/2024 06:07:19 - INFO - llamafactory.train.sft.trainer - Saving prediction results to /content/drive/MyDrive/9900/Original_phi/eval_PubMedQA_val/generated_predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "amXzzSSfD1lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vbBlV6iGD1nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3inRhDupD1pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_max_500 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4Fc-MkTqJRN8",
        "outputId": "8932f5de-e0b2-453d-fdc7-744fd02bd416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:01:12.461454: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:01:12.514615: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:01:12.514662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:01:12.516220: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:01:12.525049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:01:13.762143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:01:19 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:01:19,752 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:01:19,753 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:01:19,753 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:01:19,753 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:01:19,753 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:01:19,832 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:01:19 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:01:19 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:01:19 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:01:20 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:01:21,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:01:22,349 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:01:22,350 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:01:22,675 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:01:22,675 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:01:22,676 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.50s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:01:27,746 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:01:27,746 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:01:27,990 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:01:27,990 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:01:27 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:01:27 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:01:27 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:01:27 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:01:28 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:01:28 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:01:28,037 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:01:29,161 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:01:29,161 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:01:29,161 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:01:29,161 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:01:29,161 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:01:29,161 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:01:29,161 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:01:29,162 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/54 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.5308, 'grad_norm': 1.1609392166137695, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.3542, 'grad_norm': 1.0540406703948975, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:23<01:42,  2.34s/it][INFO|trainer.py:3719] 2024-07-14 08:01:52,989 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:01:52,989 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:01:52,989 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.95it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.26it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.01it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.15it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.90it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.80it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.65it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.82it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.70it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.57it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.21it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.92it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.94it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.41it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.53it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.10it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.300437331199646, 'eval_accuracy': 0.7828000466145462, 'eval_runtime': 3.5908, 'eval_samples_per_second': 27.849, 'eval_steps_per_second': 9.469, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:27<01:42,  2.34s/it]\n",
            "100% 34/34 [00:03<00:00, 10.23it/s]\u001b[A\n",
            "{'loss': 1.2316, 'grad_norm': 0.8017536401748657, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.3269, 'grad_norm': 0.8991929292678833, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:50<01:20,  2.36s/it][INFO|trainer.py:3719] 2024-07-14 08:02:19,549 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:02:19,550 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:02:19,550 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.87it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.24it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.70it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.69it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.90it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.46it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.72it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2646162509918213, 'eval_accuracy': 0.7876328738927981, 'eval_runtime': 3.589, 'eval_samples_per_second': 27.863, 'eval_steps_per_second': 9.473, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:53<01:20,  2.36s/it]\n",
            "100% 34/34 [00:03<00:00, 10.38it/s]\u001b[A\n",
            "{'loss': 1.2274, 'grad_norm': 0.8117754459381104, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.1184, 'grad_norm': 0.672272801399231, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:17<00:57,  2.39s/it][INFO|trainer.py:3719] 2024-07-14 08:02:46,426 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:02:46,426 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:02:46,426 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.91it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.96it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.09it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.21it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.69it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.77it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.28it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.41it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.01it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.73it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2560014724731445, 'eval_accuracy': 0.7885586331779266, 'eval_runtime': 3.5963, 'eval_samples_per_second': 27.807, 'eval_steps_per_second': 9.454, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:20<00:57,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.37it/s]\u001b[A\n",
            "{'loss': 1.1, 'grad_norm': 0.801800549030304, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.2123, 'grad_norm': 0.8885489702224731, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:43<00:32,  2.30s/it][INFO|trainer.py:3719] 2024-07-14 08:03:12,694 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:03:12,694 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:03:12,694 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.93it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.31it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.05it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.16it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.73it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.79it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.64it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.81it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.70it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.57it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.22it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.92it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.51it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.09it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2546640634536743, 'eval_accuracy': 0.7872182217338284, 'eval_runtime': 3.5757, 'eval_samples_per_second': 27.966, 'eval_steps_per_second': 9.509, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:47<00:32,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.41it/s]\u001b[A\n",
            "{'loss': 1.1044, 'grad_norm': 0.8327363133430481, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 1.1094, 'grad_norm': 0.7460633516311646, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:10<00:09,  2.43s/it][INFO|trainer.py:3719] 2024-07-14 08:03:39,650 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:03:39,650 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:03:39,650 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.03it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.76it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.69it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.57it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.22it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.93it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.08it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2541685104370117, 'eval_accuracy': 0.7860643952625894, 'eval_runtime': 3.5781, 'eval_samples_per_second': 27.948, 'eval_steps_per_second': 9.502, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:14<00:09,  2.43s/it]\n",
            "100% 34/34 [00:03<00:00, 10.43it/s]\u001b[A\n",
            "100% 54/54 [02:22<00:00,  2.58s/it][INFO|trainer.py:2329] 2024-07-14 08:03:51,882 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7314, 'train_samples_per_second': 18.917, 'train_steps_per_second': 0.378, 'train_loss': 1.220452591224953, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [02:22<00:00,  2.64s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:03:51,898 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/train_max_500\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:03:51,918 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:03:51,922 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:04:24,661 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:04:24,666 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:04:28,666 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30654134GF\n",
            "  train_loss               =     1.2205\n",
            "  train_runtime            = 0:02:22.73\n",
            "  train_samples_per_second =     18.917\n",
            "  train_steps_per_second   =      0.378\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_max_500/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:04:29,150 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:04:29,150 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:04:29,151 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.88it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7862\n",
            "  eval_loss               =     1.2541\n",
            "  eval_runtime            = 0:00:03.60\n",
            "  eval_samples_per_second =     27.768\n",
            "  eval_steps_per_second   =      9.441\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:04:32,763 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7861578532065145}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 1000 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_max_1000 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-ocE97j9MHER",
        "outputId": "4a7c7364-f80d-4ab4-f3cf-ba2cf81ee760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:04:37.753292: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:04:37.804742: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:04:37.804786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:04:37.806160: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:04:37.813878: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:04:39.042195: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:04:44 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:04:45,043 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:04:45,043 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:04:45,043 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:04:45,043 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:04:45,043 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:04:45,115 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:04:45 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:04:45 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:04:45 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:04:45 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:04:47,159 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:04:47,670 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:04:47,671 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:04:47,957 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:04:47,958 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:04:47,959 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.68s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:04:53,407 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:04:53,407 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:04:53,644 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:04:53,644 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:04:53 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:04:53,700 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:04:54,839 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:04:54,839 >>   Num examples = 1,710\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:04:54,839 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:04:54,839 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:04:54,839 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:04:54,839 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:04:54,839 >>   Total optimization steps = 105\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:04:54,841 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/105 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.4734, 'grad_norm': 1.0815943479537964, 'learning_rate': 4.972077065562821e-05, 'epoch': 0.14, 'num_input_tokens_seen': 140256}\n",
            "{'loss': 1.2981, 'grad_norm': 1.0037301778793335, 'learning_rate': 4.888932014465352e-05, 'epoch': 0.28, 'num_input_tokens_seen': 271680}\n",
            " 10% 10/105 [00:23<03:36,  2.28s/it][INFO|trainer.py:3719] 2024-07-14 08:05:18,577 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:05:18,577 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:05:18,577 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.79it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.22it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.05it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.47it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:06,  8.83it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  8.89it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.69it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.78it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.55it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.68it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.14it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.44it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.51it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.57it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.88it/s]\u001b[A\n",
            " 44% 28/64 [00:03<00:03,  9.14it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.05it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:03, 10.41it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.26it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.71it/s]\u001b[A\n",
            " 58% 37/64 [00:03<00:02,  9.61it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.13it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.12it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  8.81it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.23it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.00it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.27it/s]\u001b[A\n",
            " 72% 46/64 [00:05<00:02,  8.50it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.79it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.87it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.46it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.24it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 10.94it/s]\u001b[A\n",
            " 89% 57/64 [00:06<00:00,  9.56it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.20it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.81it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.211934208869934, 'eval_accuracy': 0.802057790774709, 'eval_runtime': 6.9136, 'eval_samples_per_second': 27.482, 'eval_steps_per_second': 9.257, 'epoch': 0.28, 'num_input_tokens_seen': 271680}\n",
            " 10% 10/105 [00:30<03:36,  2.28s/it]\n",
            "100% 64/64 [00:06<00:00,  9.02it/s]\u001b[A\n",
            "{'loss': 1.2714, 'grad_norm': 0.9538838863372803, 'learning_rate': 4.752422169756048e-05, 'epoch': 0.42, 'num_input_tokens_seen': 406848}\n",
            "{'loss': 1.318, 'grad_norm': 0.9180609583854675, 'learning_rate': 4.5655969357899874e-05, 'epoch': 0.56, 'num_input_tokens_seen': 544272}\n",
            " 19% 20/105 [00:53<03:26,  2.43s/it][INFO|trainer.py:3719] 2024-07-14 08:05:48,480 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:05:48,480 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:05:48,480 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.77it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.44it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.12it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.53it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.88it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  8.81it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  8.82it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.62it/s]\u001b[A\n",
            " 23% 15/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.68it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.45it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.80it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.24it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.50it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.54it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.56it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.87it/s]\u001b[A\n",
            " 44% 28/64 [00:03<00:03,  9.34it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.33it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:03, 10.49it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.28it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.76it/s]\u001b[A\n",
            " 58% 37/64 [00:03<00:02,  9.78it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.18it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  8.84it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.30it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  7.78it/s]\u001b[A\n",
            " 72% 46/64 [00:05<00:02,  8.07it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:02,  8.48it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.65it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.35it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.11it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.01it/s]\u001b[A\n",
            " 89% 57/64 [00:06<00:00,  9.66it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.26it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.85it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1839128732681274, 'eval_accuracy': 0.8032323215086102, 'eval_runtime': 6.9099, 'eval_samples_per_second': 27.497, 'eval_steps_per_second': 9.262, 'epoch': 0.56, 'num_input_tokens_seen': 544272}\n",
            " 19% 20/105 [01:00<03:26,  2.43s/it]\n",
            "100% 64/64 [00:06<00:00,  9.07it/s]\u001b[A\n",
            "{'loss': 1.3439, 'grad_norm': 0.9016620516777039, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.7, 'num_input_tokens_seen': 685008}\n",
            "{'loss': 1.1891, 'grad_norm': 0.8492844700813293, 'learning_rate': 4.058724504646834e-05, 'epoch': 0.84, 'num_input_tokens_seen': 822864}\n",
            " 29% 30/105 [01:24<03:03,  2.45s/it][INFO|trainer.py:3719] 2024-07-14 08:06:19,029 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:06:19,029 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:06:19,029 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.72it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.42it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.16it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.54it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.88it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.17it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.70it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.86it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.25it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.53it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.63it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.92it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.37it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.76it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.47it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.91it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.65it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.33it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.42it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.41it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.60it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.92it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.94it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.51it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.27it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.33it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.81it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.38it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.93it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1798300743103027, 'eval_accuracy': 0.8026835768672649, 'eval_runtime': 6.802, 'eval_samples_per_second': 27.933, 'eval_steps_per_second': 9.409, 'epoch': 0.84, 'num_input_tokens_seen': 822864}\n",
            " 29% 30/105 [01:30<03:03,  2.45s/it]\n",
            "100% 64/64 [00:06<00:00,  9.14it/s]\u001b[A\n",
            "{'loss': 1.2776, 'grad_norm': 0.8297629356384277, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.98, 'num_input_tokens_seen': 965232}\n",
            "{'loss': 1.0926, 'grad_norm': 0.808607816696167, 'learning_rate': 3.413352560915988e-05, 'epoch': 1.12, 'num_input_tokens_seen': 1103904}\n",
            " 38% 40/105 [01:54<02:36,  2.41s/it][INFO|trainer.py:3719] 2024-07-14 08:06:49,592 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:06:49,592 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:06:49,592 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.71it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.42it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.16it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.54it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.87it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.15it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.84it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.90it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.68it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.83it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.22it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.50it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.60it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.61it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.91it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.34it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.41it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.73it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.47it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.90it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.63it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.31it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.03it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.41it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.40it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.59it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.90it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.24it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.10it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01,  9.95it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.08it/s]\u001b[A\n",
            " 89% 57/64 [00:06<00:00,  9.68it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.27it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.87it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1742044687271118, 'eval_accuracy': 0.8063412699065988, 'eval_runtime': 6.8459, 'eval_samples_per_second': 27.754, 'eval_steps_per_second': 9.349, 'epoch': 1.12, 'num_input_tokens_seen': 1103904}\n",
            " 38% 40/105 [02:01<02:36,  2.41s/it]\n",
            "100% 64/64 [00:06<00:00,  9.12it/s]\u001b[A\n",
            "{'loss': 1.1858, 'grad_norm': 0.8811272978782654, 'learning_rate': 3.056302334890786e-05, 'epoch': 1.26, 'num_input_tokens_seen': 1246272}\n",
            "{'loss': 1.1052, 'grad_norm': 0.8659697771072388, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.4, 'num_input_tokens_seen': 1382640}\n",
            " 48% 50/105 [02:25<02:12,  2.42s/it][INFO|trainer.py:3719] 2024-07-14 08:07:19,994 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:07:19,994 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:07:19,994 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.76it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.44it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.17it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.55it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.88it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.17it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.13it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.69it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.84it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.23it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.52it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.62it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.92it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.36it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.77it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.48it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.90it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.65it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.34it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.43it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.17it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.43it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.62it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.92it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.95it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.52it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.27it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.33it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.81it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.37it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.93it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1743860244750977, 'eval_accuracy': 0.807637852788026, 'eval_runtime': 6.802, 'eval_samples_per_second': 27.933, 'eval_steps_per_second': 9.409, 'epoch': 1.4, 'num_input_tokens_seen': 1382640}\n",
            " 48% 50/105 [02:31<02:12,  2.42s/it]\n",
            "100% 64/64 [00:06<00:00,  9.13it/s]\u001b[A\n",
            "{'loss': 1.069, 'grad_norm': 0.7484283447265625, 'learning_rate': 2.3131747660339394e-05, 'epoch': 1.54, 'num_input_tokens_seen': 1509024}\n",
            "{'loss': 1.0949, 'grad_norm': 0.7964583039283752, 'learning_rate': 1.9436976651092144e-05, 'epoch': 1.68, 'num_input_tokens_seen': 1646304}\n",
            " 57% 60/105 [02:54<01:48,  2.41s/it][INFO|trainer.py:3719] 2024-07-14 08:07:48,937 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:07:48,937 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:07:48,937 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.52it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.33it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.11it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.52it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:06,  8.87it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.15it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.69it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.86it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.25it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.54it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.63it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.92it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.36it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.76it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.47it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.91it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.64it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.33it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.42it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.41it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.60it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.91it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.92it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.50it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.25it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.31it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.80it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.36it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.93it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1765695810317993, 'eval_accuracy': 0.8046872889491749, 'eval_runtime': 6.8059, 'eval_samples_per_second': 27.917, 'eval_steps_per_second': 9.404, 'epoch': 1.68, 'num_input_tokens_seen': 1646304}\n",
            " 57% 60/105 [03:00<01:48,  2.41s/it]\n",
            "100% 64/64 [00:06<00:00,  9.15it/s]\u001b[A\n",
            "{'loss': 1.1028, 'grad_norm': 0.7425421476364136, 'learning_rate': 1.5866474390840125e-05, 'epoch': 1.82, 'num_input_tokens_seen': 1789968}\n",
            "{'loss': 1.1784, 'grad_norm': 0.7975205183029175, 'learning_rate': 1.2500000000000006e-05, 'epoch': 1.96, 'num_input_tokens_seen': 1927824}\n",
            " 67% 70/105 [03:24<01:24,  2.41s/it][INFO|trainer.py:3719] 2024-07-14 08:08:19,486 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:08:19,486 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:08:19,486 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.70it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.40it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.15it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.53it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.88it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.16it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.68it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.84it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.24it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.52it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.62it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.92it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.36it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.76it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.48it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.90it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.63it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.30it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.03it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.41it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.41it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.60it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.91it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.94it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.52it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.29it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.35it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.82it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.37it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.93it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1775418519973755, 'eval_accuracy': 0.8053444394027602, 'eval_runtime': 6.8056, 'eval_samples_per_second': 27.918, 'eval_steps_per_second': 9.404, 'epoch': 1.96, 'num_input_tokens_seen': 1927824}\n",
            " 67% 70/105 [03:31<01:24,  2.41s/it]\n",
            "100% 64/64 [00:06<00:00,  9.14it/s]\u001b[A\n",
            "{'loss': 0.9569, 'grad_norm': 0.7968747615814209, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.11, 'num_input_tokens_seen': 2057616}\n",
            "{'loss': 1.0534, 'grad_norm': 0.7243838906288147, 'learning_rate': 6.673703204254347e-06, 'epoch': 2.25, 'num_input_tokens_seen': 2195040}\n",
            " 76% 80/105 [03:53<00:58,  2.36s/it][INFO|trainer.py:3719] 2024-07-14 08:08:48,791 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:08:48,791 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:08:48,791 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.77it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.42it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.16it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.53it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.87it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.16it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.11it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.83it/s]\u001b[A\n",
            " 23% 15/64 [00:01<00:05,  9.09it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.83it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.59it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.94it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.34it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.55it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.60it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.59it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.89it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.35it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.43it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.76it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.48it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.92it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.64it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.33it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.42it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.16it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.41it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.60it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.91it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.93it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.51it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.25it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.32it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.80it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.35it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.91it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1781684160232544, 'eval_accuracy': 0.8066758752424086, 'eval_runtime': 6.8098, 'eval_samples_per_second': 27.901, 'eval_steps_per_second': 9.398, 'epoch': 2.25, 'num_input_tokens_seen': 2195040}\n",
            " 76% 80/105 [04:00<00:58,  2.36s/it]\n",
            "100% 64/64 [00:06<00:00,  9.13it/s]\u001b[A\n",
            "{'loss': 1.0609, 'grad_norm': 0.7859824299812317, 'learning_rate': 4.344030642100133e-06, 'epoch': 2.39, 'num_input_tokens_seen': 2330544}\n",
            "{'loss': 1.1028, 'grad_norm': 0.7450571656227112, 'learning_rate': 2.475778302439524e-06, 'epoch': 2.53, 'num_input_tokens_seen': 2464800}\n",
            " 86% 90/105 [04:23<00:35,  2.35s/it][INFO|trainer.py:3719] 2024-07-14 08:09:18,349 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:09:18,349 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:09:18,349 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.67it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.37it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.13it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.51it/s]\u001b[A\n",
            " 16% 10/64 [00:01<00:06,  8.85it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.13it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.09it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.82it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.88it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.67it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.82it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.23it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.52it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.63it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.91it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.36it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.44it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.76it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.48it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.91it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.65it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.34it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.06it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.42it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.16it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.42it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.61it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.91it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.93it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.51it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.26it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.33it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.82it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.37it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.93it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.1796194314956665, 'eval_accuracy': 0.8050442883495186, 'eval_runtime': 6.8076, 'eval_samples_per_second': 27.91, 'eval_steps_per_second': 9.401, 'epoch': 2.53, 'num_input_tokens_seen': 2464800}\n",
            " 86% 90/105 [04:30<00:35,  2.35s/it]\n",
            "100% 64/64 [00:06<00:00,  9.14it/s]\u001b[A\n",
            "{'loss': 1.0835, 'grad_norm': 0.7489157915115356, 'learning_rate': 1.1106798553464804e-06, 'epoch': 2.67, 'num_input_tokens_seen': 2605776}\n",
            "{'loss': 1.0743, 'grad_norm': 0.8008033037185669, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.81, 'num_input_tokens_seen': 2747280}\n",
            " 95% 100/105 [04:54<00:12,  2.49s/it][INFO|trainer.py:3719] 2024-07-14 08:09:49,003 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:09:49,003 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:09:49,003 >>   Batch size = 3\n",
            "\n",
            "  0% 0/64 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 3/64 [00:00<00:03, 16.75it/s]\u001b[A\n",
            "  8% 5/64 [00:00<00:04, 13.43it/s]\u001b[A\n",
            " 11% 7/64 [00:00<00:05, 10.15it/s]\u001b[A\n",
            " 14% 9/64 [00:00<00:05,  9.54it/s]\u001b[A\n",
            " 16% 10/64 [00:00<00:06,  8.88it/s]\u001b[A\n",
            " 19% 12/64 [00:01<00:05,  9.17it/s]\u001b[A\n",
            " 20% 13/64 [00:01<00:05,  9.12it/s]\u001b[A\n",
            " 22% 14/64 [00:01<00:05,  8.85it/s]\u001b[A\n",
            " 25% 16/64 [00:01<00:05,  8.91it/s]\u001b[A\n",
            " 27% 17/64 [00:01<00:05,  8.70it/s]\u001b[A\n",
            " 30% 19/64 [00:01<00:04,  9.85it/s]\u001b[A\n",
            " 33% 21/64 [00:02<00:04, 10.25it/s]\u001b[A\n",
            " 36% 23/64 [00:02<00:04,  9.53it/s]\u001b[A\n",
            " 38% 24/64 [00:02<00:04,  8.62it/s]\u001b[A\n",
            " 39% 25/64 [00:02<00:05,  7.63it/s]\u001b[A\n",
            " 41% 26/64 [00:02<00:04,  7.92it/s]\u001b[A\n",
            " 44% 28/64 [00:02<00:03,  9.37it/s]\u001b[A\n",
            " 47% 30/64 [00:03<00:03,  9.45it/s]\u001b[A\n",
            " 50% 32/64 [00:03<00:02, 10.77it/s]\u001b[A\n",
            " 53% 34/64 [00:03<00:02, 10.48it/s]\u001b[A\n",
            " 56% 36/64 [00:03<00:02,  9.91it/s]\u001b[A\n",
            " 59% 38/64 [00:04<00:03,  8.64it/s]\u001b[A\n",
            " 62% 40/64 [00:04<00:02,  9.33it/s]\u001b[A\n",
            " 64% 41/64 [00:04<00:02,  9.05it/s]\u001b[A\n",
            " 67% 43/64 [00:04<00:02,  9.42it/s]\u001b[A\n",
            " 69% 44/64 [00:04<00:02,  9.15it/s]\u001b[A\n",
            " 70% 45/64 [00:04<00:02,  8.41it/s]\u001b[A\n",
            " 72% 46/64 [00:04<00:02,  8.60it/s]\u001b[A\n",
            " 73% 47/64 [00:05<00:01,  8.89it/s]\u001b[A\n",
            " 77% 49/64 [00:05<00:01,  9.91it/s]\u001b[A\n",
            " 80% 51/64 [00:05<00:01,  9.49it/s]\u001b[A\n",
            " 83% 53/64 [00:05<00:01, 10.26it/s]\u001b[A\n",
            " 86% 55/64 [00:05<00:00, 11.32it/s]\u001b[A\n",
            " 89% 57/64 [00:05<00:00,  9.79it/s]\u001b[A\n",
            " 92% 59/64 [00:06<00:00, 10.35it/s]\u001b[A\n",
            " 95% 61/64 [00:06<00:00,  9.91it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.1800681352615356, 'eval_accuracy': 0.805839262213579, 'eval_runtime': 6.8054, 'eval_samples_per_second': 27.919, 'eval_steps_per_second': 9.404, 'epoch': 2.81, 'num_input_tokens_seen': 2747280}\n",
            " 95% 100/105 [05:00<00:12,  2.49s/it]\n",
            "100% 64/64 [00:06<00:00,  9.13it/s]\u001b[A\n",
            "{'loss': 1.0454, 'grad_norm': 0.7392283082008362, 'learning_rate': 0.0, 'epoch': 2.95, 'num_input_tokens_seen': 2885232}\n",
            "100% 105/105 [05:12<00:00,  2.86s/it][INFO|trainer.py:2329] 2024-07-14 08:10:07,404 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 312.5631, 'train_samples_per_second': 16.413, 'train_steps_per_second': 0.336, 'train_loss': 1.1608295985630581, 'epoch': 2.95, 'num_input_tokens_seen': 2885232}\n",
            "100% 105/105 [05:12<00:00,  2.98s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:10:07,411 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:10:07,431 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:10:07,435 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:10:36,639 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:10:36,644 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:10:36,648 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =     2.9474\n",
            "  num_input_tokens_seen    =    2885232\n",
            "  total_flos               = 60017242GF\n",
            "  train_loss               =     1.1608\n",
            "  train_runtime            = 0:05:12.56\n",
            "  train_samples_per_second =     16.413\n",
            "  train_steps_per_second   =      0.336\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/freeze/train_max_1000/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:10:40,528 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:10:40,528 >>   Num examples = 190\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:10:40,528 >>   Batch size = 3\n",
            "100% 64/64 [00:06<00:00,  9.60it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9474\n",
            "  eval_accuracy           =     0.8059\n",
            "  eval_loss               =     1.1801\n",
            "  eval_runtime            = 0:00:06.79\n",
            "  eval_samples_per_second =     27.945\n",
            "  eval_steps_per_second   =      9.413\n",
            "  num_input_tokens_seen   =    2885232\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:10:47,342 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8058719448788646}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 1e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c6s-tqPgMHGT",
        "outputId": "6bf5934c-5c7e-4f43-d885-9e2fac712b63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:10:52.378055: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:10:52.429363: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:10:52.429410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:10:52.430855: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:10:52.438782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:10:53.677878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:10:59 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:11:00,127 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:11:00,127 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:11:00,127 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:11:00,127 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:11:00,127 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:11:00,220 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:11:00 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:11:00 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:11:00 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:11:01 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:11:02,326 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:11:02,796 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:11:02,797 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:11:03,079 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:11:03,080 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:11:03,081 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.52s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:11:08,174 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:11:08,174 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:11:08,427 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:11:08,427 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:11:08 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:11:08,476 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:11:09,580 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:11:09,580 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:11:09,580 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:11:09,580 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:11:09,580 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:11:09,580 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:11:09,580 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:11:09,581 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/54 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.4835, 'grad_norm': 1.010589838027954, 'learning_rate': 9.789947561577445e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.341, 'grad_norm': 0.9721996784210205, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:23<01:42,  2.32s/it][INFO|trainer.py:3719] 2024-07-14 08:11:33,340 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:11:33,340 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:11:33,340 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.78it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.21it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.13it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.75it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.76it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.52it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.18it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.88it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.89it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.08it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2709102630615234, 'eval_accuracy': 0.7835069433593305, 'eval_runtime': 3.5923, 'eval_samples_per_second': 27.837, 'eval_steps_per_second': 9.465, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:27<01:42,  2.32s/it]\n",
            "100% 34/34 [00:03<00:00, 10.41it/s]\u001b[A\n",
            "{'loss': 1.2175, 'grad_norm': 0.7952814102172852, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.2967, 'grad_norm': 0.7766717076301575, 'learning_rate': 6.980398830195785e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:50<01:20,  2.36s/it][INFO|trainer.py:3719] 2024-07-14 08:11:59,878 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:11:59,878 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:11:59,878 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.25it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.15it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  8.86it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  8.93it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  8.99it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  8.81it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.64it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.75it/s]\u001b[A\n",
            " 85% 29/34 [00:03<00:00,  9.27it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.42it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.02it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.73it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2559995651245117, 'eval_accuracy': 0.7867566363100135, 'eval_runtime': 3.6396, 'eval_samples_per_second': 27.476, 'eval_steps_per_second': 9.342, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:53<01:20,  2.36s/it]\n",
            "100% 34/34 [00:03<00:00, 10.37it/s]\u001b[A\n",
            "{'loss': 1.1256, 'grad_norm': 0.7590355277061462, 'learning_rate': 5.5804645706261514e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.0184, 'grad_norm': 0.6180357933044434, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:17<00:57,  2.39s/it][INFO|trainer.py:3719] 2024-07-14 08:12:26,830 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:12:26,831 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:12:26,831 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.18it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.95it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.58it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.09it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.21it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.92it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.08it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.77it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2583707571029663, 'eval_accuracy': 0.7857399810419959, 'eval_runtime': 3.5856, 'eval_samples_per_second': 27.889, 'eval_steps_per_second': 9.482, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:20<00:57,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.38it/s]\u001b[A\n",
            "{'loss': 0.9884, 'grad_norm': 0.7599381804466248, 'learning_rate': 2.7560040989976892e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.0636, 'grad_norm': 0.9637690186500549, 'learning_rate': 1.5687918106563326e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:43<00:32,  2.30s/it][INFO|trainer.py:3719] 2024-07-14 08:12:53,123 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:12:53,123 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:12:53,123 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.93it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.15it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.70it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.66it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.53it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.90it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.07it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2705636024475098, 'eval_accuracy': 0.7873210624747133, 'eval_runtime': 3.5849, 'eval_samples_per_second': 27.895, 'eval_steps_per_second': 9.484, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:47<00:32,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "{'loss': 0.9345, 'grad_norm': 0.7748917937278748, 'learning_rate': 6.698729810778065e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 0.9293, 'grad_norm': 0.7208929061889648, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:10<00:09,  2.43s/it][INFO|trainer.py:3719] 2024-07-14 08:13:20,111 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:13:20,111 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:13:20,111 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.94it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.29it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.03it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.13it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.70it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.76it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2765624523162842, 'eval_accuracy': 0.7853434557353384, 'eval_runtime': 3.5887, 'eval_samples_per_second': 27.865, 'eval_steps_per_second': 9.474, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:14<00:09,  2.43s/it]\n",
            "100% 34/34 [00:03<00:00, 10.29it/s]\u001b[A\n",
            "100% 54/54 [02:22<00:00,  2.59s/it][INFO|trainer.py:2329] 2024-07-14 08:13:32,379 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.8085, 'train_samples_per_second': 18.906, 'train_steps_per_second': 0.378, 'train_loss': 1.1221361381036263, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [02:22<00:00,  2.64s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:13:32,394 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:13:32,418 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:13:32,422 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:14:01,107 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:14:01,116 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:14:01,124 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30654134GF\n",
            "  train_loss               =     1.1221\n",
            "  train_runtime            = 0:02:22.80\n",
            "  train_samples_per_second =     18.906\n",
            "  train_steps_per_second   =      0.378\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-04/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:14:05,664 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:14:05,664 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:14:05,664 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.89it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7852\n",
            "  eval_loss               =     1.2767\n",
            "  eval_runtime            = 0:00:03.60\n",
            "  eval_samples_per_second =     27.738\n",
            "  eval_steps_per_second   =      9.431\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:14:09,282 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7851976224020052}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 1e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ApWahftiMHIN",
        "outputId": "a1101551-84b7-403d-d7ba-a5285948af67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:14:14.476343: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:14:14.529337: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:14:14.529382: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:14:14.530934: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:14:14.538891: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:14:15.837852: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:14:21 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:14:21,891 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:14:21,891 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:14:21,891 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:14:21,891 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:14:21,891 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:14:21,962 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:14:21 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:14:21 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:14:21 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:14:22 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:14:23,940 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:14:24,447 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:14:24,448 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:14:24,721 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:14:24,722 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:14:24,723 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.57s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:14:29,936 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:14:29,936 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:14:30,183 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:14:30,184 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:14:30 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:14:30,226 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:14:31,334 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:14:31,334 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:14:31,334 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:14:31,335 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:14:31,335 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:14:31,335 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:14:31,335 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:14:31,336 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/54 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6768, 'grad_norm': 1.6035410165786743, 'learning_rate': 9.789947561577445e-06, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.5084, 'grad_norm': 1.3572044372558594, 'learning_rate': 9.177439057064684e-06, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:23<01:42,  2.33s/it][INFO|trainer.py:3719] 2024-07-14 08:14:55,054 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:14:55,054 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:14:55,054 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.87it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.12it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.92it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.07it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.4303747415542603, 'eval_accuracy': 0.7655013310871673, 'eval_runtime': 3.5915, 'eval_samples_per_second': 27.843, 'eval_steps_per_second': 9.467, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:27<01:42,  2.33s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "{'loss': 1.3392, 'grad_norm': 1.1608043909072876, 'learning_rate': 8.213938048432697e-06, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.4366, 'grad_norm': 1.0009721517562866, 'learning_rate': 6.980398830195785e-06, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:50<01:20,  2.36s/it][INFO|trainer.py:3719] 2024-07-14 08:15:21,662 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:15:21,662 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:15:21,662 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.86it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.22it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.99it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.12it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.74it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.76it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.52it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.18it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.88it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.90it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.48it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.343410611152649, 'eval_accuracy': 0.7764647241755, 'eval_runtime': 3.5923, 'eval_samples_per_second': 27.837, 'eval_steps_per_second': 9.465, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:53<01:20,  2.36s/it]\n",
            "100% 34/34 [00:03<00:00, 10.38it/s]\u001b[A\n",
            "{'loss': 1.3863, 'grad_norm': 0.9932084679603577, 'learning_rate': 5.5804645706261515e-06, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.2597, 'grad_norm': 0.7804640531539917, 'learning_rate': 4.131759111665349e-06, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:17<00:57,  2.39s/it][INFO|trainer.py:3719] 2024-07-14 08:15:48,579 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:15:48,579 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:15:48,579 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.10it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.86it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.76it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.88it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.89it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.35it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.47it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.04it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.74it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3167823553085327, 'eval_accuracy': 0.7816195361159457, 'eval_runtime': 3.5891, 'eval_samples_per_second': 27.862, 'eval_steps_per_second': 9.473, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:20<00:57,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.37it/s]\u001b[A\n",
            "{'loss': 1.2574, 'grad_norm': 0.9141162633895874, 'learning_rate': 2.7560040989976894e-06, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.3918, 'grad_norm': 1.0068477392196655, 'learning_rate': 1.5687918106563326e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:43<00:32,  2.31s/it][INFO|trainer.py:3719] 2024-07-14 08:16:14,925 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:16:14,925 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:16:14,925 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.83it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.24it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.97it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.11it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.73it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.74it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.63it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.51it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.17it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.88it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.87it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.33it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.44it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.03it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.74it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3095544576644897, 'eval_accuracy': 0.7831457636313562, 'eval_runtime': 3.6068, 'eval_samples_per_second': 27.725, 'eval_steps_per_second': 9.427, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:47<00:32,  2.31s/it]\n",
            "100% 34/34 [00:03<00:00, 10.12it/s]\u001b[A\n",
            "{'loss': 1.29, 'grad_norm': 0.9618868231773376, 'learning_rate': 6.698729810778065e-07, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 1.3046, 'grad_norm': 0.8043785691261292, 'learning_rate': 1.3477564710088097e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:10<00:09,  2.43s/it][INFO|trainer.py:3719] 2024-07-14 08:16:41,929 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:16:41,929 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:16:41,929 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.88it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.24it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.96it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.58it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.09it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.67it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.57it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.75it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.18it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.90it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.48it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.05it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.75it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3080930709838867, 'eval_accuracy': 0.7830746513673317, 'eval_runtime': 3.5899, 'eval_samples_per_second': 27.856, 'eval_steps_per_second': 9.471, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:14<00:09,  2.43s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "100% 54/54 [02:22<00:00,  2.59s/it][INFO|trainer.py:2329] 2024-07-14 08:16:54,184 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.8597, 'train_samples_per_second': 18.9, 'train_steps_per_second': 0.378, 'train_loss': 1.3772635018384014, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [02:22<00:00,  2.65s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:16:54,200 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:16:54,239 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:16:54,244 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:17:27,243 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:17:27,247 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:17:27,251 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30654134GF\n",
            "  train_loss               =     1.3773\n",
            "  train_runtime            = 0:02:22.85\n",
            "  train_samples_per_second =       18.9\n",
            "  train_steps_per_second   =      0.378\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_1e-05/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:17:31,682 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:17:31,682 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:17:31,682 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7832\n",
            "  eval_loss               =     1.3081\n",
            "  eval_runtime            = 0:00:03.58\n",
            "  eval_samples_per_second =     27.909\n",
            "  eval_steps_per_second   =      9.489\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:17:35,275 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7832188072114876}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jJWSA_qwMHKS",
        "outputId": "e00da4d7-d5e2-4711-d79f-8ce21a7a92bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:17:40.508143: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:17:40.559847: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:17:40.559894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:17:40.561404: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:17:40.569468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:17:41.862017: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:17:47 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:17:47,963 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:17:47,963 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:17:47,963 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:17:47,963 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:17:47,963 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:17:48,035 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:17:48 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:17:48 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:17:48 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:17:48 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:17:49,947 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:17:50,468 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:17:50,469 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:17:51,200 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:17:51,201 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:17:51,202 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.55s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:17:56,373 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:17:56,373 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:17:57,076 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:17:57,077 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:17:57 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:17:57,120 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:17:58,162 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:17:58,162 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:17:58,162 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:17:58,162 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:17:58,162 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:17:58,162 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:17:58,162 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:17:58,163 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/54 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.5308, 'grad_norm': 1.1609392166137695, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.3542, 'grad_norm': 1.0540406703948975, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:23<01:42,  2.33s/it][INFO|trainer.py:3719] 2024-07-14 08:18:21,855 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:18:21,855 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:18:21,855 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.86it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 12.98it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.92it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.55it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.59it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.10it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.75it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.75it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.53it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.17it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.09it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.79it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.300437331199646, 'eval_accuracy': 0.7828000466145462, 'eval_runtime': 3.6012, 'eval_samples_per_second': 27.769, 'eval_steps_per_second': 9.441, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:27<01:42,  2.33s/it]\n",
            "100% 34/34 [00:03<00:00, 10.32it/s]\u001b[A\n",
            "{'loss': 1.2316, 'grad_norm': 0.8017536401748657, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.3269, 'grad_norm': 0.8991929292678833, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:50<01:20,  2.36s/it][INFO|trainer.py:3719] 2024-07-14 08:18:48,417 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:18:48,417 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:18:48,417 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.29it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.03it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.65it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.13it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.76it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.91it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.51it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.10it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2646162509918213, 'eval_accuracy': 0.7876328738927981, 'eval_runtime': 3.5844, 'eval_samples_per_second': 27.898, 'eval_steps_per_second': 9.485, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:53<01:20,  2.36s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "{'loss': 1.2274, 'grad_norm': 0.8117754459381104, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.1184, 'grad_norm': 0.672272801399231, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:17<00:57,  2.40s/it][INFO|trainer.py:3719] 2024-07-14 08:19:15,339 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:19:15,339 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:19:15,339 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.91it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 12.81it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 10.84it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.54it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.55it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02,  9.91it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.55it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:02,  9.49it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.79it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.70it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.55it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.75it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.85it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.89it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.44it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.03it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.69it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2560014724731445, 'eval_accuracy': 0.7885586331779266, 'eval_runtime': 3.6197, 'eval_samples_per_second': 27.627, 'eval_steps_per_second': 9.393, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:20<00:57,  2.40s/it]\n",
            "100% 34/34 [00:03<00:00, 10.26it/s]\u001b[A\n",
            "{'loss': 1.1, 'grad_norm': 0.801800549030304, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.2123, 'grad_norm': 0.8885489702224731, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:43<00:32,  2.30s/it][INFO|trainer.py:3719] 2024-07-14 08:19:41,715 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:19:41,715 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:19:41,715 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.92it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.03it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.65it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.16it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.78it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.22it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.92it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.94it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.40it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.44it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.03it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.71it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2546640634536743, 'eval_accuracy': 0.7872182217338284, 'eval_runtime': 3.5928, 'eval_samples_per_second': 27.833, 'eval_steps_per_second': 9.463, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:47<00:32,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.13it/s]\u001b[A\n",
            "{'loss': 1.1044, 'grad_norm': 0.8327363133430481, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 1.1094, 'grad_norm': 0.7460633516311646, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:10<00:09,  2.42s/it][INFO|trainer.py:3719] 2024-07-14 08:20:08,662 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:20:08,663 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:20:08,663 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.95it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.31it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.05it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.65it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.91it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.92it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.2541685104370117, 'eval_accuracy': 0.7860643952625894, 'eval_runtime': 3.5801, 'eval_samples_per_second': 27.932, 'eval_steps_per_second': 9.497, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:14<00:09,  2.42s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "100% 54/54 [02:22<00:00,  2.58s/it][INFO|trainer.py:2329] 2024-07-14 08:20:20,908 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.7554, 'train_samples_per_second': 18.913, 'train_steps_per_second': 0.378, 'train_loss': 1.220452591224953, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [02:22<00:00,  2.64s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:20:20,923 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:20:20,944 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:20:20,948 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:20:55,507 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:20:55,513 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:20:55,518 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30654134GF\n",
            "  train_loss               =     1.2205\n",
            "  train_runtime            = 0:02:22.75\n",
            "  train_samples_per_second =     18.913\n",
            "  train_steps_per_second   =      0.378\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-05/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:20:56,049 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:20:56,049 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:20:56,049 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =     0.7862\n",
            "  eval_loss               =     1.2541\n",
            "  eval_runtime            = 0:00:03.59\n",
            "  eval_samples_per_second =     27.799\n",
            "  eval_steps_per_second   =      9.452\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:20:59,666 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7861578532065145}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-04 \\\n",
        "    --num_train_epochs 3.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExbAQjjoPVNg",
        "outputId": "e081a2c9-0477-483f-985b-d0cd80e64f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-14 08:22:59.196486: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-14 08:22:59.249741: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-14 08:22:59.249786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-14 08:22:59.251407: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-14 08:22:59.259812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-14 08:23:00.513980: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/14/2024 08:23:06 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:23:06,543 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:23:06,543 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:23:06,543 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:23:06,543 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-14 08:23:06,543 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-14 08:23:06,613 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/14/2024 08:23:06 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/14/2024 08:23:06 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/14/2024 08:23:06 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/14/2024 08:23:07 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:23:08,487 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-14 08:23:08,949 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-14 08:23:08,950 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-14 08:23:09,254 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-14 08:23:09,254 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:23:09,255 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:05<00:00,  2.56s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-14 08:23:14,438 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-14 08:23:14,438 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-14 08:23:14,679 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-14 08:23:14,680 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/14/2024 08:23:14 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-14 08:23:14,726 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-14 08:23:15,811 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-14 08:23:15,811 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-14 08:23:15,811 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2081] 2024-07-14 08:23:15,811 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-14 08:23:15,812 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-14 08:23:15,812 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-14 08:23:15,812 >>   Total optimization steps = 54\n",
            "[INFO|trainer.py:2087] 2024-07-14 08:23:15,813 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/54 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.6819, 'grad_norm': 0.9834575653076172, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.4052, 'grad_norm': 0.6535486578941345, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:23<01:42,  2.32s/it][INFO|trainer.py:3719] 2024-07-14 08:23:39,706 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:23:39,707 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:23:39,707 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.97it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.06it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.65it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.67it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.17it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.74it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.65it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.92it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.80it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.80it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.69it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.57it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.23it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.94it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.95it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.42it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.53it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.12it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.80it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.3412346839904785, 'eval_accuracy': 0.7754067231289441, 'eval_runtime': 3.5759, 'eval_samples_per_second': 27.965, 'eval_steps_per_second': 9.508, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            " 19% 10/54 [00:27<01:42,  2.32s/it]\n",
            "100% 34/34 [00:03<00:00, 10.45it/s]\u001b[A\n",
            "{'loss': 1.2826, 'grad_norm': 0.55906742811203, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.3316, 'grad_norm': 0.50966477394104, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:50<01:19,  2.35s/it][INFO|trainer.py:3719] 2024-07-14 08:24:06,174 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:24:06,174 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:24:06,174 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.94it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.04it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.16it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.78it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.64it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.81it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.70it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.57it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.22it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.92it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.51it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.08it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.77it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.317428708076477, 'eval_accuracy': 0.7790661311735118, 'eval_runtime': 3.5788, 'eval_samples_per_second': 27.942, 'eval_steps_per_second': 9.5, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 37% 20/54 [00:53<01:19,  2.35s/it]\n",
            "100% 34/34 [00:03<00:00, 10.40it/s]\u001b[A\n",
            "{'loss': 0.9625, 'grad_norm': 0.48696911334991455, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 0.7974, 'grad_norm': 0.3672252297401428, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:17<00:57,  2.39s/it][INFO|trainer.py:3719] 2024-07-14 08:24:33,111 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:24:33,111 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:24:33,112 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.88it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.13it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.70it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.79it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.64it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.80it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.69it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.53it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.07it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.77it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.366786241531372, 'eval_accuracy': 0.781665573454598, 'eval_runtime': 3.5812, 'eval_samples_per_second': 27.924, 'eval_steps_per_second': 9.494, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 56% 30/54 [01:20<00:57,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.41it/s]\u001b[A\n",
            "{'loss': 0.7372, 'grad_norm': 0.42683786153793335, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 0.7068, 'grad_norm': 0.5128708481788635, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:43<00:32,  2.30s/it][INFO|trainer.py:3719] 2024-07-14 08:24:59,394 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:24:59,394 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:24:59,395 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.93it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.01it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.15it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.4430090188980103, 'eval_accuracy': 0.7712057167373706, 'eval_runtime': 3.5831, 'eval_samples_per_second': 27.909, 'eval_steps_per_second': 9.489, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 74% 40/54 [01:47<00:32,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.42it/s]\u001b[A\n",
            "{'loss': 0.4986, 'grad_norm': 0.47876569628715515, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 0.4825, 'grad_norm': 0.37972888350486755, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:10<00:09,  2.43s/it][INFO|trainer.py:3719] 2024-07-14 08:25:26,354 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:25:26,354 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:25:26,354 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.92it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.26it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.11it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.59it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.80it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.54it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.48it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                   \n",
            "\u001b[A{'eval_loss': 1.5274887084960938, 'eval_accuracy': 0.7713718547164617, 'eval_runtime': 3.5832, 'eval_samples_per_second': 27.908, 'eval_steps_per_second': 9.489, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 93% 50/54 [02:14<00:09,  2.43s/it]\n",
            "100% 34/34 [00:03<00:00, 10.41it/s]\u001b[A\n",
            "100% 54/54 [02:22<00:00,  2.58s/it][INFO|trainer.py:2329] 2024-07-14 08:25:38,589 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 142.787, 'train_samples_per_second': 18.909, 'train_steps_per_second': 0.378, 'train_loss': 0.9508257927717986, 'epoch': 2.88, 'num_input_tokens_seen': 1473648}\n",
            "100% 54/54 [02:22<00:00,  2.64s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-14 08:25:38,605 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04\n",
            "[INFO|configuration_utils.py:472] 2024-07-14 08:25:38,632 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-14 08:25:38,637 >> Configuration saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-14 08:26:11,039 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-14 08:26:11,064 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-14 08:26:11,069 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       2.88\n",
            "  num_input_tokens_seen    =    1473648\n",
            "  total_flos               = 30654134GF\n",
            "  train_loss               =     0.9508\n",
            "  train_runtime            = 0:02:22.78\n",
            "  train_samples_per_second =     18.909\n",
            "  train_steps_per_second   =      0.378\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_max_samples/train_lr_5e-04/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-14 08:26:11,612 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-14 08:26:11,612 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-14 08:26:11,612 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.95it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       2.88\n",
            "  eval_accuracy           =      0.771\n",
            "  eval_loss               =     1.5307\n",
            "  eval_runtime            = 0:00:03.57\n",
            "  eval_samples_per_second =     27.966\n",
            "  eval_steps_per_second   =      9.508\n",
            "  num_input_tokens_seen   =    1473648\n",
            "[INFO|modelcard.py:450] 2024-07-14 08:26:15,904 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7710386807386062}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 5.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_epoch/train_epoch5 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "id": "TB3ANMXSvsxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory;llamafactory-cli train \\\n",
        "    --stage sft \\\n",
        "    --do_train True \\\n",
        "    --model_name_or_path microsoft/Phi-3-mini-4k-instruct \\\n",
        "    --preprocessing_num_workers 16 \\\n",
        "    --finetuning_type freeze \\\n",
        "    --template phi \\\n",
        "    --flash_attn auto \\\n",
        "    --dataset_dir data \\\n",
        "    --dataset MedQA_train_US,PubMedQA_train \\\n",
        "    --cutoff_len 1024 \\\n",
        "    --learning_rate 5e-05 \\\n",
        "    --num_train_epochs 7.0 \\\n",
        "    --max_samples 500 \\\n",
        "    --per_device_train_batch_size 6 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --logging_steps 5 \\\n",
        "    --save_steps 1000 \\\n",
        "    --warmup_steps 0 \\\n",
        "    --optim adamw_torch \\\n",
        "    --packing False \\\n",
        "    --report_to none \\\n",
        "    --output_dir /content/drive/MyDrive/9900/freeze_epoch/train_epoch7 \\\n",
        "    --fp16 True \\\n",
        "    --plot_loss True \\\n",
        "    --ddp_timeout 180000000 \\\n",
        "    --include_num_input_tokens_seen True \\\n",
        "    --freeze_trainable_layers 2 \\\n",
        "    --freeze_trainable_modules all \\\n",
        "    --val_size 0.1 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 10 \\\n",
        "    --per_device_eval_batch_size 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAuo6r4WSj2t",
        "outputId": "864ad643-5c51-40a6-8da6-28a17c130e9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-15 09:29:43.473795: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-07-15 09:29:43.524900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-15 09:29:43.524945: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-15 09:29:43.526488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-15 09:29:43.534256: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-15 09:29:44.718831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/15/2024 09:29:50 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 09:29:50,268 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 09:29:50,268 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 09:29:50,268 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 09:29:50,268 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-07-15 09:29:50,268 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/tokenizer_config.json\n",
            "[WARNING|logging.py:314] 2024-07-15 09:29:50,336 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "07/15/2024 09:29:50 - INFO - llamafactory.data.template - Replace eos token: <|end|>\n",
            "07/15/2024 09:29:50 - WARNING - llamafactory.data.template - New tokens have been added, make sure `resize_vocab` is True.\n",
            "07/15/2024 09:29:50 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/train.json...\n",
            "07/15/2024 09:29:50 - INFO - llamafactory.data.loader - Loading dataset /content/drive/MyDrive/9900/pqal_train_set.json...\n",
            "training example:\n",
            "input_ids:\n",
            "[1, 32010, 673, 278, 1494, 2999, 7348, 1139, 29901, 319, 29871, 29906, 29941, 29899, 6360, 29899, 1025, 758, 5138, 424, 6114, 472, 29871, 29906, 29906, 11405, 7737, 362, 22981, 411, 25535, 2501, 5065, 3381, 29889, 2296, 5922, 372, 4687, 29871, 29896, 2462, 8020, 322, 756, 1063, 281, 943, 8333, 15020, 13748, 292, 901, 4094, 322, 5622, 274, 661, 16344, 6597, 29889, 2296, 6467, 23880, 1532, 322, 338, 5643, 491, 263, 11619, 363, 902, 758, 5138, 6906, 29889, 2439, 10430, 338, 29871, 29929, 29955, 29889, 29955, 30073, 29943, 313, 29941, 29953, 29889, 29945, 30073, 29907, 511, 10416, 12959, 338, 29871, 29896, 29906, 29906, 29914, 29955, 29955, 5654, 29950, 29887, 29892, 9505, 344, 338, 29871, 29947, 29900, 29914, 1195, 29892, 4613, 381, 800, 526, 29871, 29896, 29929, 29914, 1195, 29892, 322, 288, 28596, 269, 1337, 362, 338, 29871, 29929, 29947, 29995, 373, 5716, 4799, 29889, 11661, 936, 4392, 338, 18697, 363, 385, 18070, 310, 3438, 957, 371, 1182, 284, 10696, 10331, 824, 404, 322, 263, 8310, 333, 318, 357, 375, 29889, 8449, 310, 278, 1494, 338, 278, 1900, 14502, 363, 445, 16500, 14579, 319, 29901, 319, 1526, 293, 453, 262, 29892, 350, 29901, 7747, 615, 374, 1165, 650, 29892, 315, 29901, 315, 666, 307, 29888, 417, 29916, 562, 262, 29892, 360, 29901, 1938, 3594, 1270, 695, 457, 29892, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007, 29871, 13, 32001, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "inputs:\n",
            "<s><|user|> Answer the following multiple choice question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?, A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|end|> \n",
            "<|assistant|> E: Nitrofurantoin<|end|>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 382, 29901, 405, 277, 307, 22613, 5361, 262, 32007]\n",
            "labels:\n",
            "E: Nitrofurantoin<|end|>\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-07-15 09:29:50,836 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:733] 2024-07-15 09:29:50,931 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-07-15 09:29:50,932 >> Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-4k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-4k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 2047,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "[INFO|modeling_utils.py:3474] 2024-07-15 09:29:51,009 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:1519] 2024-07-15 09:29:51,010 >> Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:962] 2024-07-15 09:29:51,011 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:04<00:00,  2.39s/it]\n",
            "[INFO|modeling_utils.py:4280] 2024-07-15 09:29:55,845 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-07-15 09:29:55,845 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-4k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-07-15 09:29:55,899 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-4k-instruct/snapshots/c1358f8a35e6d2af81890deffbbfa575b978c62f/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-07-15 09:29:55,899 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.model_utils.attention - Using vanilla attention implementation.\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.adapter - Fine-tuning method: Freeze\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.adapter - Set trainable layers: .30.,.31.\n",
            "07/15/2024 09:29:55 - INFO - llamafactory.model.loader - trainable params: 226,504,704 || all params: 3,821,079,552 || trainable%: 5.9278\n",
            "[INFO|trainer.py:641] 2024-07-15 09:29:55,935 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2078] 2024-07-15 09:29:57,060 >> ***** Running training *****\n",
            "[INFO|trainer.py:2079] 2024-07-15 09:29:57,060 >>   Num examples = 900\n",
            "[INFO|trainer.py:2080] 2024-07-15 09:29:57,060 >>   Num Epochs = 7\n",
            "[INFO|trainer.py:2081] 2024-07-15 09:29:57,060 >>   Instantaneous batch size per device = 6\n",
            "[INFO|trainer.py:2084] 2024-07-15 09:29:57,060 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "[INFO|trainer.py:2085] 2024-07-15 09:29:57,060 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2086] 2024-07-15 09:29:57,060 >>   Total optimization steps = 126\n",
            "[INFO|trainer.py:2087] 2024-07-15 09:29:57,061 >>   Number of trainable parameters = 226,504,704\n",
            "  0% 0/126 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.5307, 'grad_norm': 1.16056489944458, 'learning_rate': 4.9805980165004304e-05, 'epoch': 0.27, 'num_input_tokens_seen': 136704}\n",
            "{'loss': 1.3541, 'grad_norm': 1.0603601932525635, 'learning_rate': 4.922693215572695e-05, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            "  8% 10/126 [00:23<04:28,  2.32s/it][INFO|trainer.py:3719] 2024-07-15 09:30:20,710 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:30:20,711 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:30:20,711 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.95it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.06it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.66it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.68it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.20it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.77it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.67it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.94it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.83it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.68it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.84it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.74it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.61it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.26it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.96it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.96it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.42it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.54it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.11it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.80it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.299908995628357, 'eval_accuracy': 0.7832677342503481, 'eval_runtime': 3.5696, 'eval_samples_per_second': 28.014, 'eval_steps_per_second': 9.525, 'epoch': 0.53, 'num_input_tokens_seen': 273120}\n",
            "  8% 10/126 [00:27<04:28,  2.32s/it]\n",
            "100% 34/34 [00:03<00:00, 10.46it/s]\u001b[A\n",
            "{'loss': 1.2308, 'grad_norm': 0.8050819039344788, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.8, 'num_input_tokens_seen': 404016}\n",
            "{'loss': 1.3238, 'grad_norm': 0.8832111358642578, 'learning_rate': 4.6955539334255716e-05, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 16% 20/126 [00:50<04:08,  2.35s/it][INFO|trainer.py:3719] 2024-07-15 09:30:47,146 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:30:47,146 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:30:47,146 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.92it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.32it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.04it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.66it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.19it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.75it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.65it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.92it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.81it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.66it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.83it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.72it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.59it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.23it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.94it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.94it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.41it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.52it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.10it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.79it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2616766691207886, 'eval_accuracy': 0.7862663284989171, 'eval_runtime': 3.5711, 'eval_samples_per_second': 28.003, 'eval_steps_per_second': 9.521, 'epoch': 1.07, 'num_input_tokens_seen': 545376}\n",
            " 16% 20/126 [00:53<04:08,  2.35s/it]\n",
            "100% 34/34 [00:03<00:00, 10.43it/s]\u001b[A\n",
            "{'loss': 1.2144, 'grad_norm': 0.837008535861969, 'learning_rate': 4.529845014289642e-05, 'epoch': 1.33, 'num_input_tokens_seen': 684240}\n",
            "{'loss': 1.1029, 'grad_norm': 0.6776818633079529, 'learning_rate': 4.332629679574566e-05, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 24% 30/126 [01:16<03:49,  2.39s/it][INFO|trainer.py:3719] 2024-07-15 09:31:14,006 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:31:14,006 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:31:14,006 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.30it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.04it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.66it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.15it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.73it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.91it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.80it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.64it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.81it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.70it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.58it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.23it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.93it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.51it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.09it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.78it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2547017335891724, 'eval_accuracy': 0.7863345465291216, 'eval_runtime': 3.5739, 'eval_samples_per_second': 27.981, 'eval_steps_per_second': 9.514, 'epoch': 1.6, 'num_input_tokens_seen': 822576}\n",
            " 24% 30/126 [01:20<03:49,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.44it/s]\u001b[A\n",
            "{'loss': 1.0764, 'grad_norm': 0.7982872724533081, 'learning_rate': 4.1069690242163484e-05, 'epoch': 1.87, 'num_input_tokens_seen': 953904}\n",
            "{'loss': 1.1609, 'grad_norm': 0.9419064521789551, 'learning_rate': 3.856365659664399e-05, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 32% 40/126 [01:43<03:17,  2.30s/it][INFO|trainer.py:3719] 2024-07-15 09:31:40,269 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:31:40,269 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:31:40,269 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.89it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.64it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.16it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.90it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.78it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.64it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.80it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.90it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.47it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.05it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.74it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2603238821029663, 'eval_accuracy': 0.7835735681755535, 'eval_runtime': 3.5826, 'eval_samples_per_second': 27.913, 'eval_steps_per_second': 9.49, 'epoch': 2.13, 'num_input_tokens_seen': 1092192}\n",
            " 32% 40/126 [01:46<03:17,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.39it/s]\u001b[A\n",
            "{'loss': 1.0161, 'grad_norm': 0.8472700119018555, 'learning_rate': 3.5847093477938956e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1228944}\n",
            "{'loss': 0.995, 'grad_norm': 0.7272021770477295, 'learning_rate': 3.2962166256292113e-05, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 40% 50/126 [02:10<03:04,  2.43s/it][INFO|trainer.py:3719] 2024-07-15 09:32:07,225 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:32:07,225 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:32:07,225 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.87it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.26it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.11it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.68it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.75it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.76it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.65it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.53it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.90it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.92it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.48it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.2816543579101562, 'eval_accuracy': 0.777839821452706, 'eval_runtime': 3.5867, 'eval_samples_per_second': 27.881, 'eval_steps_per_second': 9.479, 'epoch': 2.67, 'num_input_tokens_seen': 1369728}\n",
            " 40% 50/126 [02:13<03:04,  2.43s/it]\n",
            "100% 34/34 [00:03<00:00, 10.42it/s]\u001b[A\n",
            "{'loss': 0.9657, 'grad_norm': 0.8786497712135315, 'learning_rate': 2.9953653579984942e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1502976}\n",
            "{'loss': 0.891, 'grad_norm': 0.7617467641830444, 'learning_rate': 2.686825233966061e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1640112}\n",
            " 48% 60/126 [02:36<02:30,  2.28s/it][INFO|trainer.py:3719] 2024-07-15 09:32:33,502 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:32:33,503 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:32:33,503 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.86it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.72it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.79it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.56it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.21it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.92it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.93it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.39it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.07it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.77it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.3128191232681274, 'eval_accuracy': 0.7814697537336192, 'eval_runtime': 3.58, 'eval_samples_per_second': 27.933, 'eval_steps_per_second': 9.497, 'epoch': 3.2, 'num_input_tokens_seen': 1640112}\n",
            " 48% 60/126 [02:40<02:30,  2.28s/it]\n",
            "100% 34/34 [00:03<00:00, 10.43it/s]\u001b[A\n",
            "{'loss': 0.8772, 'grad_norm': 0.7342725992202759, 'learning_rate': 2.375385285848257e-05, 'epoch': 3.47, 'num_input_tokens_seen': 1777824}\n",
            "{'loss': 0.8388, 'grad_norm': 0.8497952818870544, 'learning_rate': 2.0658795558326743e-05, 'epoch': 3.73, 'num_input_tokens_seen': 1913328}\n",
            " 56% 70/126 [03:02<02:11,  2.34s/it][INFO|trainer.py:3719] 2024-07-15 09:33:00,027 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:33:00,027 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:33:00,027 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.89it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.29it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:01<00:02,  8.84it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02,  9.56it/s]\u001b[A\n",
            " 38% 13/34 [00:01<00:02,  9.23it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.38it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:02,  9.33it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.71it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.49it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.71it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.61it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.49it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.16it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.89it/s]\u001b[A\n",
            " 85% 29/34 [00:03<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.75it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.3842159509658813, 'eval_accuracy': 0.7708107791526031, 'eval_runtime': 3.6365, 'eval_samples_per_second': 27.499, 'eval_steps_per_second': 9.35, 'epoch': 3.73, 'num_input_tokens_seen': 1913328}\n",
            " 56% 70/126 [03:06<02:11,  2.34s/it]\n",
            "100% 34/34 [00:03<00:00, 10.42it/s]\u001b[A\n",
            "{'loss': 0.8734, 'grad_norm': 0.9512373805046082, 'learning_rate': 1.7631120639727393e-05, 'epoch': 4.0, 'num_input_tokens_seen': 2059008}\n",
            "{'loss': 0.7199, 'grad_norm': 0.7211918830871582, 'learning_rate': 1.4717822421734718e-05, 'epoch': 4.27, 'num_input_tokens_seen': 2201712}\n",
            " 63% 80/126 [03:30<01:55,  2.52s/it][INFO|trainer.py:3719] 2024-07-15 09:33:28,008 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:33:28,008 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:33:28,008 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.90it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.30it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.03it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.60it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.88it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.78it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.91it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.50it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.07it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.77it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.4252737760543823, 'eval_accuracy': 0.7693643494903672, 'eval_runtime': 3.5805, 'eval_samples_per_second': 27.929, 'eval_steps_per_second': 9.496, 'epoch': 4.27, 'num_input_tokens_seen': 2201712}\n",
            " 63% 80/126 [03:34<01:55,  2.52s/it]\n",
            "100% 34/34 [00:03<00:00, 10.42it/s]\u001b[A\n",
            "{'loss': 0.6775, 'grad_norm': 0.8187654614448547, 'learning_rate': 1.196411991551255e-05, 'epoch': 4.53, 'num_input_tokens_seen': 2334768}\n",
            "{'loss': 0.6659, 'grad_norm': 0.8367564678192139, 'learning_rate': 9.412754953531663e-06, 'epoch': 4.8, 'num_input_tokens_seen': 2472912}\n",
            " 71% 90/126 [03:57<01:25,  2.37s/it][INFO|trainer.py:3719] 2024-07-15 09:33:54,414 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:33:54,414 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:33:54,414 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.93it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.28it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.02it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.61it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.63it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.12it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.70it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.87it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.77it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.77it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.67it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.20it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.89it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.89it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.36it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.47it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.04it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.73it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 1.5092651844024658, 'eval_accuracy': 0.7676974397944507, 'eval_runtime': 3.5849, 'eval_samples_per_second': 27.895, 'eval_steps_per_second': 9.484, 'epoch': 4.8, 'num_input_tokens_seen': 2472912}\n",
            " 71% 90/126 [04:00<01:25,  2.37s/it]\n",
            "100% 34/34 [00:03<00:00, 10.40it/s]\u001b[A\n",
            "{'loss': 0.7168, 'grad_norm': 0.8117745518684387, 'learning_rate': 7.103328768507039e-06, 'epoch': 5.07, 'num_input_tokens_seen': 2603472}\n",
            "{'loss': 0.6487, 'grad_norm': 0.8840070962905884, 'learning_rate': 5.071687319426946e-06, 'epoch': 5.33, 'num_input_tokens_seen': 2737632}\n",
            " 79% 100/126 [04:23<00:59,  2.30s/it][INFO|trainer.py:3719] 2024-07-15 09:34:20,245 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:34:20,245 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:34:20,245 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.89it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.01it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.69it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.58it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.85it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.75it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.76it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.64it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.53it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.17it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.87it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.88it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.34it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.46it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.04it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.74it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5272927284240723, 'eval_accuracy': 0.7663790716688245, 'eval_runtime': 3.5897, 'eval_samples_per_second': 27.857, 'eval_steps_per_second': 9.471, 'epoch': 5.33, 'num_input_tokens_seen': 2737632}\n",
            " 79% 100/126 [04:26<00:59,  2.30s/it]\n",
            "100% 34/34 [00:03<00:00, 10.40it/s]\u001b[A\n",
            "{'loss': 0.583, 'grad_norm': 0.8034355044364929, 'learning_rate': 3.3493649053890326e-06, 'epoch': 5.6, 'num_input_tokens_seen': 2879328}\n",
            "{'loss': 0.6621, 'grad_norm': 0.8642693161964417, 'learning_rate': 1.9630947032398067e-06, 'epoch': 5.87, 'num_input_tokens_seen': 3016080}\n",
            " 87% 110/126 [04:50<00:38,  2.39s/it][INFO|trainer.py:3719] 2024-07-15 09:34:47,319 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:34:47,319 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:34:47,319 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.86it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.27it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.01it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.14it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.62it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.78it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.80it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.21it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.91it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.38it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.49it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5500506162643433, 'eval_accuracy': 0.7634976179073264, 'eval_runtime': 3.5822, 'eval_samples_per_second': 27.916, 'eval_steps_per_second': 9.491, 'epoch': 5.87, 'num_input_tokens_seen': 3016080}\n",
            " 87% 110/126 [04:53<00:38,  2.39s/it]\n",
            "100% 34/34 [00:03<00:00, 10.42it/s]\u001b[A\n",
            "{'loss': 0.6148, 'grad_norm': 0.7811758518218994, 'learning_rate': 9.343938262496993e-07, 'epoch': 6.13, 'num_input_tokens_seen': 3150912}\n",
            "{'loss': 0.5996, 'grad_norm': 0.7773964405059814, 'learning_rate': 2.7922934437178695e-07, 'epoch': 6.4, 'num_input_tokens_seen': 3288048}\n",
            " 95% 120/126 [05:16<00:14,  2.34s/it][INFO|trainer.py:3719] 2024-07-15 09:35:13,773 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:35:13,773 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:35:13,773 >>   Batch size = 3\n",
            "\n",
            "  0% 0/34 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 2/34 [00:00<00:02, 15.84it/s]\u001b[A\n",
            " 12% 4/34 [00:00<00:02, 13.23it/s]\u001b[A\n",
            " 18% 6/34 [00:00<00:02, 11.00it/s]\u001b[A\n",
            " 24% 8/34 [00:00<00:02,  9.60it/s]\u001b[A\n",
            " 29% 10/34 [00:00<00:02,  9.62it/s]\u001b[A\n",
            " 35% 12/34 [00:01<00:02, 10.13it/s]\u001b[A\n",
            " 41% 14/34 [00:01<00:02,  9.71it/s]\u001b[A\n",
            " 44% 15/34 [00:01<00:01,  9.61it/s]\u001b[A\n",
            " 50% 17/34 [00:01<00:01,  9.89it/s]\u001b[A\n",
            " 53% 18/34 [00:01<00:01,  9.78it/s]\u001b[A\n",
            " 56% 19/34 [00:01<00:01,  9.63it/s]\u001b[A\n",
            " 62% 21/34 [00:02<00:01,  9.79it/s]\u001b[A\n",
            " 65% 22/34 [00:02<00:01,  9.68it/s]\u001b[A\n",
            " 68% 23/34 [00:02<00:01,  9.55it/s]\u001b[A\n",
            " 71% 24/34 [00:02<00:01,  9.19it/s]\u001b[A\n",
            " 76% 26/34 [00:02<00:00,  9.91it/s]\u001b[A\n",
            " 79% 27/34 [00:02<00:00,  8.91it/s]\u001b[A\n",
            " 85% 29/34 [00:02<00:00,  9.37it/s]\u001b[A\n",
            " 88% 30/34 [00:03<00:00,  9.47it/s]\u001b[A\n",
            " 91% 31/34 [00:03<00:00,  9.06it/s]\u001b[A\n",
            " 94% 32/34 [00:03<00:00,  8.76it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 1.5569690465927124, 'eval_accuracy': 0.7620109395035708, 'eval_runtime': 3.5823, 'eval_samples_per_second': 27.915, 'eval_steps_per_second': 9.491, 'epoch': 6.4, 'num_input_tokens_seen': 3288048}\n",
            " 95% 120/126 [05:20<00:14,  2.34s/it]\n",
            "100% 34/34 [00:03<00:00, 10.43it/s]\u001b[A\n",
            "{'loss': 0.5803, 'grad_norm': 0.7907907962799072, 'learning_rate': 7.770449979593864e-09, 'epoch': 6.67, 'num_input_tokens_seen': 3424080}\n",
            "100% 126/126 [05:34<00:00,  2.60s/it][INFO|trainer.py:2329] 2024-07-15 09:35:31,441 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 334.3904, 'train_samples_per_second': 18.84, 'train_steps_per_second': 0.377, 'train_loss': 0.9142889470335037, 'epoch': 6.72, 'num_input_tokens_seen': 3455136}\n",
            "100% 126/126 [05:34<00:00,  2.65s/it]\n",
            "[INFO|trainer.py:3410] 2024-07-15 09:35:31,456 >> Saving model checkpoint to /content/drive/MyDrive/9900/freeze_epoch/train_epoch7\n",
            "[INFO|configuration_utils.py:472] 2024-07-15 09:35:31,488 >> Configuration saved in /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/config.json\n",
            "[INFO|configuration_utils.py:731] 2024-07-15 09:35:31,493 >> Configuration saved in /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/generation_config.json\n",
            "[INFO|modeling_utils.py:2626] 2024-07-15 09:36:04,915 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/model.safetensors.index.json.\n",
            "[INFO|tokenization_utils_base.py:2513] 2024-07-15 09:36:04,921 >> tokenizer config file saved in /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2522] 2024-07-15 09:36:04,926 >> Special tokens file saved in /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       6.72\n",
            "  num_input_tokens_seen    =    3455136\n",
            "  total_flos               = 71872118GF\n",
            "  train_loss               =     0.9143\n",
            "  train_runtime            = 0:05:34.39\n",
            "  train_samples_per_second =      18.84\n",
            "  train_steps_per_second   =      0.377\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/training_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/training_eval_loss.png\n",
            "Figure saved at: /content/drive/MyDrive/9900/freeze_epoch/train_epoch7/training_eval_accuracy.png\n",
            "[INFO|trainer.py:3719] 2024-07-15 09:36:05,472 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3721] 2024-07-15 09:36:05,472 >>   Num examples = 100\n",
            "[INFO|trainer.py:3724] 2024-07-15 09:36:05,472 >>   Batch size = 3\n",
            "100% 34/34 [00:03<00:00,  9.89it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       6.72\n",
            "  eval_accuracy           =     0.7619\n",
            "  eval_loss               =     1.5576\n",
            "  eval_runtime            = 0:00:03.60\n",
            "  eval_samples_per_second =     27.768\n",
            "  eval_steps_per_second   =      9.441\n",
            "  num_input_tokens_seen   =    3455136\n",
            "[INFO|modelcard.py:450] 2024-07-15 09:36:09,087 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7619129002878844}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gCS3hWQjSj4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7aUxzx3jSj6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xHEDMRQRSj8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ithL5izSj-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}