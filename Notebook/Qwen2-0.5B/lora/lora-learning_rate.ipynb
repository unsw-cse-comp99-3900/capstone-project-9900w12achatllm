{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04dd5b1b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-13T04:15:22.532337Z",
     "iopub.status.busy": "2024-07-13T04:15:22.531730Z",
     "iopub.status.idle": "2024-07-13T04:16:31.829685Z",
     "shell.execute_reply": "2024-07-13T04:16:31.828635Z"
    },
    "papermill": {
     "duration": 69.306568,
     "end_time": "2024-07-13T04:16:31.831962",
     "exception": false,
     "start_time": "2024-07-13T04:15:22.525394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\r\n",
      "remote: Enumerating objects: 15353, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (303/303), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (135/135), done.\u001b[K\r\n",
      "remote: Total 15353 (delta 191), reused 262 (delta 167), pack-reused 15050\u001b[K\r\n",
      "Receiving objects: 100% (15353/15353), 221.76 MiB | 28.02 MiB/s, done.\r\n",
      "Resolving deltas: 100% (11224/11224), done.\r\n",
      "Obtaining file:///kaggle/working/LLaMA-Factory\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (4.41.2)\r\n",
      "Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.19.2)\r\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.30.1)\r\n",
      "Collecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.2.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.11.4)\r\n",
      "Collecting einops (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.2.0)\r\n",
      "Collecting tiktoken (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\r\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.25.0)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.5.3)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.108.0)\r\n",
      "Collecting sse-starlette (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.7.5)\r\n",
      "Collecting fire (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.2.4)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.42.1)\r\n",
      "Collecting rouge-chinese (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.3)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.3.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.1)\r\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (22.1.0)\r\n",
      "Requirement already satisfied: altair<6.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (5.3.0)\r\n",
      "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting gradio-client==1.1.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.27.0)\r\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.1.1)\r\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.3)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.9.10)\r\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.5.0)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.9.0)\r\n",
      "Collecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.14.6)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.2.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (0.19.1)\r\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.14.0)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.8.3.dev0) (0.32.0.post1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->llamafactory==0.8.3.dev0) (4.2.0)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.20.0)\r\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.0.3)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.0.5)\r\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.0)\r\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.32.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.16.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\r\n",
      "Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\r\n",
      "Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\r\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\r\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, ffmpy\r\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20627 sha256=700f37669c596a9fc950b24658eaeab29bcd5a0db9e4f4183b90fadf11afc61f\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lhwv0_kn/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=ecdd6f4e5b2e049b5c50e194c42266bb6eb642cbe259c7b4ee61405b40933bdb\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\r\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=94e2793dc6106bb66ae6647f3e85b757f6865088d9257f2a19ff4bbcd5e7cf5e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\r\n",
      "Successfully built llamafactory fire ffmpy\r\n",
      "Installing collected packages: ffmpy, websockets, urllib3, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, fire, einops, docstring-parser, tyro, typer, tiktoken, sse-starlette, gradio-client, gradio, trl, peft, llamafactory\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 12.0\r\n",
      "    Uninstalling websockets-12.0:\r\n",
      "      Successfully uninstalled websockets-12.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: tomlkit\r\n",
      "    Found existing installation: tomlkit 0.12.5\r\n",
      "    Uninstalling tomlkit-0.12.5:\r\n",
      "      Successfully uninstalled tomlkit-0.12.5\r\n",
      "  Attempting uninstall: docstring-parser\r\n",
      "    Found existing installation: docstring-parser 0.15\r\n",
      "    Uninstalling docstring-parser-0.15:\r\n",
      "      Successfully uninstalled docstring-parser-0.15\r\n",
      "  Attempting uninstall: typer\r\n",
      "    Found existing installation: typer 0.9.0\r\n",
      "    Uninstalling typer-0.9.0:\r\n",
      "      Successfully uninstalled typer-0.9.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 einops-0.8.0 ffmpy-0.3.2 fire-0.6.0 gradio-4.38.1 gradio-client-1.1.0 llamafactory-0.8.3.dev0 peft-0.11.1 python-multipart-0.0.9 rouge-chinese-1.0.3 ruff-0.5.1 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 typer-0.12.3 tyro-0.8.5 urllib3-2.1.0 websockets-11.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\"\n",
    "!pip install bitsandbytes>=0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe1b7fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:16:31.863247Z",
     "iopub.status.busy": "2024-07-13T04:16:31.862931Z",
     "iopub.status.idle": "2024-07-13T04:16:31.871547Z",
     "shell.execute_reply": "2024-07-13T04:16:31.870788Z"
    },
    "papermill": {
     "duration": 0.026119,
     "end_time": "2024-07-13T04:16:31.873357",
     "exception": false,
     "start_time": "2024-07-13T04:16:31.847238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/kaggle/working/LLaMA-Factory/data/dataset_info.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# configure the MedQA and PubMedQA dataset information \n",
    "new_data = {\n",
    "    \"MedQA_train\": {\"file_name\": \"MedQA/train.json\"},\n",
    "    \"MedQA_test\": {\"file_name\": \"MedQA/test.json\"},\n",
    "    \"PubMedQA_pqal_train\": {\"file_name\": \"PubMedQA/pqal_train_set.json\"},\n",
    "    \"PubMedQA_pqal_test\": {\"file_name\": \"PubMedQA/pqal_test_set.json\"}\n",
    "}\n",
    "\n",
    "# update information in dataset_info.json file\n",
    "data.update(new_data)\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90671bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:16:31.904032Z",
     "iopub.status.busy": "2024-07-13T04:16:31.903385Z",
     "iopub.status.idle": "2024-07-13T04:16:32.912564Z",
     "shell.execute_reply": "2024-07-13T04:16:32.911611Z"
    },
    "papermill": {
     "duration": 1.026917,
     "end_time": "2024-07-13T04:16:32.914871",
     "exception": false,
     "start_time": "2024-07-13T04:16:31.887954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa-pubmedqa\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a2409d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:16:32.947966Z",
     "iopub.status.busy": "2024-07-13T04:16:32.947326Z",
     "iopub.status.idle": "2024-07-13T04:16:39.049585Z",
     "shell.execute_reply": "2024-07-13T04:16:39.048501Z"
    },
    "papermill": {
     "duration": 6.120896,
     "end_time": "2024-07-13T04:16:39.052022",
     "exception": false,
     "start_time": "2024-07-13T04:16:32.931126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create MedQA and PubMedQA directories\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/MedQA\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/PubMedQA\n",
    "\n",
    "# copy MedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/train.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/test.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "\n",
    "# copy PubMedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_train_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_test_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12163a8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:16:39.083588Z",
     "iopub.status.busy": "2024-07-13T04:16:39.083002Z",
     "iopub.status.idle": "2024-07-13T04:16:39.089483Z",
     "shell.execute_reply": "2024-07-13T04:16:39.088693Z"
    },
    "papermill": {
     "duration": 0.024715,
     "end_time": "2024-07-13T04:16:39.091777",
     "exception": false,
     "start_time": "2024-07-13T04:16:39.067062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b364c65d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:16:39.122773Z",
     "iopub.status.busy": "2024-07-13T04:16:39.122251Z",
     "iopub.status.idle": "2024-07-13T04:17:08.596658Z",
     "shell.execute_reply": "2024-07-13T04:17:08.595752Z"
    },
    "papermill": {
     "duration": 29.492222,
     "end_time": "2024-07-13T04:17:08.598929",
     "exception": false,
     "start_time": "2024-07-13T04:16:39.106707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\r\n",
      "  Downloading optimum-1.21.2-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting coloredlogs (from optimum)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12.1)\r\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.41.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.23.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.19.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.3.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (3.20.3)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\r\n",
      "Downloading optimum-1.21.2-py3-none-any.whl (424 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.21.2\r\n"
     ]
    }
   ],
   "source": [
    "# install packages for quantizaiton\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8fbf2",
   "metadata": {
    "papermill": {
     "duration": 0.016313,
     "end_time": "2024-07-13T04:17:08.632009",
     "exception": false,
     "start_time": "2024-07-13T04:17:08.615696",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 500, lr = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b69e8607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:17:08.665737Z",
     "iopub.status.busy": "2024-07-13T04:17:08.665410Z",
     "iopub.status.idle": "2024-07-13T04:31:48.552538Z",
     "shell.execute_reply": "2024-07-13T04:31:48.551415Z"
    },
    "papermill": {
     "duration": 879.906905,
     "end_time": "2024-07-13T04:31:48.554962",
     "exception": false,
     "start_time": "2024-07-13T04:17:08.648057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 04:17:17.992202: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 04:17:17.992324: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 04:17:18.127092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 04:17:34 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 8.30MB/s]\r\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 7.15MB/s]\r\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 25.3MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 37.3MB/s]\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:17:36,457 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 04:17:36,754 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 04:17:36 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 04:17:36 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "Generating train split: 10178 examples [00:00, 122526.63 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1143.5\r\n",
      "07/13/2024 04:17:37 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "Generating train split: 900 examples [00:00, 41481.22 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1477.2\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:03<00:00, 275.\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 4.45MB/s]\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:17:43,029 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:17:43,033 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "model.safetensors: 100%|██████████████████████| 988M/988M [00:03<00:00, 279MB/s]\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 04:17:46,937 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 04:17:46,964 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:17:46,966 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 04:17:48,968 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 04:17:48,968 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.81MB/s]\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 04:17:49,148 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:17:49,148 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,gate_proj,v_proj,k_proj,q_proj,o_proj,down_proj\r\n",
      "07/13/2024 04:17:49 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-13 04:17:49,568 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-13 04:17:49,986 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-13 04:17:49,986 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-13 04:17:49,986 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-13 04:17:49,986 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-13 04:17:49,986 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-13 04:17:49,986 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-13 04:17:49,986 >>   Total optimization steps = 54\r\n",
      "[INFO|trainer.py:2087] 2024-07-13 04:17:49,992 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 2.1512, 'grad_norm': 2.9229509830474854, 'learning_rate': 9.789947561577445e-06, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "{'loss': 2.0736, 'grad_norm': 2.2478840351104736, 'learning_rate': 9.177439057064684e-06, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:26<10:57, 14.94s/it][INFO|trainer.py:3719] 2024-07-13 04:20:16,634 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:20:16,634 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:20:16,634 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.28it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.95it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.44it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.32it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.38it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.61it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.53it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.32it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.69it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.23it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.54it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.82it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.79it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.91it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.88it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.58it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.86it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.32it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.19it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.7122803926467896, 'eval_accuracy': 0.6809852197990074, 'eval_runtime': 8.3204, 'eval_samples_per_second': 12.019, 'eval_steps_per_second': 6.009, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:34<10:57, 14.94s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.824, 'grad_norm': 1.5824049711227417, 'learning_rate': 8.213938048432697e-06, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      "{'loss': 2.0249, 'grad_norm': 1.2831043004989624, 'learning_rate': 6.980398830195785e-06, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:02<08:36, 15.19s/it][INFO|trainer.py:3719] 2024-07-13 04:22:52,687 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:22:52,687 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:22:52,687 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.33it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.01it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.49it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.33it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.77it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.38it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.61it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.53it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.25it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.86it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.78it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.79it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.92it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.89it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.35it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.636036992073059, 'eval_accuracy': 0.6964947739536465, 'eval_runtime': 8.2956, 'eval_samples_per_second': 12.055, 'eval_steps_per_second': 6.027, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:10<08:36, 15.19s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.9803, 'grad_norm': 1.3203586339950562, 'learning_rate': 5.5804645706261515e-06, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      "{'loss': 1.8209, 'grad_norm': 1.107837438583374, 'learning_rate': 4.131759111665349e-06, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:36<05:47, 14.49s/it][INFO|trainer.py:3719] 2024-07-13 04:25:26,767 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:25:26,767 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:25:26,767 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.48it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.79it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.64it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.55it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.36it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.27it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.57it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.80it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.62it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.18it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.89it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.35it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.20it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5996007919311523, 'eval_accuracy': 0.6985962180136366, 'eval_runtime': 8.2791, 'eval_samples_per_second': 12.079, 'eval_steps_per_second': 6.039, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:45<05:47, 14.49s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.65it/s]\u001b[A\r\n",
      "{'loss': 1.7978, 'grad_norm': 1.2702580690383911, 'learning_rate': 2.7560040989976894e-06, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      "{'loss': 1.9779, 'grad_norm': 1.2053591012954712, 'learning_rate': 1.5687918106563326e-06, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:07<03:21, 14.41s/it][INFO|trainer.py:3719] 2024-07-13 04:27:57,957 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:27:57,957 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:27:57,957 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.98it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.99it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.33it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.74it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.37it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.42it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.41it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.38it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.38it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.29it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.32it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.70it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.19it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.51it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.15it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.82it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.24it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.47it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.73it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.85it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.24it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.85it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.56it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.87it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.33it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5823943614959717, 'eval_accuracy': 0.7049990453292554, 'eval_runtime': 8.345, 'eval_samples_per_second': 11.983, 'eval_steps_per_second': 5.992, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:16<03:21, 14.41s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.8231, 'grad_norm': 1.1860222816467285, 'learning_rate': 6.698729810778065e-07, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      "{'loss': 1.8806, 'grad_norm': 1.217285394668579, 'learning_rate': 1.3477564710088097e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:44<01:01, 15.36s/it][INFO|trainer.py:3719] 2024-07-13 04:30:34,476 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:30:34,476 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:30:34,476 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.29it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.01it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.98it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.44it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.29it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.74it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.36it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.57it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.46it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.34it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.69it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.22it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.16it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.87it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.77it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.79it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.92it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.89it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.36it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.17it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5779281854629517, 'eval_accuracy': 0.7066700450938047, 'eval_runtime': 8.3102, 'eval_samples_per_second': 12.033, 'eval_steps_per_second': 6.017, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:52<01:01, 15.36s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:46<00:00, 14.58s/it][INFO|trainer.py:2329] 2024-07-13 04:31:36,478 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 826.4865, 'train_samples_per_second': 3.267, 'train_steps_per_second': 0.065, 'train_loss': 1.92777888863175, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:46<00:00, 15.31s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-13 04:31:36,480 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:31:36,729 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:31:36,731 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-13 04:31:36,782 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-13 04:31:36,782 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       2.88\r\n",
      "  num_input_tokens_seen    =    1278720\r\n",
      "  total_flos               =  2588760GF\r\n",
      "  train_loss               =     1.9278\r\n",
      "  train_runtime            = 0:13:46.48\r\n",
      "  train_samples_per_second =      3.267\r\n",
      "  train_steps_per_second   =      0.065\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 04:31:37,580 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:31:37,580 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:31:37,580 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.16it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       2.88\r\n",
      "  eval_accuracy           =     0.7068\r\n",
      "  eval_loss               =     1.5778\r\n",
      "  eval_runtime            = 0:00:08.29\r\n",
      "  eval_samples_per_second =     12.062\r\n",
      "  eval_steps_per_second   =      6.031\r\n",
      "  num_input_tokens_seen   =    1278720\r\n",
      "[INFO|modelcard.py:450] 2024-07-13 04:31:45,873 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7068313354163854}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-25 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975a7140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:31:48.650697Z",
     "iopub.status.busy": "2024-07-13T04:31:48.650370Z",
     "iopub.status.idle": "2024-07-13T04:34:42.989348Z",
     "shell.execute_reply": "2024-07-13T04:34:42.988396Z"
    },
    "papermill": {
     "duration": 174.389173,
     "end_time": "2024-07-13T04:34:42.991696",
     "exception": false,
     "start_time": "2024-07-13T04:31:48.602523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 04:31:53.493148: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 04:31:53.493210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 04:31:53.494819: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 04:32:02 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,259 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,260 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,260 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,260 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,260 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:32:02,260 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 04:32:02,522 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 04:32:02 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 04:32:02 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "Generating train split: 1273 examples [00:00, 99032.72 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 276.25\r\n",
      "07/13/2024 04:32:03 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "Generating train split: 100 examples [00:00, 23922.34 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 310.06\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 200/200 [00:03<00:00, 58.80 \r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:32:08,295 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:32:08,298 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:32:08 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 04:32:08,347 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 04:32:08,365 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:32:08,367 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 04:32:10,468 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 04:32:10,468 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 04:32:10,559 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:32:10,559 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:32:10 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 04:32:11 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/13/2024 04:32:11 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25\r\n",
      "07/13/2024 04:32:11 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 04:32:11,185 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:32:11,185 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:32:11,185 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:26<00:00,  1.53s/it]Building prefix dict from the default dictionary ...\r\n",
      "Dumping model to file cache /tmp/jieba.cache\r\n",
      "Loading model cost 1.112 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:28<00:00,  1.48s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    19.2137\r\n",
      "  predict_rouge-1            =    27.9177\r\n",
      "  predict_rouge-2            =    11.3215\r\n",
      "  predict_rouge-l            =    24.7342\r\n",
      "  predict_runtime            = 0:02:29.09\r\n",
      "  predict_samples_per_second =      1.341\r\n",
      "  predict_steps_per_second   =      0.671\r\n",
      "07/13/2024 04:34:40 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-34/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-34 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a287990f",
   "metadata": {
    "papermill": {
     "duration": 0.061749,
     "end_time": "2024-07-13T04:34:43.112630",
     "exception": false,
     "start_time": "2024-07-13T04:34:43.050881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 500, lr = 1e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d71d3d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:34:43.232911Z",
     "iopub.status.busy": "2024-07-13T04:34:43.232536Z",
     "iopub.status.idle": "2024-07-13T04:48:58.234414Z",
     "shell.execute_reply": "2024-07-13T04:48:58.233533Z"
    },
    "papermill": {
     "duration": 855.065101,
     "end_time": "2024-07-13T04:48:58.236787",
     "exception": false,
     "start_time": "2024-07-13T04:34:43.171686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 04:34:48.242886: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 04:34:48.242958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 04:34:48.244482: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 04:34:56 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:34:57,047 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 04:34:57,350 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 04:34:57 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 04:34:57 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "07/13/2024 04:34:57 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:34:58,210 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:34:58,212 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 04:34:58,247 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 04:34:58,257 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:34:58,259 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 04:34:59,956 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 04:34:59,957 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 04:35:00,047 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:35:00,047 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,k_proj,up_proj,gate_proj,q_proj,v_proj,o_proj\r\n",
      "07/13/2024 04:35:00 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-13 04:35:00,461 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-13 04:35:00,872 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-13 04:35:00,872 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-13 04:35:00,872 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-13 04:35:00,872 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-13 04:35:00,872 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-13 04:35:00,872 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-13 04:35:00,872 >>   Total optimization steps = 54\r\n",
      "[INFO|trainer.py:2087] 2024-07-13 04:35:00,879 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 1.9936, 'grad_norm': 1.0419822931289673, 'learning_rate': 9.789947561577445e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "{'loss': 1.8184, 'grad_norm': 0.789757251739502, 'learning_rate': 9.177439057064683e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:25<10:53, 14.84s/it][INFO|trainer.py:3719] 2024-07-13 04:37:26,858 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:37:26,858 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:37:26,858 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.99it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.68it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.57it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.49it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.46it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.64it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.89it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.36it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.26it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.20it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.78it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.62it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.89it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.34it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.64it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4628695249557495, 'eval_accuracy': 0.7253567873285551, 'eval_runtime': 8.2862, 'eval_samples_per_second': 12.068, 'eval_steps_per_second': 6.034, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:34<10:53, 14.84s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.5772, 'grad_norm': 0.761689305305481, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      "{'loss': 1.7757, 'grad_norm': 0.8455022573471069, 'learning_rate': 6.980398830195785e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:01<08:37, 15.23s/it][INFO|trainer.py:3719] 2024-07-13 04:40:02,699 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:40:02,699 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:40:02,699 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.33it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.01it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.49it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.60it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.53it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.45it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.60it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.30it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.33it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.70it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.16it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.49it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.13it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.85it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.27it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.78it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.78it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.91it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.28it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.86it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.57it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.86it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.31it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.17it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4308269023895264, 'eval_accuracy': 0.725820393204659, 'eval_runtime': 8.3172, 'eval_samples_per_second': 12.023, 'eval_steps_per_second': 6.012, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:10<08:37, 15.23s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7111, 'grad_norm': 0.9197217226028442, 'learning_rate': 5.5804645706261514e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      "{'loss': 1.6062, 'grad_norm': 0.6901183128356934, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:35<05:47, 14.50s/it][INFO|trainer.py:3719] 2024-07-13 04:42:36,583 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:42:36,583 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:42:36,583 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.02it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.68it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.57it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.82it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.29it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.16it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.49it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.13it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.83it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.25it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.74it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.47it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.87it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.25it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.86it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.56it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.84it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.31it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.15it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4209729433059692, 'eval_accuracy': 0.7258349255588166, 'eval_runtime': 8.3308, 'eval_samples_per_second': 12.004, 'eval_steps_per_second': 6.002, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:44<05:47, 14.50s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.60it/s]\u001b[A\r\n",
      "{'loss': 1.5746, 'grad_norm': 0.7383669018745422, 'learning_rate': 2.7560040989976892e-05, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      "{'loss': 1.7347, 'grad_norm': 0.7479909658432007, 'learning_rate': 1.5687918106563326e-05, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:07<03:22, 14.47s/it][INFO|trainer.py:3719] 2024-07-13 04:45:08,143 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:45:08,144 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:45:08,144 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.30it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.98it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.44it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.31it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.75it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.36it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.61it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.45it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.60it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.38it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.29it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.34it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.79it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.14it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.47it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.12it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.82it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.25it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.75it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.49it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.76it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.89it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.26it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.86it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.57it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.85it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.30it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.14it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.419270634651184, 'eval_accuracy': 0.7254464881778343, 'eval_runtime': 8.3429, 'eval_samples_per_second': 11.986, 'eval_steps_per_second': 5.993, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:15<03:22, 14.47s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.5943, 'grad_norm': 0.790576159954071, 'learning_rate': 6.698729810778065e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      "{'loss': 1.6423, 'grad_norm': 0.7363040447235107, 'learning_rate': 1.3477564710088098e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:44<01:01, 15.37s/it][INFO|trainer.py:3719] 2024-07-13 04:47:45,005 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:47:45,005 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:47:45,005 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.30it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.48it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.71it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.59it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.50it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.22it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.17it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.31it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.81it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.32it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.57it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.88it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.59it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.35it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.64it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4192737340927124, 'eval_accuracy': 0.7249622865672676, 'eval_runtime': 8.2858, 'eval_samples_per_second': 12.069, 'eval_steps_per_second': 6.034, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:52<01:01, 15.37s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:46<00:00, 14.59s/it][INFO|trainer.py:2329] 2024-07-13 04:48:47,015 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 826.1364, 'train_samples_per_second': 3.268, 'train_steps_per_second': 0.065, 'train_loss': 1.6957546693307382, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:46<00:00, 15.30s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-13 04:48:47,017 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:48:47,275 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:48:47,276 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-13 04:48:47,327 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-13 04:48:47,327 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       2.88\r\n",
      "  num_input_tokens_seen    =    1278720\r\n",
      "  total_flos               =  2588760GF\r\n",
      "  train_loss               =     1.6958\r\n",
      "  train_runtime            = 0:13:46.13\r\n",
      "  train_samples_per_second =      3.268\r\n",
      "  train_steps_per_second   =      0.065\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 04:48:47,991 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:48:47,991 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:48:47,991 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.18it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       2.88\r\n",
      "  eval_accuracy           =     0.7248\r\n",
      "  eval_loss               =     1.4192\r\n",
      "  eval_runtime            = 0:00:08.26\r\n",
      "  eval_samples_per_second =     12.098\r\n",
      "  eval_steps_per_second   =      6.049\r\n",
      "  num_input_tokens_seen   =    1278720\r\n",
      "[INFO|modelcard.py:450] 2024-07-13 04:48:56,260 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7248060365672676}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a7eab26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:48:58.405946Z",
     "iopub.status.busy": "2024-07-13T04:48:58.405604Z",
     "iopub.status.idle": "2024-07-13T04:51:26.023958Z",
     "shell.execute_reply": "2024-07-13T04:51:26.022855Z"
    },
    "papermill": {
     "duration": 147.704836,
     "end_time": "2024-07-13T04:51:26.026414",
     "exception": false,
     "start_time": "2024-07-13T04:48:58.321578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 04:49:03.236834: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 04:49:03.236890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 04:49:03.238445: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 04:49:11 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,016 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,017 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,017 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,017 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,017 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:49:12,017 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 04:49:12,275 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 04:49:12 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 04:49:12 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "07/13/2024 04:49:12 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:49:13,146 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:49:13,148 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:49:13 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 04:49:13,182 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 04:49:13,193 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:49:13,195 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 04:49:14,999 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 04:49:14,999 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 04:49:15,094 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:49:15,094 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:49:15 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 04:49:15 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/13/2024 04:49:15 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "07/13/2024 04:49:15 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 04:49:15,696 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:49:15,696 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:49:15,696 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:05<00:00,  1.90s/it]Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.872 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:07<00:00,  1.27s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    29.8367\r\n",
      "  predict_rouge-1            =    36.6785\r\n",
      "  predict_rouge-2            =    14.9457\r\n",
      "  predict_rouge-l            =    32.5098\r\n",
      "  predict_runtime            = 0:02:07.76\r\n",
      "  predict_samples_per_second =      1.565\r\n",
      "  predict_steps_per_second   =      0.783\r\n",
      "07/13/2024 04:51:23 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-35/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-35 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801dd280",
   "metadata": {
    "papermill": {
     "duration": 0.092537,
     "end_time": "2024-07-13T04:51:26.212584",
     "exception": false,
     "start_time": "2024-07-13T04:51:26.120047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 500, lr = 5e-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "353fa560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T04:51:26.401183Z",
     "iopub.status.busy": "2024-07-13T04:51:26.400275Z",
     "iopub.status.idle": "2024-07-13T05:05:39.788373Z",
     "shell.execute_reply": "2024-07-13T05:05:39.787107Z"
    },
    "papermill": {
     "duration": 853.485371,
     "end_time": "2024-07-13T05:05:39.791190",
     "exception": false,
     "start_time": "2024-07-13T04:51:26.305819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 04:51:31.316709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 04:51:31.316771: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 04:51:31.318274: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 04:51:39 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 04:51:40,100 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 04:51:40,352 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 04:51:40 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 04:51:40 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "07/13/2024 04:51:40 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 04:51:41,207 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 04:51:41,209 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 04:51:41,243 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 04:51:41,253 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:51:41,255 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 04:51:43,038 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 04:51:43,038 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 04:51:43,144 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 04:51:43,145 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,o_proj,v_proj,q_proj,gate_proj,down_proj,k_proj\r\n",
      "07/13/2024 04:51:43 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-13 04:51:43,546 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-13 04:51:43,962 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-13 04:51:43,962 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-13 04:51:43,962 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-13 04:51:43,962 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-13 04:51:43,962 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-13 04:51:43,962 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-13 04:51:43,962 >>   Total optimization steps = 54\r\n",
      "[INFO|trainer.py:2087] 2024-07-13 04:51:43,968 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 1.9161, 'grad_norm': 0.756595253944397, 'learning_rate': 0.0004894973780788722, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "{'loss': 1.7605, 'grad_norm': 0.6887317299842834, 'learning_rate': 0.0004588719528532341, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:26<10:55, 14.90s/it][INFO|trainer.py:3719] 2024-07-13 04:54:10,083 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:54:10,083 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:54:10,083 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.33it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.03it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.50it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.79it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.70it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.59it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.45it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.25it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.32it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.85it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.33it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.80it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.88it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.59it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.85it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.34it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.64it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.434049367904663, 'eval_accuracy': 0.7256181796451263, 'eval_runtime': 8.2829, 'eval_samples_per_second': 12.073, 'eval_steps_per_second': 6.037, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:34<10:55, 14.90s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.5582, 'grad_norm': 0.6249284744262695, 'learning_rate': 0.0004106969024216348, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      "{'loss': 1.7307, 'grad_norm': 0.5386167764663696, 'learning_rate': 0.00034901994150978924, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:01<08:36, 15.19s/it][INFO|trainer.py:3719] 2024-07-13 04:56:45,910 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:56:45,910 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:56:45,910 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.51it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.36it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.80it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.42it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.67it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.56it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.40it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.89it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.75it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.87it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.31it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.59it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.92it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.32it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.82it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.32it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.52it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.79it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.92it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.89it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.89it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.37it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.66it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.20it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4229949712753296, 'eval_accuracy': 0.725750425982677, 'eval_runtime': 8.2736, 'eval_samples_per_second': 12.087, 'eval_steps_per_second': 6.043, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:10<08:36, 15.19s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.5801, 'grad_norm': 0.5936943888664246, 'learning_rate': 0.0002790232285313076, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      "{'loss': 1.4729, 'grad_norm': 0.5919080376625061, 'learning_rate': 0.00020658795558326743, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:35<05:46, 14.44s/it][INFO|trainer.py:3719] 2024-07-13 04:59:19,560 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 04:59:19,560 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 04:59:19,560 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.02it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.68it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.57it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.49it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.37it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.27it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.57it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.29it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.78it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.35it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4257380962371826, 'eval_accuracy': 0.7269897763285976, 'eval_runtime': 8.2871, 'eval_samples_per_second': 12.067, 'eval_steps_per_second': 6.033, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [07:43<05:46, 14.44s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.4169, 'grad_norm': 0.5684257745742798, 'learning_rate': 0.00013780020494988447, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      "{'loss': 1.5345, 'grad_norm': 0.6510177254676819, 'learning_rate': 7.843959053281663e-05, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:06<03:21, 14.41s/it][INFO|trainer.py:3719] 2024-07-13 05:01:50,794 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:01:50,794 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:01:50,794 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.33it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.49it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.66it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.56it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.37it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.27it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.57it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.29it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.81it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.55it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.82it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.95it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.34it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.64it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.17it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4353556632995605, 'eval_accuracy': 0.7183005815048279, 'eval_runtime': 8.2879, 'eval_samples_per_second': 12.066, 'eval_steps_per_second': 6.033, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:15<03:21, 14.41s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.3694, 'grad_norm': 0.7092785835266113, 'learning_rate': 3.3493649053890325e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      "{'loss': 1.3931, 'grad_norm': 0.6551541090011597, 'learning_rate': 6.7387823550440485e-06, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:42<01:01, 15.30s/it][INFO|trainer.py:3719] 2024-07-13 05:04:26,639 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:04:26,639 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:04:26,639 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.02it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.98it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.71it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.59it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.49it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.46it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.64it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.89it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.71it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.25it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.55it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.31it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.84it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.33it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.58it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.84it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.97it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.33it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.91it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.62it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.38it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.19it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4403027296066284, 'eval_accuracy': 0.7199117378596658, 'eval_runtime': 8.2717, 'eval_samples_per_second': 12.089, 'eval_steps_per_second': 6.045, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [12:50<01:01, 15.30s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.65it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:44<00:00, 14.59s/it][INFO|trainer.py:2329] 2024-07-13 05:05:28,539 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 824.5708, 'train_samples_per_second': 3.274, 'train_steps_per_second': 0.065, 'train_loss': 1.5577646714669686, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\r\n",
      "100%|███████████████████████████████████████████| 54/54 [13:44<00:00, 15.27s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-13 05:05:28,541 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:05:28,814 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:05:28,815 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-13 05:05:28,867 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-13 05:05:28,867 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       2.88\r\n",
      "  num_input_tokens_seen    =    1278720\r\n",
      "  total_flos               =  2588760GF\r\n",
      "  train_loss               =     1.5578\r\n",
      "  train_runtime            = 0:13:44.57\r\n",
      "  train_samples_per_second =      3.274\r\n",
      "  train_steps_per_second   =      0.065\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 05:05:29,552 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:05:29,552 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:05:29,552 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.18it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       2.88\r\n",
      "  eval_accuracy           =     0.7199\r\n",
      "  eval_loss               =     1.4404\r\n",
      "  eval_runtime            = 0:00:08.26\r\n",
      "  eval_samples_per_second =     12.096\r\n",
      "  eval_steps_per_second   =      6.048\r\n",
      "  num_input_tokens_seen   =    1278720\r\n",
      "[INFO|modelcard.py:450] 2024-07-13 05:05:37,821 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7199117378596658}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-04 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bfbc293",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:05:40.035750Z",
     "iopub.status.busy": "2024-07-13T05:05:40.035384Z",
     "iopub.status.idle": "2024-07-13T05:07:55.316487Z",
     "shell.execute_reply": "2024-07-13T05:07:55.315378Z"
    },
    "papermill": {
     "duration": 135.402201,
     "end_time": "2024-07-13T05:07:55.318812",
     "exception": false,
     "start_time": "2024-07-13T05:05:39.916611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 05:05:44.875975: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 05:05:44.876032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 05:05:44.877694: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 05:05:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:05:53,639 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 05:05:53,888 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 05:05:53 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 05:05:53 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "07/13/2024 05:05:54 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:05:54,731 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:05:54,733 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:05:54 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 05:05:54,767 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 05:05:54,777 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:05:54,779 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 05:05:56,562 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 05:05:56,562 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 05:05:56,650 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:05:56,650 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:05:56 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 05:05:57 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/13/2024 05:05:57 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "07/13/2024 05:05:57 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 05:05:57,271 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:05:57,271 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:05:57,271 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:53<00:00,  2.15s/it]Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.845 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:55<00:00,  1.15s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    29.1855\r\n",
      "  predict_rouge-1            =    36.8191\r\n",
      "  predict_rouge-2            =    14.4832\r\n",
      "  predict_rouge-l            =    32.2254\r\n",
      "  predict_runtime            = 0:01:55.56\r\n",
      "  predict_samples_per_second =      1.731\r\n",
      "  predict_steps_per_second   =      0.865\r\n",
      "07/13/2024 05:07:52 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-36/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-36 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5295003,
     "sourceId": 8869298,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351712,
     "sourceId": 8902036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351730,
     "sourceId": 8902062,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3155.900858,
   "end_time": "2024-07-13T05:07:55.673624",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-13T04:15:19.772766",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
