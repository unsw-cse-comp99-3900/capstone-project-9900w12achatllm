{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cf8b5d4",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T04:37:23.918550Z",
     "iopub.status.busy": "2024-07-11T04:37:23.917673Z",
     "iopub.status.idle": "2024-07-11T04:38:32.944352Z",
     "shell.execute_reply": "2024-07-11T04:38:32.943057Z"
    },
    "papermill": {
     "duration": 69.035545,
     "end_time": "2024-07-11T04:38:32.946925",
     "exception": false,
     "start_time": "2024-07-11T04:37:23.911380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\r\n",
      "remote: Enumerating objects: 15349, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (299/299), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (139/139), done.\u001b[K\r\n",
      "remote: Total 15349 (delta 186), reused 230 (delta 159), pack-reused 15050\u001b[K\r\n",
      "Receiving objects: 100% (15349/15349), 221.57 MiB | 28.23 MiB/s, done.\r\n",
      "Resolving deltas: 100% (11235/11235), done.\r\n",
      "Obtaining file:///kaggle/working/LLaMA-Factory\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (4.41.2)\r\n",
      "Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.19.2)\r\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.30.1)\r\n",
      "Collecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio-4.37.2-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.2.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.11.4)\r\n",
      "Collecting einops (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.2.0)\r\n",
      "Collecting tiktoken (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\r\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.25.0)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.5.3)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.108.0)\r\n",
      "Collecting sse-starlette (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.7.5)\r\n",
      "Collecting fire (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.2.4)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.42.1)\r\n",
      "Collecting rouge-chinese (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.3)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.3.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.1)\r\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (22.1.0)\r\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (5.3.0)\r\n",
      "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting gradio-client==1.0.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio_client-1.0.2-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.27.0)\r\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.1.1)\r\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.3)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.9.10)\r\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.5.0)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.9.0)\r\n",
      "Collecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.2->gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.14.6)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.2.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (0.19.1)\r\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.14.0)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.8.3.dev0) (0.32.0.post1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->llamafactory==0.8.3.dev0) (4.2.0)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.20.0)\r\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.0.3)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.0.5)\r\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.0)\r\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.32.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.16.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\r\n",
      "Downloading gradio-4.37.2-py3-none-any.whl (12.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.0.2-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\r\n",
      "Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\r\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\r\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, ffmpy\r\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20627 sha256=91fcb4cf629e162d52c4b3e90dfd30ab3f56e4b40763038d391e10d858cdc603\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pu77i9b0/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=2347afb75ca30e517fa0a7ac441eed8611f6700e64b97c64f2a2c2bf88330c35\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\r\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=8639e9e63f804f0f5f2914847798fafe0c5cdfb31e3c3a85452ead8b1ac55fc9\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\r\n",
      "Successfully built llamafactory fire ffmpy\r\n",
      "Installing collected packages: ffmpy, websockets, urllib3, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, fire, einops, docstring-parser, tyro, typer, tiktoken, sse-starlette, gradio-client, gradio, trl, peft, llamafactory\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 12.0\r\n",
      "    Uninstalling websockets-12.0:\r\n",
      "      Successfully uninstalled websockets-12.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: tomlkit\r\n",
      "    Found existing installation: tomlkit 0.12.5\r\n",
      "    Uninstalling tomlkit-0.12.5:\r\n",
      "      Successfully uninstalled tomlkit-0.12.5\r\n",
      "  Attempting uninstall: docstring-parser\r\n",
      "    Found existing installation: docstring-parser 0.15\r\n",
      "    Uninstalling docstring-parser-0.15:\r\n",
      "      Successfully uninstalled docstring-parser-0.15\r\n",
      "  Attempting uninstall: typer\r\n",
      "    Found existing installation: typer 0.9.0\r\n",
      "    Uninstalling typer-0.9.0:\r\n",
      "      Successfully uninstalled typer-0.9.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 einops-0.8.0 ffmpy-0.3.2 fire-0.6.0 gradio-4.37.2 gradio-client-1.0.2 llamafactory-0.8.3.dev0 peft-0.11.1 python-multipart-0.0.9 rouge-chinese-1.0.3 ruff-0.5.1 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 typer-0.12.3 tyro-0.8.5 urllib3-2.1.0 websockets-11.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\"\n",
    "!pip install bitsandbytes>=0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a32fb95a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:38:32.980399Z",
     "iopub.status.busy": "2024-07-11T04:38:32.979706Z",
     "iopub.status.idle": "2024-07-11T04:38:32.988551Z",
     "shell.execute_reply": "2024-07-11T04:38:32.987713Z"
    },
    "papermill": {
     "duration": 0.027156,
     "end_time": "2024-07-11T04:38:32.990671",
     "exception": false,
     "start_time": "2024-07-11T04:38:32.963515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/kaggle/working/LLaMA-Factory/data/dataset_info.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# configure the MedQA and PubMedQA dataset information \n",
    "new_data = {\n",
    "    \"MedQA_train\": {\"file_name\": \"MedQA/train.json\"},\n",
    "    \"MedQA_test\": {\"file_name\": \"MedQA/test.json\"},\n",
    "    \"PubMedQA_pqal_train\": {\"file_name\": \"PubMedQA/pqal_train_set.json\"},\n",
    "    \"PubMedQA_pqal_test\": {\"file_name\": \"PubMedQA/pqal_test_set.json\"}\n",
    "}\n",
    "\n",
    "# update information in dataset_info.json file\n",
    "data.update(new_data)\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4916e85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:38:33.021321Z",
     "iopub.status.busy": "2024-07-11T04:38:33.021049Z",
     "iopub.status.idle": "2024-07-11T04:38:34.023994Z",
     "shell.execute_reply": "2024-07-11T04:38:34.023088Z"
    },
    "papermill": {
     "duration": 1.020597,
     "end_time": "2024-07-11T04:38:34.026323",
     "exception": false,
     "start_time": "2024-07-11T04:38:33.005726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa-pubmedqa\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cff918b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:38:34.060395Z",
     "iopub.status.busy": "2024-07-11T04:38:34.060075Z",
     "iopub.status.idle": "2024-07-11T04:38:40.125179Z",
     "shell.execute_reply": "2024-07-11T04:38:40.123947Z"
    },
    "papermill": {
     "duration": 6.084302,
     "end_time": "2024-07-11T04:38:40.127541",
     "exception": false,
     "start_time": "2024-07-11T04:38:34.043239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create MedQA and PubMedQA directories\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/MedQA\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/PubMedQA\n",
    "\n",
    "# copy MedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/train.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/test.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "\n",
    "# copy PubMedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_train_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_test_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83381e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:38:40.158967Z",
     "iopub.status.busy": "2024-07-11T04:38:40.158648Z",
     "iopub.status.idle": "2024-07-11T04:38:40.165087Z",
     "shell.execute_reply": "2024-07-11T04:38:40.164129Z"
    },
    "papermill": {
     "duration": 0.024481,
     "end_time": "2024-07-11T04:38:40.167054",
     "exception": false,
     "start_time": "2024-07-11T04:38:40.142573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0514c6fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:38:40.198950Z",
     "iopub.status.busy": "2024-07-11T04:38:40.198595Z",
     "iopub.status.idle": "2024-07-11T04:39:08.980804Z",
     "shell.execute_reply": "2024-07-11T04:39:08.979645Z"
    },
    "papermill": {
     "duration": 28.80125,
     "end_time": "2024-07-11T04:39:08.983169",
     "exception": false,
     "start_time": "2024-07-11T04:38:40.181919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\r\n",
      "  Downloading optimum-1.21.2-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting coloredlogs (from optimum)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12.1)\r\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.41.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.23.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.19.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.3.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (3.20.3)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\r\n",
      "Downloading optimum-1.21.2-py3-none-any.whl (424 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.21.2\r\n"
     ]
    }
   ],
   "source": [
    "# install packages for quantizaiton\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa6e07",
   "metadata": {
    "papermill": {
     "duration": 0.016383,
     "end_time": "2024-07-11T04:39:09.016793",
     "exception": false,
     "start_time": "2024-07-11T04:39:09.000410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed835fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:39:09.052130Z",
     "iopub.status.busy": "2024-07-11T04:39:09.051316Z",
     "iopub.status.idle": "2024-07-11T04:46:56.453975Z",
     "shell.execute_reply": "2024-07-11T04:46:56.453020Z"
    },
    "papermill": {
     "duration": 467.422915,
     "end_time": "2024-07-11T04:46:56.456353",
     "exception": false,
     "start_time": "2024-07-11T04:39:09.033438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 04:39:18.735627: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 04:39:18.735754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 04:39:18.884104: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 04:39:35 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 8.01MB/s]\r\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 35.9MB/s]\r\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 22.3MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:00<00:00, 58.7MB/s]\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:39:37,222 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 04:39:37,517 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 04:39:37 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 04:39:37 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "Generating train split: 10178 examples [00:00, 123330.52 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 250/250 [00:00<00:00, 587.57\r\n",
      "07/11/2024 04:39:38 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "Generating train split: 900 examples [00:00, 41600.99 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 250/250 [00:00<00:00, 745.18\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 500/500 [00:03<00:00, 140.49\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 4.80MB/s]\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 04:39:43,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 04:39:43,709 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "model.safetensors: 100%|██████████████████████| 988M/988M [00:04<00:00, 222MB/s]\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 04:39:48,530 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 04:39:48,559 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:39:48,562 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 04:39:50,664 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 04:39:50,664 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.38MB/s]\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 04:39:50,876 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:39:50,877 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 04:39:50 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/11/2024 04:39:50 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 04:39:50 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/11/2024 04:39:50 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/11/2024 04:39:50 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,down_proj,up_proj,o_proj,q_proj,gate_proj,k_proj\r\n",
      "07/11/2024 04:39:51 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-11 04:39:51,309 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-11 04:39:51,738 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-11 04:39:51,738 >>   Num examples = 450\r\n",
      "[INFO|trainer.py:2080] 2024-07-11 04:39:51,738 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-11 04:39:51,738 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-11 04:39:51,738 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-11 04:39:51,738 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-11 04:39:51,738 >>   Total optimization steps = 27\r\n",
      "[INFO|trainer.py:2087] 2024-07-11 04:39:51,744 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 2.0757, 'grad_norm': 1.133683204650879, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 121536}\r\n",
      " 19%|████████▏                                   | 5/27 [01:15<05:33, 15.16s/it][INFO|trainer.py:3719] 2024-07-11 04:41:07,314 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:41:07,314 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:41:07,314 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 13.39it/s]\u001b[A\r\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.37it/s]\u001b[A\r\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.61it/s]\u001b[A\r\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  6.90it/s]\u001b[A\r\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  7.20it/s]\u001b[A\r\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.59it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  6.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.66it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  6.77it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  6.10it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  8.02it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.91it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.54it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  7.23it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  5.99it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.42it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.04it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.09it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.6721937656402588, 'eval_accuracy': 0.7198571078966041, 'eval_runtime': 4.1578, 'eval_samples_per_second': 12.026, 'eval_steps_per_second': 6.013, 'epoch': 0.53, 'num_input_tokens_seen': 121536}\r\n",
      " 19%|████████▏                                   | 5/27 [01:19<05:33, 15.16s/it]\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  4.63it/s]\u001b[A\r\n",
      "{'loss': 1.831, 'grad_norm': 1.0777555704116821, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 236640}\r\n",
      " 37%|███████████████▉                           | 10/27 [02:29<04:09, 14.70s/it][INFO|trainer.py:3719] 2024-07-11 04:42:21,501 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:42:21,501 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:42:21,501 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 13.38it/s]\u001b[A\r\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.38it/s]\u001b[A\r\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.64it/s]\u001b[A\r\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  6.92it/s]\u001b[A\r\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  7.21it/s]\u001b[A\r\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.60it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  6.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.64it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  6.75it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  6.07it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  7.98it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.88it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.53it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  7.30it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  6.04it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.46it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.07it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.11it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.6014888286590576, 'eval_accuracy': 0.7453179058723565, 'eval_runtime': 4.1383, 'eval_samples_per_second': 12.082, 'eval_steps_per_second': 6.041, 'epoch': 1.07, 'num_input_tokens_seen': 236640}\r\n",
      " 37%|███████████████▉                           | 10/27 [02:33<04:09, 14.70s/it]\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  4.64it/s]\u001b[A\r\n",
      "{'loss': 1.7927, 'grad_norm': 0.8723506927490234, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 358224}\r\n",
      " 56%|███████████████████████▉                   | 15/27 [03:48<03:01, 15.12s/it][INFO|trainer.py:3719] 2024-07-11 04:43:40,265 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:43:40,265 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:43:40,265 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 13.41it/s]\u001b[A\r\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.38it/s]\u001b[A\r\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.64it/s]\u001b[A\r\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  6.92it/s]\u001b[A\r\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  7.21it/s]\u001b[A\r\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.60it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  6.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.67it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  6.78it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  6.09it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  8.02it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.91it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.54it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  7.29it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  6.03it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.44it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.05it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.09it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.565363883972168, 'eval_accuracy': 0.7425531327072179, 'eval_runtime': 4.1389, 'eval_samples_per_second': 12.081, 'eval_steps_per_second': 6.04, 'epoch': 1.6, 'num_input_tokens_seen': 358224}\r\n",
      " 56%|███████████████████████▉                   | 15/27 [03:52<03:01, 15.12s/it]\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  4.63it/s]\u001b[A\r\n",
      "{'loss': 1.7234, 'grad_norm': 0.7737622857093811, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 473712}\r\n",
      " 74%|███████████████████████████████▊           | 20/27 [05:03<01:43, 14.78s/it][INFO|trainer.py:3719] 2024-07-11 04:44:55,144 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:44:55,144 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:44:55,144 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 13.39it/s]\u001b[A\r\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.39it/s]\u001b[A\r\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.65it/s]\u001b[A\r\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  6.93it/s]\u001b[A\r\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  7.22it/s]\u001b[A\r\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.60it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  6.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.70it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  6.80it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  6.12it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  8.07it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.93it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.57it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  7.34it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  6.05it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.44it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.06it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.09it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5501469373703003, 'eval_accuracy': 0.7454665469254167, 'eval_runtime': 4.1319, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 6.05, 'epoch': 2.13, 'num_input_tokens_seen': 473712}\r\n",
      " 74%|███████████████████████████████▊           | 20/27 [05:07<01:43, 14.78s/it]\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  4.63it/s]\u001b[A\r\n",
      "{'loss': 1.7848, 'grad_norm': 0.853995680809021, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 595296}\r\n",
      " 93%|███████████████████████████████████████▊   | 25/27 [06:22<00:30, 15.30s/it][INFO|trainer.py:3719] 2024-07-11 04:46:14,001 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:46:14,002 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:46:14,002 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 2/25 [00:00<00:01, 13.40it/s]\u001b[A\r\n",
      " 16%|███████                                     | 4/25 [00:00<00:02,  9.39it/s]\u001b[A\r\n",
      " 24%|██████████▌                                 | 6/25 [00:00<00:02,  6.63it/s]\u001b[A\r\n",
      " 28%|████████████▎                               | 7/25 [00:00<00:02,  6.92it/s]\u001b[A\r\n",
      " 32%|██████████████                              | 8/25 [00:01<00:02,  7.21it/s]\u001b[A\r\n",
      " 36%|███████████████▊                            | 9/25 [00:01<00:02,  6.59it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 10/25 [00:01<00:02,  6.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 12/25 [00:01<00:01,  7.68it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 13/25 [00:01<00:01,  6.78it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 14/25 [00:02<00:01,  6.09it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 16/25 [00:02<00:01,  8.02it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 17/25 [00:02<00:01,  6.91it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 18/25 [00:02<00:01,  6.54it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 20/25 [00:02<00:00,  7.31it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 21/25 [00:03<00:00,  6.04it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 22/25 [00:03<00:00,  5.45it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 23/25 [00:03<00:00,  5.07it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 24/25 [00:03<00:00,  5.11it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5464825630187988, 'eval_accuracy': 0.7462283244062052, 'eval_runtime': 4.1336, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 6.048, 'epoch': 2.67, 'num_input_tokens_seen': 595296}\r\n",
      " 93%|███████████████████████████████████████▊   | 25/27 [06:26<00:30, 15.30s/it]\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  4.64it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 27/27 [06:56<00:00, 16.17s/it][INFO|trainer.py:2329] 2024-07-11 04:46:48,503 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 416.7594, 'train_samples_per_second': 3.239, 'train_steps_per_second': 0.065, 'train_loss': 1.8433136586789731, 'epoch': 2.88, 'num_input_tokens_seen': 644400}\r\n",
      "100%|███████████████████████████████████████████| 27/27 [06:56<00:00, 15.44s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-11 04:46:48,505 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 04:46:48,785 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 04:46:48,786 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-11 04:46:48,838 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-11 04:46:48,838 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       2.88\r\n",
      "  num_input_tokens_seen    =     644400\r\n",
      "  total_flos               =  1304583GF\r\n",
      "  train_loss               =     1.8433\r\n",
      "  train_runtime            = 0:06:56.75\r\n",
      "  train_samples_per_second =      3.239\r\n",
      "  train_steps_per_second   =      0.065\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 04:46:49,613 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:46:49,613 >>   Num examples = 50\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:46:49,613 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 25/25 [00:03<00:00,  6.29it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       2.88\r\n",
      "  eval_accuracy           =     0.7462\r\n",
      "  eval_loss               =     1.5463\r\n",
      "  eval_runtime            = 0:00:04.13\r\n",
      "  eval_samples_per_second =     12.103\r\n",
      "  eval_steps_per_second   =      6.051\r\n",
      "  num_input_tokens_seen   =     644400\r\n",
      "[INFO|modelcard.py:450] 2024-07-11 04:46:53,747 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7462283244062052}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 250 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-25 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 5 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942c63c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:46:56.529003Z",
     "iopub.status.busy": "2024-07-11T04:46:56.528217Z",
     "iopub.status.idle": "2024-07-11T04:49:38.654445Z",
     "shell.execute_reply": "2024-07-11T04:49:38.653431Z"
    },
    "papermill": {
     "duration": 162.165127,
     "end_time": "2024-07-11T04:49:38.656968",
     "exception": false,
     "start_time": "2024-07-11T04:46:56.491841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 04:47:01.571791: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 04:47:01.571847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 04:47:01.573349: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 04:47:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:47:10,320 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 04:47:10,571 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 04:47:10 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 04:47:10 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "Generating train split: 1273 examples [00:00, 98812.79 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 229.84\r\n",
      "07/11/2024 04:47:11 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "Generating train split: 100 examples [00:00, 23224.27 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 261.54\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 200/200 [00:03<00:00, 61.22 \r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 04:47:16,401 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 04:47:16,403 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 04:47:16 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 04:47:16,439 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 04:47:16,451 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:47:16,453 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 04:47:18,445 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 04:47:18,446 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 04:47:18,660 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:47:18,661 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 04:47:18 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 04:47:19 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/11/2024 04:47:19 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-25\r\n",
      "07/11/2024 04:47:19 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 04:47:19,296 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:47:19,296 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:47:19,296 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:13<00:00,  2.44s/it]Building prefix dict from the default dictionary ...\r\n",
      "Dumping model to file cache /tmp/jieba.cache\r\n",
      "Loading model cost 1.113 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:15<00:00,  1.36s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    28.3873\r\n",
      "  predict_rouge-1            =    36.3596\r\n",
      "  predict_rouge-2            =     17.395\r\n",
      "  predict_rouge-l            =    32.7366\r\n",
      "  predict_runtime            = 0:02:16.61\r\n",
      "  predict_samples_per_second =      1.464\r\n",
      "  predict_steps_per_second   =      0.732\r\n",
      "07/11/2024 04:49:35 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-34/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-34 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b93321",
   "metadata": {
    "papermill": {
     "duration": 0.050585,
     "end_time": "2024-07-11T04:49:38.757716",
     "exception": false,
     "start_time": "2024-07-11T04:49:38.707131",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fb3e99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T04:49:38.851744Z",
     "iopub.status.busy": "2024-07-11T04:49:38.851375Z",
     "iopub.status.idle": "2024-07-11T05:04:39.935669Z",
     "shell.execute_reply": "2024-07-11T05:04:39.934740Z"
    },
    "papermill": {
     "duration": 901.133973,
     "end_time": "2024-07-11T05:04:39.937982",
     "exception": false,
     "start_time": "2024-07-11T04:49:38.804009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 04:49:43.768470: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 04:49:43.768526: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 04:49:43.770092: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 04:49:52 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,581 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,581 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,581 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,582 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,582 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 04:49:52,582 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 04:49:52,847 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 04:49:52 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 04:49:52 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1243.3\r\n",
      "07/11/2024 04:49:53 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1304.1\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:03<00:00, 264.\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 04:49:59,184 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 04:49:59,187 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 04:49:59,233 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 04:49:59,249 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:49:59,252 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 04:50:01,087 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 04:50:01,087 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 04:50:01,189 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 04:50:01,189 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,q_proj,gate_proj,o_proj,k_proj,v_proj,down_proj\r\n",
      "07/11/2024 04:50:01 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-11 04:50:01,619 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-11 04:50:02,049 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-11 04:50:02,049 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-11 04:50:02,049 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-11 04:50:02,049 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-11 04:50:02,050 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-11 04:50:02,050 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-11 04:50:02,050 >>   Total optimization steps = 54\r\n",
      "[INFO|trainer.py:2087] 2024-07-11 04:50:02,055 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 2.0502, 'grad_norm': 1.2584071159362793, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "  9%|████                                        | 5/54 [01:12<11:46, 14.41s/it][INFO|trainer.py:3719] 2024-07-11 04:51:14,326 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:51:14,326 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:51:14,326 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.30it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.01it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.49it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.77it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.65it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.56it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.70it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.24it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.55it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.29it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.56it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.83it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.95it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.28it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.85it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.56it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.85it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.23it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.56it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.13it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.6102389097213745, 'eval_accuracy': 0.6986974234439193, 'eval_runtime': 8.3153, 'eval_samples_per_second': 12.026, 'eval_steps_per_second': 6.013, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "  9%|████                                        | 5/54 [01:20<11:46, 14.41s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.58it/s]\u001b[A\r\n",
      "{'loss': 1.9023, 'grad_norm': 1.1184602975845337, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:34<11:22, 15.52s/it][INFO|trainer.py:3719] 2024-07-11 04:52:36,587 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:52:36,587 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:52:36,587 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.33it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.50it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.77it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.56it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.49it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.43it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.60it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.39it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.29it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.34it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.26it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.85it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.27it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.79it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.62it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.32it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.16it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5271893739700317, 'eval_accuracy': 0.7178652118914941, 'eval_runtime': 8.308, 'eval_samples_per_second': 12.037, 'eval_steps_per_second': 6.018, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 19%|███████▉                                   | 10/54 [02:42<11:22, 15.52s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.638, 'grad_norm': 0.9001755714416504, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      " 28%|███████████▉                               | 15/54 [03:52<09:28, 14.57s/it][INFO|trainer.py:3719] 2024-07-11 04:53:54,081 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:53:54,081 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:53:54,081 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.50it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.79it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.67it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.56it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.49it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.59it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.39it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.31it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.23it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.54it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.87it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.79it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.16it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.88it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.34it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.17it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4814419746398926, 'eval_accuracy': 0.7236170524034955, 'eval_runtime': 8.2947, 'eval_samples_per_second': 12.056, 'eval_steps_per_second': 6.028, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      " 28%|███████████▉                               | 15/54 [04:00<09:28, 14.57s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.8208, 'grad_norm': 0.8166706562042236, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:18<08:57, 15.81s/it][INFO|trainer.py:3719] 2024-07-11 04:55:20,703 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:55:20,703 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:55:20,703 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.98it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.99it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.75it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.63it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.55it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.71it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.26it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.19it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.90it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.81it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.54it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.88it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.58it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.85it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.33it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.17it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.46001398563385, 'eval_accuracy': 0.7271351976101371, 'eval_runtime': 8.2967, 'eval_samples_per_second': 12.053, 'eval_steps_per_second': 6.026, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 37%|███████████████▉                           | 20/54 [05:26<08:57, 15.81s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7629, 'grad_norm': 0.8164336681365967, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      " 46%|███████████████████▉                       | 25/54 [06:40<07:30, 15.54s/it][INFO|trainer.py:3719] 2024-07-11 04:56:43,028 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:56:43,028 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:56:43,028 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.48it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.68it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.58it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.45it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.33it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.30it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.70it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.12it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.47it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.13it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.88it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.55it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.82it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.94it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.31it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.61it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.17it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.89it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.35it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.64it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4497332572937012, 'eval_accuracy': 0.7257041302276306, 'eval_runtime': 8.2947, 'eval_samples_per_second': 12.056, 'eval_steps_per_second': 6.028, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      " 46%|███████████████████▉                       | 25/54 [06:49<07:30, 15.54s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.6429, 'grad_norm': 0.7100741863250732, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [08:00<06:01, 15.06s/it][INFO|trainer.py:3719] 2024-07-11 04:58:02,640 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:58:02,640 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:58:02,640 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.50it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.36it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.77it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.69it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.58it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.45it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.86it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.23it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.79it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.35it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.28it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.58it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.20it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.32it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.85it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.32it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.55it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.87it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.55it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.10it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.83it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.19it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.54it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.12it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4416371583938599, 'eval_accuracy': 0.7270358919245823, 'eval_runtime': 8.3116, 'eval_samples_per_second': 12.031, 'eval_steps_per_second': 6.016, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 56%|███████████████████████▉                   | 30/54 [08:08<06:01, 15.06s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.58it/s]\u001b[A\r\n",
      "{'loss': 1.6129, 'grad_norm': 0.7778649926185608, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      " 65%|███████████████████████████▊               | 35/54 [09:16<04:32, 14.37s/it][INFO|trainer.py:3719] 2024-07-11 04:59:18,423 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 04:59:18,424 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 04:59:18,424 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.49it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.67it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.57it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.48it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.44it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.36it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.71it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.25it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.29it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.79it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.55it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.80it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.92it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.87it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.59it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.87it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.32it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.16it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.437058687210083, 'eval_accuracy': 0.7282830175484486, 'eval_runtime': 8.2966, 'eval_samples_per_second': 12.053, 'eval_steps_per_second': 6.027, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      " 65%|███████████████████████████▊               | 35/54 [09:24<04:32, 14.37s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7812, 'grad_norm': 0.8400895595550537, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:40<03:30, 15.02s/it][INFO|trainer.py:3719] 2024-07-11 05:00:42,187 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:00:42,187 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:00:42,187 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:05,  7.51it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.36it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.79it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.42it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.71it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.58it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.50it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.45it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.63it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.88it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.84it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.85it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.22it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.17it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.53it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.75it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.88it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.27it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.87it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.58it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.87it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.37it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.66it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.19it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.434517502784729, 'eval_accuracy': 0.7283940558069336, 'eval_runtime': 8.2872, 'eval_samples_per_second': 12.067, 'eval_steps_per_second': 6.033, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 74%|███████████████████████████████▊           | 40/54 [10:48<03:30, 15.02s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.6392, 'grad_norm': 0.8254467248916626, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      " 83%|███████████████████████████████████▊       | 45/54 [12:00<02:16, 15.14s/it][INFO|trainer.py:3719] 2024-07-11 05:02:02,663 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:02:02,663 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:02:02,663 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.32it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  6.00it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.02it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.50it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.35it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.78it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.60it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.53it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.87it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.32it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.85it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.38it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.87it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.73it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.28it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.57it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.32it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.84it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.32it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.56it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:05<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.90it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.60it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.87it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.37it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.66it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.19it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4333683252334595, 'eval_accuracy': 0.7299331744225052, 'eval_runtime': 8.2854, 'eval_samples_per_second': 12.069, 'eval_steps_per_second': 6.035, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      " 83%|███████████████████████████████████▊       | 45/54 [12:08<02:16, 15.14s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.64it/s]\u001b[A\r\n",
      "{'loss': 1.6895, 'grad_norm': 0.8361890316009521, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [13:24<01:03, 15.87s/it][INFO|trainer.py:3719] 2024-07-11 05:03:26,347 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:03:26,347 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:03:26,347 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.28it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.99it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.99it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.47it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.76it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.40it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.62it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.54it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.45it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.29it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.83it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.37it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.72it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.25it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.56it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.18it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.91it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.31it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.55it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.30it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.89it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.60it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.15it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.87it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.37it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.65it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.18it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4330073595046997, 'eval_accuracy': 0.7302479592451208, 'eval_runtime': 8.3, 'eval_samples_per_second': 12.048, 'eval_steps_per_second': 6.024, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [13:32<01:03, 15.87s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.60it/s]\u001b[A\r\n",
      "100%|███████████████████████████████████████████| 54/54 [14:26<00:00, 14.72s/it][INFO|trainer.py:2329] 2024-07-11 05:04:28,378 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 866.3224, 'train_samples_per_second': 3.117, 'train_steps_per_second': 0.062, 'train_loss': 1.7465689447191026, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\r\n",
      "100%|███████████████████████████████████████████| 54/54 [14:26<00:00, 16.04s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-11 05:04:28,380 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 05:04:28,863 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 05:04:28,865 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-11 05:04:28,916 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-11 05:04:28,917 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       2.88\r\n",
      "  num_input_tokens_seen    =    1278720\r\n",
      "  total_flos               =  2588760GF\r\n",
      "  train_loss               =     1.7466\r\n",
      "  train_runtime            = 0:14:26.32\r\n",
      "  train_samples_per_second =      3.117\r\n",
      "  train_steps_per_second   =      0.062\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 05:04:29,598 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:04:29,599 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:04:29,599 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.17it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       2.88\r\n",
      "  eval_accuracy           =     0.7306\r\n",
      "  eval_loss               =      1.433\r\n",
      "  eval_runtime            = 0:00:08.27\r\n",
      "  eval_samples_per_second =     12.083\r\n",
      "  eval_steps_per_second   =      6.041\r\n",
      "  num_input_tokens_seen   =    1278720\r\n",
      "[INFO|modelcard.py:450] 2024-07-11 05:04:37,877 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7305958603377027}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 5 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c8c4e44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:04:40.124052Z",
     "iopub.status.busy": "2024-07-11T05:04:40.123733Z",
     "iopub.status.idle": "2024-07-11T05:07:18.009523Z",
     "shell.execute_reply": "2024-07-11T05:07:18.008325Z"
    },
    "papermill": {
     "duration": 157.983768,
     "end_time": "2024-07-11T05:07:18.012014",
     "exception": false,
     "start_time": "2024-07-11T05:04:40.028246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 05:04:44.985601: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 05:04:44.985657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 05:04:44.987127: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 05:04:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:04:53,719 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 05:04:53,998 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 05:04:53 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 05:04:53 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "07/11/2024 05:04:54 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 05:04:54,853 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 05:04:54,855 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 05:04:54 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 05:04:54,891 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 05:04:54,903 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:04:54,906 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 05:04:56,679 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 05:04:56,679 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 05:04:56,769 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:04:56,770 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 05:04:56 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 05:04:57 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/11/2024 05:04:57 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "07/11/2024 05:04:57 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 05:04:57,379 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:04:57,379 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:04:57,379 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:15<00:00,  2.61s/it]Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.881 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    30.2035\r\n",
      "  predict_rouge-1            =    38.2005\r\n",
      "  predict_rouge-2            =    17.4951\r\n",
      "  predict_rouge-l            =    33.7111\r\n",
      "  predict_runtime            = 0:02:18.09\r\n",
      "  predict_samples_per_second =      1.448\r\n",
      "  predict_steps_per_second   =      0.724\r\n",
      "07/11/2024 05:07:15 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-35/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-35 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981538c",
   "metadata": {
    "papermill": {
     "duration": 0.098714,
     "end_time": "2024-07-11T05:07:18.230465",
     "exception": false,
     "start_time": "2024-07-11T05:07:18.131751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ad353d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:07:18.432807Z",
     "iopub.status.busy": "2024-07-11T05:07:18.432439Z",
     "iopub.status.idle": "2024-07-11T05:39:04.128456Z",
     "shell.execute_reply": "2024-07-11T05:39:04.127335Z"
    },
    "papermill": {
     "duration": 1905.800069,
     "end_time": "2024-07-11T05:39:04.130618",
     "exception": false,
     "start_time": "2024-07-11T05:07:18.330549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 05:07:23.268568: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 05:07:23.268626: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 05:07:23.270165: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 05:07:31 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,002 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,002 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,002 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,002 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,002 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:07:32,003 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 05:07:32,265 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 05:07:32 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 05:07:32 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 1000/1000 [00:00<00:00, 1882\r\n",
      "07/11/2024 05:07:33 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 900/900 [00:00<00:00, 2196.3\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1900/1900 [00:04<00:00, 451.\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 05:07:39,224 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 05:07:39,227 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 05:07:39,281 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 05:07:39,298 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:07:39,301 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 05:07:41,233 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 05:07:41,233 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 05:07:41,334 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:07:41,335 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,o_proj,v_proj,gate_proj,down_proj,up_proj\r\n",
      "07/11/2024 05:07:41 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-11 05:07:41,766 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-11 05:07:42,179 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-11 05:07:42,179 >>   Num examples = 1,710\r\n",
      "[INFO|trainer.py:2080] 2024-07-11 05:07:42,179 >>   Num Epochs = 3\r\n",
      "[INFO|trainer.py:2081] 2024-07-11 05:07:42,179 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-11 05:07:42,179 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-11 05:07:42,179 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-11 05:07:42,179 >>   Total optimization steps = 105\r\n",
      "[INFO|trainer.py:2087] 2024-07-11 05:07:42,185 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 2.0173, 'grad_norm': 1.1895174980163574, 'learning_rate': 4.972077065562821e-05, 'epoch': 0.14, 'num_input_tokens_seen': 121488}\r\n",
      "  5%|██                                         | 5/105 [01:14<24:59, 14.99s/it][INFO|trainer.py:3719] 2024-07-11 05:08:56,922 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:08:56,922 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:08:56,922 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.97it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.91it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.83it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.56it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.75it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.97it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.79it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.29it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.01it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  5.94it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:13,  5.99it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.41it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.04it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.54it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.39it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.05it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:04<00:12,  5.72it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:10,  6.60it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.23it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.84it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.97it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.76it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.44it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.60it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.47it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.95it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.78it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.30it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.59it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.97it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.53it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.31it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.29it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.16it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.38it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.88it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.30it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.20it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.27it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.58it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.46it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.80it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.12it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.32it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.28it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.29it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.87it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.75it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.75it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.89it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.21it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.69it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.55it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.46it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5878522396087646, 'eval_accuracy': 0.7104682047713633, 'eval_runtime': 15.6443, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 6.073, 'epoch': 0.14, 'num_input_tokens_seen': 121488}\r\n",
      "  5%|██                                         | 5/105 [01:30<24:59, 14.99s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7686, 'grad_norm': 1.1397461891174316, 'learning_rate': 4.888932014465352e-05, 'epoch': 0.28, 'num_input_tokens_seen': 233520}\r\n",
      " 10%|████                                      | 10/105 [02:39<24:37, 15.55s/it][INFO|trainer.py:3719] 2024-07-11 05:10:21,892 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:10:21,892 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:10:21,892 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.00it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 4/95 [00:00<00:09,  9.16it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.60it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.45it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.72it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.94it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.77it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.28it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.17it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.03it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.06it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.47it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.09it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.74it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.36it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.61it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.25it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.88it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.00it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.49it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  6.97it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.32it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.87it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.98it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.53it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.28it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.29it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.16it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.37it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.87it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.31it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.24it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.36it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.30it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.58it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.46it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.80it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.12it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.31it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.89it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.72it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.76it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.92it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.22it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.70it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.5041003227233887, 'eval_accuracy': 0.7309637075700907, 'eval_runtime': 15.5852, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 6.096, 'epoch': 0.28, 'num_input_tokens_seen': 233520}\r\n",
      " 10%|████                                      | 10/105 [02:55<24:37, 15.55s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.7365, 'grad_norm': 0.9180939197540283, 'learning_rate': 4.752422169756048e-05, 'epoch': 0.42, 'num_input_tokens_seen': 348144}\r\n",
      " 14%|██████                                    | 15/105 [04:06<22:54, 15.27s/it][INFO|trainer.py:3719] 2024-07-11 05:11:48,528 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:11:48,528 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:11:48,529 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.00it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.97it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.88it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.58it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.78it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.99it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.80it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.30it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.19it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.04it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.10it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.49it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.41it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.07it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.72it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.34it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:10,  6.59it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.23it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.85it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.99it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.91it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.76it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.29it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.58it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.83it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.87it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.45it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.24it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.26it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.14it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.36it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.84it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.29it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.15it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.29it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.25it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.57it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.45it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.78it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.11it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.33it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.28it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.86it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.75it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.74it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.89it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.20it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.68it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.54it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.46it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.22it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.38it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.45990788936615, 'eval_accuracy': 0.7392003113860317, 'eval_runtime': 15.6311, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 6.078, 'epoch': 0.42, 'num_input_tokens_seen': 348144}\r\n",
      " 14%|██████                                    | 15/105 [04:21<22:54, 15.27s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7601, 'grad_norm': 0.8346230983734131, 'learning_rate': 4.5655969357899874e-05, 'epoch': 0.56, 'num_input_tokens_seen': 470112}\r\n",
      " 19%|████████                                  | 20/105 [05:36<23:06, 16.32s/it][INFO|trainer.py:3719] 2024-07-11 05:13:18,796 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:13:18,796 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:13:18,796 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.01it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 4/95 [00:00<00:09,  9.13it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.58it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.43it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.69it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.92it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.75it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.26it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.17it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.03it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.04it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.47it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.07it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.57it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.41it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.75it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.28it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.36it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.60it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.24it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.86it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.99it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.78it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  7.01it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.33it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.63it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.87it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.99it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.53it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.31it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.30it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.16it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.38it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.84it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.29it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.23it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.35it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.29it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.59it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.45it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.79it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.11it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.33it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.27it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.86it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.81it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.81it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.96it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.25it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.57it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.62it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.24it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4374563694000244, 'eval_accuracy': 0.7414743550564097, 'eval_runtime': 15.5899, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 6.094, 'epoch': 0.56, 'num_input_tokens_seen': 470112}\r\n",
      " 19%|████████                                  | 20/105 [05:52<23:06, 16.32s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7924, 'grad_norm': 0.8555943369865417, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.7, 'num_input_tokens_seen': 592992}\r\n",
      " 24%|██████████                                | 25/105 [07:08<22:21, 16.77s/it][INFO|trainer.py:3719] 2024-07-11 05:14:50,685 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:14:50,685 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:14:50,685 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.97it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.88it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.60it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.79it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.99it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.81it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.30it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.23it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.07it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.11it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.49it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.07it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.74it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.28it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.36it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.61it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.24it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.87it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.99it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.87it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.96it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.31it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.85it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.88it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.46it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.25it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.25it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.13it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.36it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.86it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.30it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.25it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.37it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.50it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.82it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.13it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.30it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.88it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.79it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.93it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.24it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.61it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4286035299301147, 'eval_accuracy': 0.7447241917430145, 'eval_runtime': 15.5899, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 6.094, 'epoch': 0.7, 'num_input_tokens_seen': 592992}\r\n",
      " 24%|██████████                                | 25/105 [07:24<22:21, 16.77s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.5709, 'grad_norm': 0.9102014899253845, 'learning_rate': 4.058724504646834e-05, 'epoch': 0.84, 'num_input_tokens_seen': 710976}\r\n",
      " 29%|████████████                              | 30/105 [08:36<19:53, 15.92s/it][INFO|trainer.py:3719] 2024-07-11 05:16:18,971 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:16:18,971 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:16:18,971 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.96it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.88it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.61it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.81it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.01it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.81it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.22it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.06it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.49it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.09it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.03it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.71it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.25it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.34it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.61it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.24it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.88it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.00it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.78it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.47it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.94it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.77it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.29it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.60it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.95it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.51it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.27it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.27it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.15it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.37it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.82it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.28it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.23it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.35it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.30it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.59it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.49it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.80it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.10it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.31it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.88it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.08it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.77it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.78it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.93it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.23it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.70it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.59it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.22it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.44it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.39it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4246845245361328, 'eval_accuracy': 0.7443734367065683, 'eval_runtime': 15.6098, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 6.086, 'epoch': 0.84, 'num_input_tokens_seen': 710976}\r\n",
      " 29%|████████████                              | 30/105 [08:52<19:53, 15.92s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.60it/s]\u001b[A\r\n",
      "{'loss': 1.7027, 'grad_norm': 0.7979576587677002, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.98, 'num_input_tokens_seen': 833472}\r\n",
      " 33%|██████████████                            | 35/105 [10:08<19:34, 16.78s/it][INFO|trainer.py:3719] 2024-07-11 05:17:51,051 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:17:51,051 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:17:51,051 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.92it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.85it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.59it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.80it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.01it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.81it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.30it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.20it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.05it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.48it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.07it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.73it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.26it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.61it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.24it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.76it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.92it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.74it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.41it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.59it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.13it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.33it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.47it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.74it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.63it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.20it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:09,  5.55it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.80it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.93it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.49it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.25it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.25it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.12it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.35it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.84it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.28it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.21it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.34it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.29it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.58it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.49it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.81it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.12it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.32it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.32it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.89it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.10it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.76it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.75it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.89it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.21it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.69it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.54it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.61it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.46it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4192569255828857, 'eval_accuracy': 0.7452848282724296, 'eval_runtime': 15.6364, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 6.076, 'epoch': 0.98, 'num_input_tokens_seen': 833472}\r\n",
      " 33%|██████████████                            | 35/105 [10:24<19:34, 16.78s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.5484, 'grad_norm': 0.7604296803474426, 'learning_rate': 3.413352560915988e-05, 'epoch': 1.12, 'num_input_tokens_seen': 955104}\r\n",
      " 38%|████████████████                          | 40/105 [11:39<17:25, 16.08s/it][INFO|trainer.py:3719] 2024-07-11 05:19:21,539 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:19:21,539 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:19:21,539 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.00it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.97it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.89it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.61it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.81it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.00it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.82it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.23it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.07it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.09it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.49it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.05it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.73it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.26it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.64it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.26it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.88it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.00it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.80it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.46it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.60it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.12it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.32it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.46it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.95it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.78it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.30it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.59it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.84it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:06,  7.72it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.34it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.17it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:06,  7.16it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.08it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.31it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.82it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.28it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.25it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.36it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.52it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.83it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.13it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.26it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.84it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.05it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.93it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.73it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.73it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.89it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.21it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.69it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.55it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.22it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.44it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4168158769607544, 'eval_accuracy': 0.7446275228302661, 'eval_runtime': 15.6247, 'eval_samples_per_second': 12.16, 'eval_steps_per_second': 6.08, 'epoch': 1.12, 'num_input_tokens_seen': 955104}\r\n",
      " 38%|████████████████                          | 40/105 [11:54<17:25, 16.08s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7126, 'grad_norm': 0.843051552772522, 'learning_rate': 3.056302334890786e-05, 'epoch': 1.26, 'num_input_tokens_seen': 1079040}\r\n",
      " 43%|██████████████████                        | 45/105 [13:11<16:39, 16.67s/it][INFO|trainer.py:3719] 2024-07-11 05:20:53,493 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:20:53,494 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:20:53,494 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.98it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.92it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.84it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.58it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.78it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.99it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.80it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.30it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.20it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.05it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.07it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.48it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.09it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.75it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.28it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.37it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:10,  6.55it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.20it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.83it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.97it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.76it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.43it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.98it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.31it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.87it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.98it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.53it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.29it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.28it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.14it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.38it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.73it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.22it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.20it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.33it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.28it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.58it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.51it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.81it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.12it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.33it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.32it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.89it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.81it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.80it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.93it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.23it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.70it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.59it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.22it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.42it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.37it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4164013862609863, 'eval_accuracy': 0.7432751389936757, 'eval_runtime': 15.6211, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 6.082, 'epoch': 1.26, 'num_input_tokens_seen': 1079040}\r\n",
      " 43%|██████████████████                        | 45/105 [13:26<16:39, 16.67s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.6277, 'grad_norm': 0.8810274004936218, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.4, 'num_input_tokens_seen': 1195200}\r\n",
      " 48%|████████████████████                      | 50/105 [14:38<14:47, 16.14s/it][INFO|trainer.py:3719] 2024-07-11 05:22:21,095 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:22:21,096 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:22:21,096 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.94it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.84it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.57it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.73it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.96it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.78it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.28it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.19it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.05it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.04it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.45it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.08it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.57it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.41it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.74it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.28it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.37it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.63it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.26it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.89it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.00it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.80it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.46it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.87it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.85it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.71it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.25it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.58it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.84it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.42it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.24it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.25it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.13it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.36it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.85it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.30it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.26it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.37it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.54it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.84it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.14it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.33it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.90it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.10it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.81it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.82it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.96it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.25it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.38it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.43it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.36it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.414202094078064, 'eval_accuracy': 0.7457241567112795, 'eval_runtime': 15.6123, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 6.085, 'epoch': 1.4, 'num_input_tokens_seen': 1195200}\r\n",
      " 48%|████████████████████                      | 50/105 [14:54<14:47, 16.14s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.5603, 'grad_norm': 0.8238646984100342, 'learning_rate': 2.3131747660339394e-05, 'epoch': 1.54, 'num_input_tokens_seen': 1305696}\r\n",
      " 52%|██████████████████████                    | 55/105 [16:01<12:35, 15.11s/it][INFO|trainer.py:3719] 2024-07-11 05:23:44,175 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:23:44,175 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:23:44,175 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.97it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.95it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.85it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.55it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.72it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.96it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.79it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.29it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.18it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.04it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.48it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.73it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.60it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.24it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.84it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.97it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.76it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.44it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.94it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.78it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.30it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.85it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.93it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.50it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.28it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.29it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.15it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.38it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.86it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.29it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.25it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.37it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.55it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.85it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.15it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.28it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.28it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.86it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.78it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.94it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.24it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.45it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.22it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.413185954093933, 'eval_accuracy': 0.7442928405966953, 'eval_runtime': 15.6074, 'eval_samples_per_second': 12.174, 'eval_steps_per_second': 6.087, 'epoch': 1.54, 'num_input_tokens_seen': 1305696}\r\n",
      " 52%|██████████████████████                    | 55/105 [16:17<12:35, 15.11s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.63it/s]\u001b[A\r\n",
      "{'loss': 1.6009, 'grad_norm': 0.8989720940589905, 'learning_rate': 1.9436976651092144e-05, 'epoch': 1.68, 'num_input_tokens_seen': 1423152}\r\n",
      " 57%|████████████████████████                  | 60/105 [17:29<11:47, 15.71s/it][INFO|trainer.py:3719] 2024-07-11 05:25:11,238 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:25:11,239 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:25:11,239 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.97it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.95it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.73it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.36it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.61it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.90it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.73it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.26it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.15it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.03it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.50it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.10it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.58it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.07it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:04<00:12,  5.73it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.62it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.25it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.87it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.99it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.85it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.90it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.75it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.28it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.58it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.83it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.87it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.45it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.24it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.25it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.14it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.37it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.84it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.29it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.25it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.36it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.59it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.53it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.83it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.14it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.29it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.87it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.94it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.77it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.77it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.90it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.22it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.70it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.55it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.61it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.39it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4130038022994995, 'eval_accuracy': 0.744353350760375, 'eval_runtime': 15.6321, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 6.077, 'epoch': 1.68, 'num_input_tokens_seen': 1423152}\r\n",
      " 57%|████████████████████████                  | 60/105 [17:44<11:47, 15.71s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.61it/s]\u001b[A\r\n",
      "{'loss': 1.6307, 'grad_norm': 0.855922520160675, 'learning_rate': 1.5866474390840125e-05, 'epoch': 1.82, 'num_input_tokens_seen': 1548720}\r\n",
      " 62%|██████████████████████████                | 65/105 [19:02<11:12, 16.80s/it][INFO|trainer.py:3719] 2024-07-11 05:26:44,421 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:26:44,421 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:26:44,421 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.00it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 4/95 [00:00<00:10,  9.08it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.54it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.34it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.63it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.89it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.73it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.25it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.16it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.01it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.01it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.43it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.06it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.55it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.39it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:12,  5.89it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:04<00:12,  5.61it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.19it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.29it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:10,  6.58it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.22it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.84it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.97it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.77it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.44it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.95it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.78it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.29it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.60it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.96it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.51it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.27it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.27it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.14it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.37it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.85it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.29it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.24it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.36it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.30it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.52it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.82it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.13it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.31it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.89it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.80it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.80it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.94it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.24it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.21it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.38it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.39it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4130059480667114, 'eval_accuracy': 0.7434704865823637, 'eval_runtime': 15.6335, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 6.077, 'epoch': 1.82, 'num_input_tokens_seen': 1548720}\r\n",
      " 62%|██████████████████████████                | 65/105 [19:17<11:12, 16.80s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7572, 'grad_norm': 0.8496147990226746, 'learning_rate': 1.2500000000000006e-05, 'epoch': 1.96, 'num_input_tokens_seen': 1667280}\r\n",
      " 67%|████████████████████████████              | 70/105 [20:31<09:22, 16.08s/it][INFO|trainer.py:3719] 2024-07-11 05:28:13,623 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:28:13,623 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:28:13,623 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.00it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 4/95 [00:00<00:09,  9.20it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.62it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.47it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.75it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.94it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.77it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.27it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.22it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.06it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.09it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.50it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.11it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.60it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.43it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.09it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.75it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.28it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.35it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.60it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.23it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.74it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.91it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.71it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.40it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.59it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.13it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.33it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.47it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.87it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.99it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.32it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.87it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.98it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.52it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.29it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.29it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.15it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.40it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.90it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.31it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.39it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.32it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.51it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.83it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.14it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.28it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.35it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.32it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.34it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.91it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.12it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.96it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.78it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.93it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.24it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.61it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.38it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.44it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.39it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4130021333694458, 'eval_accuracy': 0.7426522700509783, 'eval_runtime': 15.5845, 'eval_samples_per_second': 12.192, 'eval_steps_per_second': 6.096, 'epoch': 1.96, 'num_input_tokens_seen': 1667280}\r\n",
      " 67%|████████████████████████████              | 70/105 [20:47<09:22, 16.08s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.60it/s]\u001b[A\r\n",
      "{'loss': 1.466, 'grad_norm': 0.8537042140960693, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.11, 'num_input_tokens_seen': 1773072}\r\n",
      " 71%|██████████████████████████████            | 75/105 [21:51<07:23, 14.80s/it][INFO|trainer.py:3719] 2024-07-11 05:29:33,724 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:29:33,724 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:29:33,724 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.77it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.85it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.82it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.59it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.82it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.01it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.82it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.25it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.08it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.10it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.50it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.12it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.59it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.43it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.08it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.74it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.29it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.37it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.62it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.25it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.85it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.98it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.46it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.49it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  7.00it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.33it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.61it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.87it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  8.00it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.54it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.31it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.29it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.14it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.39it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.88it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.30it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.39it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.33it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.61it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.53it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.83it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.13it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.34it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.31it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.30it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.89it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.10it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.78it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.81it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.96it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.26it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.72it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.57it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.63it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.25it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.39it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.46it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4125046730041504, 'eval_accuracy': 0.7424275653458777, 'eval_runtime': 15.5669, 'eval_samples_per_second': 12.205, 'eval_steps_per_second': 6.103, 'epoch': 2.11, 'num_input_tokens_seen': 1773072}\r\n",
      " 71%|██████████████████████████████            | 75/105 [22:07<07:23, 14.80s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.6516, 'grad_norm': 0.7621994614601135, 'learning_rate': 6.673703204254347e-06, 'epoch': 2.25, 'num_input_tokens_seen': 1890384}\r\n",
      " 76%|████████████████████████████████          | 80/105 [23:19<06:26, 15.44s/it][INFO|trainer.py:3719] 2024-07-11 05:31:01,653 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:31:01,654 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:31:01,654 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.97it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.88it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.58it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.79it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.99it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.81it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.21it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.06it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.11it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.53it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.13it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.61it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.45it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.11it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.77it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.30it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.37it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.65it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.27it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.90it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.01it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.46it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.63it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.36it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.49it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.89it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  7.01it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.34it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.88it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.97it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.52it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.31it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.30it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.15it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.40it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.88it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.31it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.39it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.32it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.61it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.58it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.87it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.15it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:10<00:05,  5.26it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.35it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.32it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.31it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.90it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.10it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.96it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.81it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.82it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.94it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.25it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.72it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.57it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.57it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.21it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.37it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.411655306816101, 'eval_accuracy': 0.7420642337612926, 'eval_runtime': 15.5537, 'eval_samples_per_second': 12.216, 'eval_steps_per_second': 6.108, 'epoch': 2.25, 'num_input_tokens_seen': 1890384}\r\n",
      " 76%|████████████████████████████████          | 80/105 [23:35<06:26, 15.44s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.6117, 'grad_norm': 0.7760735750198364, 'learning_rate': 4.344030642100133e-06, 'epoch': 2.39, 'num_input_tokens_seen': 2008080}\r\n",
      " 81%|██████████████████████████████████        | 85/105 [24:46<05:08, 15.42s/it][INFO|trainer.py:3719] 2024-07-11 05:32:29,058 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:32:29,058 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:32:29,058 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.92it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.87it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.61it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.79it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.99it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.80it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.30it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.19it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.05it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.46it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.08it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.57it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.40it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.07it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.74it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.27it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.36it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.61it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.23it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.84it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.96it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.75it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.43it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.60it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.13it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.87it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.93it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.76it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.28it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.60it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.84it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  7.92it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.47it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.22it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.22it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.10it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.35it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.61it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.18it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.15it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.29it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:10<00:05,  6.24it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.56it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.47it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.80it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.11it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.31it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.28it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.27it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.84it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.05it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.93it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.68it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.70it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.83it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.17it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.65it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.53it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:14<00:01,  5.45it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.56it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.20it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.38it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.44it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.39it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4113117456436157, 'eval_accuracy': 0.7418905782984029, 'eval_runtime': 15.6648, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 6.065, 'epoch': 2.39, 'num_input_tokens_seen': 2008080}\r\n",
      " 81%|██████████████████████████████████        | 85/105 [25:02<05:08, 15.42s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.61it/s]\u001b[A\r\n",
      "{'loss': 1.7185, 'grad_norm': 0.7726441621780396, 'learning_rate': 2.475778302439524e-06, 'epoch': 2.53, 'num_input_tokens_seen': 2124240}\r\n",
      " 86%|████████████████████████████████████      | 90/105 [26:13<03:52, 15.48s/it][INFO|trainer.py:3719] 2024-07-11 05:33:55,731 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:33:55,731 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:33:55,731 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.96it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.84it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.55it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.76it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.98it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.80it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.29it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.19it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.04it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.03it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.43it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.05it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.55it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.39it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.01it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:04<00:12,  5.68it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.24it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.33it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:10,  6.55it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.20it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.84it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  5.95it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.74it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.42it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:06<00:12,  4.60it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.13it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.47it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.86it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:07<00:07,  6.96it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.31it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.60it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  8.01it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.54it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.33it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.32it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.17it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.41it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.90it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.31it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.38it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.31it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.60it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.49it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.79it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.11it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.32it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.28it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.30it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.87it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.07it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.93it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.76it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.95it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.25it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.72it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.57it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.61it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.44it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.411076545715332, 'eval_accuracy': 0.742929339731901, 'eval_runtime': 15.6175, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 6.083, 'epoch': 2.53, 'num_input_tokens_seen': 2124240}\r\n",
      " 86%|████████████████████████████████████      | 90/105 [26:29<03:52, 15.48s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.62it/s]\u001b[A\r\n",
      "{'loss': 1.7108, 'grad_norm': 0.8203617334365845, 'learning_rate': 1.1106798553464804e-06, 'epoch': 2.67, 'num_input_tokens_seen': 2248464}\r\n",
      " 90%|██████████████████████████████████████    | 95/105 [27:46<02:43, 16.39s/it][INFO|trainer.py:3719] 2024-07-11 05:35:29,053 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:35:29,053 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:35:29,053 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.98it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.96it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.89it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.63it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.79it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.00it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.82it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.20it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.05it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.09it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.50it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.12it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.60it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.42it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.10it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.76it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.29it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.37it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.65it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.27it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.90it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.01it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.79it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:11,  5.45it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.61it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:14,  4.14it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.34it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.48it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.87it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  6.98it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.31it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.62it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  8.00it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.54it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.31it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.31it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.17it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.40it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.86it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.30it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.39it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.33it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.61it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.54it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.84it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.14it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.35it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.32it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.33it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.90it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.11it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.96it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.82it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.93it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.24it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.71it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.62it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.24it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.40it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4110329151153564, 'eval_accuracy': 0.743333256500567, 'eval_runtime': 15.5629, 'eval_samples_per_second': 12.209, 'eval_steps_per_second': 6.104, 'epoch': 2.67, 'num_input_tokens_seen': 2248464}\r\n",
      " 90%|██████████████████████████████████████    | 95/105 [28:02<02:43, 16.39s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.61it/s]\u001b[A\r\n",
      "{'loss': 1.6457, 'grad_norm': 0.8092706203460693, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.81, 'num_input_tokens_seen': 2368848}\r\n",
      " 95%|███████████████████████████████████████  | 100/105 [29:18<01:22, 16.51s/it][INFO|trainer.py:3719] 2024-07-11 05:37:00,313 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:37:00,313 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:37:00,313 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09, 10.01it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 4/95 [00:00<00:09,  9.21it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.62it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.46it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:11,  7.67it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  5.91it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.74it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.26it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.15it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.02it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.08it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.51it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.12it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.60it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.43it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.10it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.76it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.30it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.39it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.67it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.28it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.89it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.01it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.80it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.46it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.62it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.49it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.89it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  7.01it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.83it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.33it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.60it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.86it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:06,  7.79it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.40it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.21it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.20it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.11it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.34it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.83it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.28it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.26it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.38it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.32it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.61it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.60it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.87it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.16it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.29it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.35it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.32it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.30it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.29it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.88it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.78it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.79it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.91it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.22it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.70it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.56it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.47it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.60it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.23it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.62it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.41it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.45it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.40it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4109935760498047, 'eval_accuracy': 0.7430045306455253, 'eval_runtime': 15.5802, 'eval_samples_per_second': 12.195, 'eval_steps_per_second': 6.097, 'epoch': 2.81, 'num_input_tokens_seen': 2368848}\r\n",
      " 95%|███████████████████████████████████████  | 100/105 [29:33<01:22, 16.51s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.61it/s]\u001b[A\r\n",
      "{'loss': 1.6157, 'grad_norm': 0.732416570186615, 'learning_rate': 0.0, 'epoch': 2.95, 'num_input_tokens_seen': 2487648}\r\n",
      "100%|█████████████████████████████████████████| 105/105 [30:47<00:00, 16.42s/it][INFO|trainer.py:3719] 2024-07-11 05:38:29,895 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:38:29,895 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:38:29,895 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/95 [00:00<?, ?it/s]\u001b[A\r\n",
      "  2%|▉                                           | 2/95 [00:00<00:09,  9.99it/s]\u001b[A\r\n",
      "  3%|█▍                                          | 3/95 [00:00<00:11,  7.96it/s]\u001b[A\r\n",
      "  5%|██▎                                         | 5/95 [00:00<00:11,  7.87it/s]\u001b[A\r\n",
      "  7%|███▏                                        | 7/95 [00:00<00:11,  7.62it/s]\u001b[A\r\n",
      "  9%|████▏                                       | 9/95 [00:01<00:10,  7.83it/s]\u001b[A\r\n",
      " 11%|████▌                                      | 10/95 [00:01<00:14,  6.01it/s]\u001b[A\r\n",
      " 12%|████▉                                      | 11/95 [00:01<00:14,  5.82it/s]\u001b[A\r\n",
      " 13%|█████▍                                     | 12/95 [00:01<00:15,  5.31it/s]\u001b[A\r\n",
      " 15%|██████▎                                    | 14/95 [00:02<00:11,  7.24it/s]\u001b[A\r\n",
      " 16%|██████▊                                    | 15/95 [00:02<00:13,  6.08it/s]\u001b[A\r\n",
      " 18%|███████▋                                   | 17/95 [00:02<00:12,  6.09it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 19/95 [00:02<00:11,  6.51it/s]\u001b[A\r\n",
      " 21%|█████████                                  | 20/95 [00:03<00:12,  6.12it/s]\u001b[A\r\n",
      " 22%|█████████▌                                 | 21/95 [00:03<00:13,  5.60it/s]\u001b[A\r\n",
      " 23%|█████████▉                                 | 22/95 [00:03<00:13,  5.44it/s]\u001b[A\r\n",
      " 25%|██████████▊                                | 24/95 [00:03<00:11,  6.09it/s]\u001b[A\r\n",
      " 26%|███████████▎                               | 25/95 [00:03<00:12,  5.75it/s]\u001b[A\r\n",
      " 27%|███████████▊                               | 26/95 [00:04<00:13,  5.29it/s]\u001b[A\r\n",
      " 28%|████████████▏                              | 27/95 [00:04<00:12,  5.38it/s]\u001b[A\r\n",
      " 31%|█████████████▏                             | 29/95 [00:04<00:09,  6.64it/s]\u001b[A\r\n",
      " 32%|█████████████▌                             | 30/95 [00:04<00:10,  6.27it/s]\u001b[A\r\n",
      " 34%|██████████████▍                            | 32/95 [00:05<00:09,  6.90it/s]\u001b[A\r\n",
      " 35%|██████████████▉                            | 33/95 [00:05<00:10,  6.01it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 34/95 [00:05<00:10,  5.81it/s]\u001b[A\r\n",
      " 37%|███████████████▊                           | 35/95 [00:05<00:10,  5.47it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 36/95 [00:05<00:12,  4.63it/s]\u001b[A\r\n",
      " 39%|████████████████▋                          | 37/95 [00:06<00:13,  4.15it/s]\u001b[A\r\n",
      " 40%|█████████████████▏                         | 38/95 [00:06<00:13,  4.35it/s]\u001b[A\r\n",
      " 41%|█████████████████▋                         | 39/95 [00:06<00:12,  4.49it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 40/95 [00:06<00:11,  4.88it/s]\u001b[A\r\n",
      " 44%|███████████████████                        | 42/95 [00:06<00:07,  7.02it/s]\u001b[A\r\n",
      " 45%|███████████████████▍                       | 43/95 [00:07<00:07,  6.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▉                       | 44/95 [00:07<00:08,  6.34it/s]\u001b[A\r\n",
      " 47%|████████████████████▎                      | 45/95 [00:07<00:08,  5.63it/s]\u001b[A\r\n",
      " 48%|████████████████████▊                      | 46/95 [00:07<00:08,  5.89it/s]\u001b[A\r\n",
      " 51%|█████████████████████▋                     | 48/95 [00:07<00:05,  8.01it/s]\u001b[A\r\n",
      " 52%|██████████████████████▏                    | 49/95 [00:08<00:06,  7.55it/s]\u001b[A\r\n",
      " 54%|███████████████████████                    | 51/95 [00:08<00:06,  7.15it/s]\u001b[A\r\n",
      " 55%|███████████████████████▌                   | 52/95 [00:08<00:05,  7.17it/s]\u001b[A\r\n",
      " 56%|███████████████████████▉                   | 53/95 [00:08<00:06,  6.08it/s]\u001b[A\r\n",
      " 57%|████████████████████████▍                  | 54/95 [00:08<00:06,  6.34it/s]\u001b[A\r\n",
      " 59%|█████████████████████████▎                 | 56/95 [00:09<00:05,  6.85it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 57/95 [00:09<00:07,  5.28it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 59/95 [00:09<00:05,  6.27it/s]\u001b[A\r\n",
      " 63%|███████████████████████████▏               | 60/95 [00:09<00:05,  6.39it/s]\u001b[A\r\n",
      " 64%|███████████████████████████▌               | 61/95 [00:09<00:05,  6.34it/s]\u001b[A\r\n",
      " 65%|████████████████████████████               | 62/95 [00:10<00:05,  5.62it/s]\u001b[A\r\n",
      " 67%|████████████████████████████▉              | 64/95 [00:10<00:04,  7.57it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▍             | 65/95 [00:10<00:04,  6.86it/s]\u001b[A\r\n",
      " 69%|█████████████████████████████▊             | 66/95 [00:10<00:04,  6.15it/s]\u001b[A\r\n",
      " 71%|██████████████████████████████▎            | 67/95 [00:11<00:05,  5.27it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▊            | 68/95 [00:11<00:05,  5.35it/s]\u001b[A\r\n",
      " 73%|███████████████████████████████▏           | 69/95 [00:11<00:04,  5.29it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▋           | 70/95 [00:11<00:04,  5.28it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▌          | 72/95 [00:11<00:03,  6.29it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▍         | 74/95 [00:12<00:03,  6.88it/s]\u001b[A\r\n",
      " 79%|█████████████████████████████████▉         | 75/95 [00:12<00:02,  7.09it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 76/95 [00:12<00:03,  5.95it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 78/95 [00:12<00:02,  7.81it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████▏      | 80/95 [00:12<00:01,  7.82it/s]\u001b[A\r\n",
      " 86%|█████████████████████████████████████      | 82/95 [00:13<00:01,  7.97it/s]\u001b[A\r\n",
      " 87%|█████████████████████████████████████▌     | 83/95 [00:13<00:01,  7.27it/s]\u001b[A\r\n",
      " 88%|██████████████████████████████████████     | 84/95 [00:13<00:01,  6.73it/s]\u001b[A\r\n",
      " 89%|██████████████████████████████████████▍    | 85/95 [00:13<00:01,  5.57it/s]\u001b[A\r\n",
      " 91%|██████████████████████████████████████▉    | 86/95 [00:13<00:01,  5.48it/s]\u001b[A\r\n",
      " 93%|███████████████████████████████████████▊   | 88/95 [00:14<00:01,  6.62it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▎  | 89/95 [00:14<00:00,  6.24it/s]\u001b[A\r\n",
      " 95%|████████████████████████████████████████▋  | 90/95 [00:14<00:00,  5.63it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▏ | 91/95 [00:14<00:00,  5.41it/s]\u001b[A\r\n",
      " 98%|██████████████████████████████████████████ | 93/95 [00:15<00:00,  5.46it/s]\u001b[A\r\n",
      " 99%|██████████████████████████████████████████▌| 94/95 [00:15<00:00,  5.41it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4110181331634521, 'eval_accuracy': 0.7432357906136273, 'eval_runtime': 15.5632, 'eval_samples_per_second': 12.208, 'eval_steps_per_second': 6.104, 'epoch': 2.95, 'num_input_tokens_seen': 2487648}\r\n",
      "100%|█████████████████████████████████████████| 105/105 [31:03<00:00, 16.42s/it]\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  5.64it/s]\u001b[A\r\n",
      "                                                                                \u001b[A[INFO|trainer.py:2329] 2024-07-11 05:38:45,459 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 1863.2737, 'train_samples_per_second': 2.753, 'train_steps_per_second': 0.056, 'train_loss': 1.6765017600286574, 'epoch': 2.95, 'num_input_tokens_seen': 2487648}\r\n",
      "100%|█████████████████████████████████████████| 105/105 [31:03<00:00, 17.75s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-11 05:38:45,460 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 05:38:45,725 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 05:38:45,726 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-11 05:38:45,777 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-11 05:38:45,777 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =     2.9474\r\n",
      "  num_input_tokens_seen    =    2487648\r\n",
      "  total_flos               =  5036227GF\r\n",
      "  train_loss               =     1.6765\r\n",
      "  train_runtime            = 0:31:03.27\r\n",
      "  train_samples_per_second =      2.753\r\n",
      "  train_steps_per_second   =      0.056\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 05:38:46,464 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:38:46,465 >>   Num examples = 190\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:38:46,465 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 95/95 [00:15<00:00,  6.14it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =     2.9474\r\n",
      "  eval_accuracy           =     0.7432\r\n",
      "  eval_loss               =      1.411\r\n",
      "  eval_runtime            = 0:00:15.55\r\n",
      "  eval_samples_per_second =     12.212\r\n",
      "  eval_steps_per_second   =      6.106\r\n",
      "  num_input_tokens_seen   =    2487648\r\n",
      "[INFO|modelcard.py:450] 2024-07-11 05:39:02,025 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7432357906136273}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 1000 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 5 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c03bc11a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T05:39:04.616087Z",
     "iopub.status.busy": "2024-07-11T05:39:04.615223Z",
     "iopub.status.idle": "2024-07-11T05:41:27.909802Z",
     "shell.execute_reply": "2024-07-11T05:41:27.908406Z"
    },
    "papermill": {
     "duration": 143.537477,
     "end_time": "2024-07-11T05:41:27.912508",
     "exception": false,
     "start_time": "2024-07-11T05:39:04.375031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-11 05:39:09.551734: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-11 05:39:09.551804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-11 05:39:09.553597: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/11/2024 05:39:18 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,377 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,377 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,377 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,377 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,378 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-11 05:39:18,378 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-11 05:39:18,662 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/11/2024 05:39:18 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/11/2024 05:39:18 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "07/11/2024 05:39:18 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-11 05:39:19,533 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-11 05:39:19,535 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 05:39:19 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-11 05:39:19,570 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-11 05:39:19,581 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:39:19,583 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-11 05:39:21,473 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-11 05:39:21,473 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-11 05:39:21,600 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-11 05:39:21,600 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/11/2024 05:39:21 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/11/2024 05:39:22 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/11/2024 05:39:22 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "07/11/2024 05:39:22 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-11 05:39:22,224 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-11 05:39:22,224 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-11 05:39:22,224 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:01<00:00,  2.06s/it]Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.863 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [02:02<00:00,  1.23s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    30.2181\r\n",
      "  predict_rouge-1            =    37.5688\r\n",
      "  predict_rouge-2            =    16.1762\r\n",
      "  predict_rouge-l            =    32.7837\r\n",
      "  predict_runtime            = 0:02:03.11\r\n",
      "  predict_samples_per_second =      1.624\r\n",
      "  predict_steps_per_second   =      0.812\r\n",
      "07/11/2024 05:41:25 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-36/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-36 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5295003,
     "sourceId": 8869298,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351712,
     "sourceId": 8902036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351730,
     "sourceId": 8902062,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3847.277504,
   "end_time": "2024-07-11T05:41:28.408236",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-11T04:37:21.130732",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
