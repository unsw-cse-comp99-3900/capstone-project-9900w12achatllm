{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbb06d3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-13T05:29:49.238099Z",
     "iopub.status.busy": "2024-07-13T05:29:49.237765Z",
     "iopub.status.idle": "2024-07-13T05:30:58.130841Z",
     "shell.execute_reply": "2024-07-13T05:30:58.129641Z"
    },
    "papermill": {
     "duration": 68.901415,
     "end_time": "2024-07-13T05:30:58.133393",
     "exception": false,
     "start_time": "2024-07-13T05:29:49.231978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\r\n",
      "remote: Enumerating objects: 15353, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (303/303), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (135/135), done.\u001b[K\r\n",
      "remote: Total 15353 (delta 191), reused 261 (delta 167), pack-reused 15050\u001b[K\r\n",
      "Receiving objects: 100% (15353/15353), 221.72 MiB | 28.20 MiB/s, done.\r\n",
      "Resolving deltas: 100% (11240/11240), done.\r\n",
      "Obtaining file:///kaggle/working/LLaMA-Factory\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (4.41.2)\r\n",
      "Requirement already satisfied: datasets>=2.16.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.19.2)\r\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.30.1)\r\n",
      "Collecting peft>=0.11.1 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Collecting trl>=0.8.6 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting gradio>=4.0.0 (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio-4.38.1-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.2.1)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.11.4)\r\n",
      "Collecting einops (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.2.0)\r\n",
      "Collecting tiktoken (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.20.3)\r\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.25.0)\r\n",
      "Requirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.5.3)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.108.0)\r\n",
      "Collecting sse-starlette (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading sse_starlette-2.1.2-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.7.5)\r\n",
      "Collecting fire (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading fire-0.6.0.tar.gz (88 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: torch>=1.13.1 in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (2.1.2)\r\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (3.2.4)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/lib/python3.10/site-packages (from llamafactory==0.8.3.dev0) (0.42.1)\r\n",
      "Collecting rouge-chinese (from llamafactory==0.8.3.dev0)\r\n",
      "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (5.9.3)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.23.2)\r\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.30.1->llamafactory==0.8.3.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.3.8)\r\n",
      "Requirement already satisfied: requests>=2.32.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets>=2.16.0->llamafactory==0.8.3.dev0) (2024.3.1)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.9.1)\r\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (22.1.0)\r\n",
      "Requirement already satisfied: altair<6.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (5.3.0)\r\n",
      "Collecting ffmpy (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting gradio-client==1.1.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading gradio_client-1.1.0-py3-none-any.whl.metadata (7.1 kB)\r\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.27.0)\r\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (6.1.1)\r\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.1.2)\r\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.1.3)\r\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.9.10)\r\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (9.5.0)\r\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.25.1)\r\n",
      "Collecting python-multipart>=0.0.9 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting ruff>=0.2.2 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\r\n",
      "Collecting semantic-version~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Collecting tomlkit==0.12.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\r\n",
      "Collecting typer<1.0,>=0.12 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\r\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.9.0)\r\n",
      "Collecting urllib3~=2.0 (from gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\r\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==1.1.0->gradio>=4.0.0->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (4.47.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (1.4.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->llamafactory==0.8.3.dev0) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->llamafactory==0.8.3.dev0) (2023.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic->llamafactory==0.8.3.dev0) (2.14.6)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (1.12.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.1->llamafactory==0.8.3.dev0) (3.2.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.41.2->llamafactory==0.8.3.dev0) (0.19.1)\r\n",
      "Collecting tyro>=0.5.11 (from trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (8.1.7)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn->llamafactory==0.8.3.dev0) (0.14.0)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->llamafactory==0.8.3.dev0) (0.32.0.post1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->llamafactory==0.8.3.dev0) (2.4.0)\r\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from sse-starlette->llamafactory==0.8.3.dev0) (4.2.0)\r\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (4.20.0)\r\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.12.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0->llamafactory==0.8.3.dev0) (4.0.3)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.0.5)\r\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.6)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.1->datasets>=2.16.0->llamafactory==0.8.3.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->sse-starlette->llamafactory==0.8.3.dev0) (1.2.0)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (1.5.4)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (13.7.0)\r\n",
      "Collecting docstring-parser>=0.16 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.6->llamafactory==0.8.3.dev0)\r\n",
      "  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.1->llamafactory==0.8.3.dev0) (1.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.32.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=5.0->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.16.2)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->llamafactory==0.8.3.dev0) (0.1.2)\r\n",
      "Downloading gradio-4.38.1-py3-none-any.whl (12.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gradio_client-1.1.0-py3-none-any.whl (318 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading peft-0.11.1-py3-none-any.whl (251 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\r\n",
      "Downloading sse_starlette-2.1.2-py3-none-any.whl (9.3 kB)\r\n",
      "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\r\n",
      "Downloading ruff-0.5.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\r\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading urllib3-2.2.2-py3-none-any.whl (121 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.4/121.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\r\n",
      "Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\r\n",
      "Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hChecking if build backend supports build_editable ... \u001b[?25ldone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, ffmpy\r\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.3.dev0-0.editable-py3-none-any.whl size=20627 sha256=005b172dc65a081c6c64c4c3ef2a9033ae466f8d654241740f3e866236d6eac9\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-o1blm1c1/wheels/21/5a/a2/9a8fea19e68e32089e22401d08554f51119f2464cad3a126ec\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=277bbb227779406d7936f572099cc19cd4285101432b1db45fc5323a2fb94b97\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\r\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=0be3f4a6e38b5291ff3be9173c4dc9d1c566b4176f6a6e7c15ae1a79e208af3a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\r\n",
      "Successfully built llamafactory fire ffmpy\r\n",
      "Installing collected packages: ffmpy, websockets, urllib3, tomlkit, shtab, semantic-version, ruff, rouge-chinese, python-multipart, fire, einops, docstring-parser, tyro, typer, tiktoken, sse-starlette, gradio-client, gradio, trl, peft, llamafactory\r\n",
      "  Attempting uninstall: websockets\r\n",
      "    Found existing installation: websockets 12.0\r\n",
      "    Uninstalling websockets-12.0:\r\n",
      "      Successfully uninstalled websockets-12.0\r\n",
      "  Attempting uninstall: urllib3\r\n",
      "    Found existing installation: urllib3 1.26.18\r\n",
      "    Uninstalling urllib3-1.26.18:\r\n",
      "      Successfully uninstalled urllib3-1.26.18\r\n",
      "  Attempting uninstall: tomlkit\r\n",
      "    Found existing installation: tomlkit 0.12.5\r\n",
      "    Uninstalling tomlkit-0.12.5:\r\n",
      "      Successfully uninstalled tomlkit-0.12.5\r\n",
      "  Attempting uninstall: docstring-parser\r\n",
      "    Found existing installation: docstring-parser 0.15\r\n",
      "    Uninstalling docstring-parser-0.15:\r\n",
      "      Successfully uninstalled docstring-parser-0.15\r\n",
      "  Attempting uninstall: typer\r\n",
      "    Found existing installation: typer 0.9.0\r\n",
      "    Uninstalling typer-0.9.0:\r\n",
      "      Successfully uninstalled typer-0.9.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "distributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\r\n",
      "rapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\r\n",
      "spacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\r\n",
      "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed docstring-parser-0.16 einops-0.8.0 ffmpy-0.3.2 fire-0.6.0 gradio-4.38.1 gradio-client-1.1.0 llamafactory-0.8.3.dev0 peft-0.11.1 python-multipart-0.0.9 rouge-chinese-1.0.3 ruff-0.5.1 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.1.2 tiktoken-0.7.0 tomlkit-0.12.0 trl-0.9.6 typer-0.12.3 tyro-0.8.5 urllib3-2.1.0 websockets-11.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory;pip install -e \".[torch,metrics]\"\n",
    "!pip install bitsandbytes>=0.39.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38df4d69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:30:58.164260Z",
     "iopub.status.busy": "2024-07-13T05:30:58.163416Z",
     "iopub.status.idle": "2024-07-13T05:30:58.172219Z",
     "shell.execute_reply": "2024-07-13T05:30:58.171380Z"
    },
    "papermill": {
     "duration": 0.026014,
     "end_time": "2024-07-13T05:30:58.174228",
     "exception": false,
     "start_time": "2024-07-13T05:30:58.148214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = '/kaggle/working/LLaMA-Factory/data/dataset_info.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# configure the MedQA and PubMedQA dataset information \n",
    "new_data = {\n",
    "    \"MedQA_train\": {\"file_name\": \"MedQA/train.json\"},\n",
    "    \"MedQA_test\": {\"file_name\": \"MedQA/test.json\"},\n",
    "    \"PubMedQA_pqal_train\": {\"file_name\": \"PubMedQA/pqal_train_set.json\"},\n",
    "    \"PubMedQA_pqal_test\": {\"file_name\": \"PubMedQA/pqal_test_set.json\"}\n",
    "}\n",
    "\n",
    "# update information in dataset_info.json file\n",
    "data.update(new_data)\n",
    "with open(file_path, 'w') as file:\n",
    "    json.dump(data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3062f7ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:30:58.207459Z",
     "iopub.status.busy": "2024-07-13T05:30:58.207179Z",
     "iopub.status.idle": "2024-07-13T05:30:59.213574Z",
     "shell.execute_reply": "2024-07-13T05:30:59.212512Z"
    },
    "papermill": {
     "duration": 1.026291,
     "end_time": "2024-07-13T05:30:59.215752",
     "exception": false,
     "start_time": "2024-07-13T05:30:58.189461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medqa-pubmedqa\r\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3798af0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:30:59.247193Z",
     "iopub.status.busy": "2024-07-13T05:30:59.246586Z",
     "iopub.status.idle": "2024-07-13T05:31:05.250642Z",
     "shell.execute_reply": "2024-07-13T05:31:05.249581Z"
    },
    "papermill": {
     "duration": 6.022931,
     "end_time": "2024-07-13T05:31:05.253270",
     "exception": false,
     "start_time": "2024-07-13T05:30:59.230339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create MedQA and PubMedQA directories\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/MedQA\n",
    "!mkdir -p /kaggle/working/LLaMA-Factory/data/PubMedQA\n",
    "\n",
    "# copy MedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/train.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/MedQA/test.json /kaggle/working/LLaMA-Factory/data/MedQA/\n",
    "\n",
    "# copy PubMedQA dataset file\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_train_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/\n",
    "!cp /kaggle/input/medqa-pubmedqa/PubMedQA/pqal_test_set.json /kaggle/working/LLaMA-Factory/data/PubMedQA/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb6143f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:31:05.283519Z",
     "iopub.status.busy": "2024-07-13T05:31:05.283219Z",
     "iopub.status.idle": "2024-07-13T05:31:05.289546Z",
     "shell.execute_reply": "2024-07-13T05:31:05.288537Z"
    },
    "papermill": {
     "duration": 0.023627,
     "end_time": "2024-07-13T05:31:05.291548",
     "exception": false,
     "start_time": "2024-07-13T05:31:05.267921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/LLaMA-Factory\n"
     ]
    }
   ],
   "source": [
    "%cd LLaMA-Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda99873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:31:05.321201Z",
     "iopub.status.busy": "2024-07-13T05:31:05.320949Z",
     "iopub.status.idle": "2024-07-13T05:31:34.821232Z",
     "shell.execute_reply": "2024-07-13T05:31:34.820060Z"
    },
    "papermill": {
     "duration": 29.517896,
     "end_time": "2024-07-13T05:31:34.823672",
     "exception": false,
     "start_time": "2024-07-13T05:31:05.305776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optimum\r\n",
      "  Downloading optimum-1.21.2-py3-none-any.whl.metadata (19 kB)\r\n",
      "Collecting coloredlogs (from optimum)\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum) (1.12.1)\r\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.41.2)\r\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum) (2.1.2)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from optimum) (21.3)\r\n",
      "Requirement already satisfied: numpy<2.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (1.26.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from optimum) (0.23.2)\r\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (2.19.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.3.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->optimum) (3.1.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.3)\r\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.2.0)\r\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (3.20.3)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->optimum)\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (14.0.2)\r\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.6)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.1)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\r\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2024.2.2)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.4)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\r\n",
      "Downloading optimum-1.21.2-py3-none-any.whl (424 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.7/424.7 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, optimum\r\n",
      "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 optimum-1.21.2\r\n"
     ]
    }
   ],
   "source": [
    "# install packages for quantizaiton\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6eda2",
   "metadata": {
    "papermill": {
     "duration": 0.015731,
     "end_time": "2024-07-13T05:31:34.855576",
     "exception": false,
     "start_time": "2024-07-13T05:31:34.839845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 500, lr = 1e-04, epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469398e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:31:34.888363Z",
     "iopub.status.busy": "2024-07-13T05:31:34.888045Z",
     "iopub.status.idle": "2024-07-13T05:41:40.068781Z",
     "shell.execute_reply": "2024-07-13T05:41:40.067680Z"
    },
    "papermill": {
     "duration": 605.20007,
     "end_time": "2024-07-13T05:41:40.071250",
     "exception": false,
     "start_time": "2024-07-13T05:31:34.871180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 05:31:44.420893: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 05:31:44.421014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 05:31:44.558630: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 05:32:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "tokenizer_config.json: 100%|███████████████| 1.29k/1.29k [00:00<00:00, 10.0MB/s]\r\n",
      "vocab.json: 100%|██████████████████████████| 2.78M/2.78M [00:00<00:00, 10.6MB/s]\r\n",
      "merges.txt: 100%|██████████████████████████| 1.67M/1.67M [00:00<00:00, 6.54MB/s]\r\n",
      "tokenizer.json: 100%|██████████████████████| 7.03M/7.03M [00:01<00:00, 6.83MB/s]\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,311 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,311 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,311 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,312 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,312 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:32:04,312 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 05:32:04,596 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 05:32:04 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 05:32:04 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "Generating train split: 10178 examples [00:00, 124646.34 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1315.1\r\n",
      "07/13/2024 05:32:05 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "Generating train split: 900 examples [00:00, 40861.57 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 500/500 [00:00<00:00, 1510.8\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 1000/1000 [00:03<00:00, 268.\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "config.json: 100%|█████████████████████████████| 659/659 [00:00<00:00, 4.47MB/s]\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:32:10,842 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:32:10,846 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "model.safetensors: 100%|██████████████████████| 988M/988M [00:03<00:00, 275MB/s]\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 05:32:14,770 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 05:32:14,803 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:32:14,807 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 05:32:17,021 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 05:32:17,021 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "generation_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 2.11MB/s]\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 05:32:17,200 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:32:17,201 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,v_proj,up_proj,gate_proj,down_proj,k_proj,o_proj\r\n",
      "07/13/2024 05:32:17 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-13 05:32:17,630 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-13 05:32:18,044 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-13 05:32:18,044 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-13 05:32:18,044 >>   Num Epochs = 2\r\n",
      "[INFO|trainer.py:2081] 2024-07-13 05:32:18,044 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-13 05:32:18,044 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-13 05:32:18,044 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-13 05:32:18,044 >>   Total optimization steps = 36\r\n",
      "[INFO|trainer.py:2087] 2024-07-13 05:32:18,050 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 1.9938, 'grad_norm': 1.0436301231384277, 'learning_rate': 9.53153893518325e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "{'loss': 1.8202, 'grad_norm': 0.7930129170417786, 'learning_rate': 8.213938048432697e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 28%|███████████▉                               | 10/36 [02:26<06:27, 14.89s/it][INFO|trainer.py:3719] 2024-07-13 05:34:44,560 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:34:44,560 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:34:44,560 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.31it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.94it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.92it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.40it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.29it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.72it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.37it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.52it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.47it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.38it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.82it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.28it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.79it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.33it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.12it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.46it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.12it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.76it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.21it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.25it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.47it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.72it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.84it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.21it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.80it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.50it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.08it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.07it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.81it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.28it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.13it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4649488925933838, 'eval_accuracy': 0.7252099619317299, 'eval_runtime': 8.3893, 'eval_samples_per_second': 11.92, 'eval_steps_per_second': 5.96, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 28%|███████████▉                               | 10/36 [02:34<06:27, 14.89s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.57it/s]\u001b[A\r\n",
      "{'loss': 1.5794, 'grad_norm': 0.7500228881835938, 'learning_rate': 6.294095225512603e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      "{'loss': 1.7799, 'grad_norm': 0.7550971508026123, 'learning_rate': 4.131759111665349e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 56%|███████████████████████▉                   | 20/36 [05:02<04:03, 15.19s/it][INFO|trainer.py:3719] 2024-07-13 05:37:20,763 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:37:20,763 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:37:20,763 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.34it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.98it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  7.00it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.48it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.34it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.75it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.39it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.67it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.55it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.62it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.83it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.29it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.36it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.86it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.71it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.83it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.23it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.55it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.16it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:04<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.37it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.66it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.79it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.21it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.83it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.54it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.85it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.33it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.15it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4358832836151123, 'eval_accuracy': 0.7268516891923188, 'eval_runtime': 8.3267, 'eval_samples_per_second': 12.01, 'eval_steps_per_second': 6.005, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 56%|███████████████████████▉                   | 20/36 [05:11<04:03, 15.19s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.61it/s]\u001b[A\r\n",
      "{'loss': 1.7211, 'grad_norm': 0.8681861162185669, 'learning_rate': 2.132117818244771e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      "{'loss': 1.6156, 'grad_norm': 0.6865116953849792, 'learning_rate': 6.698729810778065e-06, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 83%|███████████████████████████████████▊       | 30/36 [07:36<01:26, 14.46s/it][INFO|trainer.py:3719] 2024-07-13 05:39:54,661 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:39:54,661 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:39:54,661 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.30it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.94it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.96it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.45it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.32it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.75it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.38it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.63it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.54it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.46it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.42it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.83it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.27it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.32it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.69it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.22it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.15it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.86it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.28it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.80it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.30it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.49it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.75it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.88it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.27it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.84it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.56it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.13it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.84it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.31it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.12it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.426135540008545, 'eval_accuracy': 0.7291995982470609, 'eval_runtime': 8.3346, 'eval_samples_per_second': 11.998, 'eval_steps_per_second': 5.999, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 83%|███████████████████████████████████▊       | 30/36 [07:44<01:26, 14.46s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.5865, 'grad_norm': 0.7832457423210144, 'learning_rate': 1.9026509541272275e-07, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      "100%|███████████████████████████████████████████| 36/36 [09:09<00:00, 15.21s/it][INFO|trainer.py:2329] 2024-07-13 05:41:27,949 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 549.8997, 'train_samples_per_second': 3.273, 'train_steps_per_second': 0.065, 'train_loss': 1.7361652188830905, 'epoch': 1.92, 'num_input_tokens_seen': 852240}\r\n",
      "100%|███████████████████████████████████████████| 36/36 [09:09<00:00, 15.27s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-13 05:41:27,951 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:41:28,213 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:41:28,215 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-13 05:41:28,268 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-13 05:41:28,268 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =       1.92\r\n",
      "  num_input_tokens_seen    =     852240\r\n",
      "  total_flos               =  1725354GF\r\n",
      "  train_loss               =     1.7362\r\n",
      "  train_runtime            = 0:09:09.89\r\n",
      "  train_samples_per_second =      3.273\r\n",
      "  train_steps_per_second   =      0.065\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 05:41:29,034 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:41:29,034 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:41:29,034 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.13it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =       1.92\r\n",
      "  eval_accuracy           =     0.7285\r\n",
      "  eval_loss               =     1.4257\r\n",
      "  eval_runtime            = 0:00:08.32\r\n",
      "  eval_samples_per_second =     12.014\r\n",
      "  eval_steps_per_second   =      6.007\r\n",
      "  num_input_tokens_seen   =     852240\r\n",
      "[INFO|modelcard.py:450] 2024-07-13 05:41:37,361 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7285274974314278}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "feba1c30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:41:40.151093Z",
     "iopub.status.busy": "2024-07-13T05:41:40.150753Z",
     "iopub.status.idle": "2024-07-13T05:44:01.852604Z",
     "shell.execute_reply": "2024-07-13T05:44:01.851353Z"
    },
    "papermill": {
     "duration": 141.74454,
     "end_time": "2024-07-13T05:44:01.855205",
     "exception": false,
     "start_time": "2024-07-13T05:41:40.110665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 05:41:45.049418: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 05:41:45.049490: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 05:41:45.051021: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 05:41:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,890 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,890 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,890 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,890 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,890 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:41:53,891 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 05:41:54,139 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 05:41:54 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 05:41:54 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "Generating train split: 1273 examples [00:00, 97629.35 examples/s]\r\n",
      "/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 272.24\r\n",
      "07/13/2024 05:41:55 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "Generating train split: 100 examples [00:00, 23815.04 examples/s]\r\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100/100 [00:00<00:00, 313.95\r\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 200/200 [00:03<00:00, 59.56 \r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:41:59,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:41:59,867 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:41:59 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 05:41:59,904 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 05:41:59,917 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:41:59,919 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 05:42:01,860 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 05:42:01,860 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 05:42:01,945 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:42:01,946 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:42:02 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 05:42:02 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/13/2024 05:42:02 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-26\r\n",
      "07/13/2024 05:42:02 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 05:42:02,574 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:42:02,574 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:42:02,574 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:54<00:00,  1.99s/it]Building prefix dict from the default dictionary ...\r\n",
      "Dumping model to file cache /tmp/jieba.cache\r\n",
      "Loading model cost 1.115 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:56<00:00,  1.16s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    29.6588\r\n",
      "  predict_rouge-1            =    38.1672\r\n",
      "  predict_rouge-2            =     17.703\r\n",
      "  predict_rouge-l            =    33.5921\r\n",
      "  predict_runtime            = 0:01:56.66\r\n",
      "  predict_samples_per_second =      1.714\r\n",
      "  predict_steps_per_second   =      0.857\r\n",
      "07/13/2024 05:43:59 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-35/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-35 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f19f6d",
   "metadata": {
    "papermill": {
     "duration": 0.050233,
     "end_time": "2024-07-13T05:44:01.957898",
     "exception": false,
     "start_time": "2024-07-13T05:44:01.907665",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "max_example = 500, lr = 1e-04, epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98ae145a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T05:44:02.059082Z",
     "iopub.status.busy": "2024-07-13T05:44:02.058198Z",
     "iopub.status.idle": "2024-07-13T06:07:48.663113Z",
     "shell.execute_reply": "2024-07-13T06:07:48.661982Z"
    },
    "papermill": {
     "duration": 1426.658238,
     "end_time": "2024-07-13T06:07:48.665589",
     "exception": false,
     "start_time": "2024-07-13T05:44:02.007351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 05:44:06.981413: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 05:44:06.981489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 05:44:06.982943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 05:44:15 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 05:44:15,977 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 05:44:16,233 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 05:44:16 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 05:44:16 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\r\n",
      "07/13/2024 05:44:16 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "label_ids:\r\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\r\n",
      "labels:\r\n",
      "E: Nitrofurantoin<|im_end|>\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 05:44:17,097 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 05:44:17,098 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 05:44:17,132 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 05:44:17,143 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:44:17,145 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 05:44:18,824 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 05:44:18,825 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 05:44:18,914 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 05:44:18,914 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 05:44:18 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\r\n",
      "07/13/2024 05:44:18 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 05:44:18 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\r\n",
      "07/13/2024 05:44:18 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\r\n",
      "07/13/2024 05:44:18 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,q_proj,k_proj,up_proj,down_proj,v_proj,o_proj\r\n",
      "07/13/2024 05:44:19 - INFO - llamafactory.model.loader - trainable params: 4,399,104 || all params: 498,431,872 || trainable%: 0.8826\r\n",
      "[INFO|trainer.py:641] 2024-07-13 05:44:19,320 >> Using auto half precision backend\r\n",
      "[INFO|trainer.py:2078] 2024-07-13 05:44:19,744 >> ***** Running training *****\r\n",
      "[INFO|trainer.py:2079] 2024-07-13 05:44:19,744 >>   Num examples = 900\r\n",
      "[INFO|trainer.py:2080] 2024-07-13 05:44:19,744 >>   Num Epochs = 5\r\n",
      "[INFO|trainer.py:2081] 2024-07-13 05:44:19,744 >>   Instantaneous batch size per device = 6\r\n",
      "[INFO|trainer.py:2084] 2024-07-13 05:44:19,744 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\r\n",
      "[INFO|trainer.py:2085] 2024-07-13 05:44:19,744 >>   Gradient Accumulation steps = 8\r\n",
      "[INFO|trainer.py:2086] 2024-07-13 05:44:19,744 >>   Total optimization steps = 90\r\n",
      "[INFO|trainer.py:2087] 2024-07-13 05:44:19,750 >>   Number of trainable parameters = 4,399,104\r\n",
      "{'loss': 1.9936, 'grad_norm': 1.0407028198242188, 'learning_rate': 9.924038765061042e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\r\n",
      "{'loss': 1.8177, 'grad_norm': 0.7885110378265381, 'learning_rate': 9.698463103929542e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 11%|████▊                                      | 10/90 [02:26<19:52, 14.91s/it][INFO|trainer.py:3719] 2024-07-13 05:46:46,121 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:46:46,122 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:46:46,122 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.30it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.94it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.88it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.37it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:07,  6.28it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.73it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.35it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.52it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.47it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.41it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.37it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.56it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.36it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.80it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.26it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.80it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.34it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.84it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.69it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.20it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.51it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.13it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.79it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.23it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.71it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.46it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.73it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.87it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.26it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.86it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.55it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.84it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.18it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.52it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.10it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4619046449661255, 'eval_accuracy': 0.7265975182764022, 'eval_runtime': 8.378, 'eval_samples_per_second': 11.936, 'eval_steps_per_second': 5.968, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\r\n",
      " 11%|████▊                                      | 10/90 [02:34<19:52, 14.91s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.57it/s]\u001b[A\r\n",
      "{'loss': 1.5763, 'grad_norm': 0.7685031890869141, 'learning_rate': 9.330127018922194e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\r\n",
      "{'loss': 1.7736, 'grad_norm': 0.8545358180999756, 'learning_rate': 8.83022221559489e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 22%|█████████▌                                 | 20/90 [05:02<17:43, 15.19s/it][INFO|trainer.py:3719] 2024-07-13 05:49:22,094 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:49:22,094 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:49:22,094 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.20it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.94it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.92it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.42it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.31it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.73it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.37it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.64it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.54it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.46it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.59it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.38it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.83it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.28it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.81it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.31it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.21it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.17it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.89it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.83it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.31it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.56it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.81it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.93it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.88it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.57it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.83it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.30it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.15it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4283132553100586, 'eval_accuracy': 0.725370396469794, 'eval_runtime': 8.3265, 'eval_samples_per_second': 12.01, 'eval_steps_per_second': 6.005, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\r\n",
      " 22%|█████████▌                                 | 20/90 [05:10<17:43, 15.19s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.7064, 'grad_norm': 0.8348555564880371, 'learning_rate': 8.213938048432697e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\r\n",
      "{'loss': 1.6014, 'grad_norm': 0.6906702518463135, 'learning_rate': 7.500000000000001e-05, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 33%|██████████████▎                            | 30/90 [07:35<14:27, 14.46s/it][INFO|trainer.py:3719] 2024-07-13 05:51:55,707 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:51:55,708 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:51:55,708 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.29it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.96it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.93it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.40it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.29it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.74it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.35it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.55it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.49it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.55it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.35it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.78it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.24it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.77it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.25it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.75it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.64it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:04,  5.78it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.13it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.43it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.10it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.79it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.23it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.50it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.86it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.23it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.83it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.52it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.08it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.10it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.82it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.27it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.12it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.420336127281189, 'eval_accuracy': 0.7265339474052419, 'eval_runtime': 8.3862, 'eval_samples_per_second': 11.924, 'eval_steps_per_second': 5.962, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\r\n",
      " 33%|██████████████▎                            | 30/90 [07:44<14:27, 14.46s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.56it/s]\u001b[A\r\n",
      "{'loss': 1.568, 'grad_norm': 0.7193354964256287, 'learning_rate': 6.710100716628344e-05, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\r\n",
      "{'loss': 1.7191, 'grad_norm': 0.7499760985374451, 'learning_rate': 5.868240888334653e-05, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 44%|███████████████████                        | 40/90 [10:07<12:04, 14.48s/it][INFO|trainer.py:3719] 2024-07-13 05:54:27,412 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:54:27,412 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:54:27,412 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.28it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.93it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.90it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.38it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:07,  6.27it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.73it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.35it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.60it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.41it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.37it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.27it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.79it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.33it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.20it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.51it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.13it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.86it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.27it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.72it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.49it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.87it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.24it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.81it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.51it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.09it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.08it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.81it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.26it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.57it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.12it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4201980829238892, 'eval_accuracy': 0.722223321467927, 'eval_runtime': 8.3637, 'eval_samples_per_second': 11.956, 'eval_steps_per_second': 5.978, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\r\n",
      " 44%|███████████████████                        | 40/90 [10:16<12:04, 14.48s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.58it/s]\u001b[A\r\n",
      "{'loss': 1.569, 'grad_norm': 0.773148238658905, 'learning_rate': 5e-05, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\r\n",
      "{'loss': 1.613, 'grad_norm': 0.792067289352417, 'learning_rate': 4.131759111665349e-05, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 56%|███████████████████████▉                   | 50/90 [12:44<10:16, 15.40s/it][INFO|trainer.py:3719] 2024-07-13 05:57:04,628 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:57:04,628 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:57:04,628 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.28it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.94it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.90it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.38it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:07,  6.28it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.72it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.36it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.55it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:06,  6.49it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.37it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.82it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.26it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.78it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.32it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.68it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.79it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.18it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.50it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.12it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.84it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.24it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.70it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.21it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.46it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.71it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.84it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.24it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.85it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.55it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.82it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.31it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.61it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.14it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4222478866577148, 'eval_accuracy': 0.7250541071962011, 'eval_runtime': 8.3666, 'eval_samples_per_second': 11.952, 'eval_steps_per_second': 5.976, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\r\n",
      " 56%|███████████████████████▉                   | 50/90 [12:53<10:16, 15.40s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.5691, 'grad_norm': 0.8409530520439148, 'learning_rate': 3.289899283371657e-05, 'epoch': 2.93, 'num_input_tokens_seen': 1304544}\r\n",
      "{'loss': 1.5755, 'grad_norm': 0.8068133592605591, 'learning_rate': 2.500000000000001e-05, 'epoch': 3.2, 'num_input_tokens_seen': 1422912}\r\n",
      " 67%|████████████████████████████▋              | 60/90 [15:15<06:57, 13.93s/it][INFO|trainer.py:3719] 2024-07-13 05:59:35,033 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 05:59:35,033 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 05:59:35,033 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.23it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.97it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.97it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.43it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.32it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.75it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.38it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.71it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.58it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.47it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.43it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.61it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.84it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.27it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.79it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.31it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.81it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.67it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.22it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.53it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.15it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.90it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.30it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.76it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.28it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.51it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.79it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.92it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.29it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.87it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.58it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.14it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.84it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.29it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.13it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.4216890335083008, 'eval_accuracy': 0.7217010900435689, 'eval_runtime': 8.3265, 'eval_samples_per_second': 12.01, 'eval_steps_per_second': 6.005, 'epoch': 3.2, 'num_input_tokens_seen': 1422912}\r\n",
      " 67%|████████████████████████████▋              | 60/90 [15:23<06:57, 13.93s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.6008, 'grad_norm': 0.7875239849090576, 'learning_rate': 1.7860619515673033e-05, 'epoch': 3.47, 'num_input_tokens_seen': 1542384}\r\n",
      "{'loss': 1.5824, 'grad_norm': 0.8309534192085266, 'learning_rate': 1.1697777844051105e-05, 'epoch': 3.73, 'num_input_tokens_seen': 1661040}\r\n",
      " 78%|█████████████████████████████████▍         | 70/90 [17:50<05:01, 15.05s/it][INFO|trainer.py:3719] 2024-07-13 06:02:10,725 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 06:02:10,725 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 06:02:10,726 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.20it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.95it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.93it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.40it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.29it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.72it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.36it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.60it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.39it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.37it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.80it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.25it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.79it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.28it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.80it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.65it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:04,  5.78it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.18it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.50it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.12it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.79it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.22it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.74it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.48it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.86it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.25it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.85it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.54it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.10it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.10it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.81it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.30it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.60it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.14it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.421587347984314, 'eval_accuracy': 0.7247275277940112, 'eval_runtime': 8.3669, 'eval_samples_per_second': 11.952, 'eval_steps_per_second': 5.976, 'epoch': 3.73, 'num_input_tokens_seen': 1661040}\r\n",
      " 78%|█████████████████████████████████▍         | 70/90 [17:59<05:01, 15.05s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.59it/s]\u001b[A\r\n",
      "{'loss': 1.646, 'grad_norm': 0.834176242351532, 'learning_rate': 6.698729810778065e-06, 'epoch': 4.0, 'num_input_tokens_seen': 1788528}\r\n",
      "{'loss': 1.5452, 'grad_norm': 0.7634144425392151, 'learning_rate': 3.0153689607045845e-06, 'epoch': 4.27, 'num_input_tokens_seen': 1912368}\r\n",
      " 89%|██████████████████████████████████████▏    | 80/90 [20:35<02:37, 15.77s/it][INFO|trainer.py:3719] 2024-07-13 06:04:55,012 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 06:04:55,012 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 06:04:55,012 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.19it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.93it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.94it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.42it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.29it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.69it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.34it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.59it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.50it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:07,  5.42it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.55it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.35it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.81it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.26it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.78it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.31it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.79it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.67it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:04<00:04,  5.78it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.17it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.49it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.11it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.84it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.25it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.51it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.74it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.85it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.24it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.85it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.56it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.12it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.82it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.30it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.59it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.15it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.421940565109253, 'eval_accuracy': 0.7240988440093933, 'eval_runtime': 8.3665, 'eval_samples_per_second': 11.952, 'eval_steps_per_second': 5.976, 'epoch': 4.27, 'num_input_tokens_seen': 1912368}\r\n",
      " 89%|██████████████████████████████████████▏    | 80/90 [20:43<02:37, 15.77s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.58it/s]\u001b[A\r\n",
      "{'loss': 1.4951, 'grad_norm': 0.8339309096336365, 'learning_rate': 7.596123493895991e-07, 'epoch': 4.53, 'num_input_tokens_seen': 2027760}\r\n",
      "{'loss': 1.5337, 'grad_norm': 0.8660382628440857, 'learning_rate': 0.0, 'epoch': 4.8, 'num_input_tokens_seen': 2147856}\r\n",
      "100%|███████████████████████████████████████████| 90/90 [23:09<00:00, 15.01s/it][INFO|trainer.py:3719] 2024-07-13 06:07:28,880 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 06:07:28,880 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 06:07:28,880 >>   Batch size = 2\r\n",
      "\r\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\r\n",
      "  4%|█▊                                          | 2/50 [00:00<00:05,  8.29it/s]\u001b[A\r\n",
      "  6%|██▋                                         | 3/50 [00:00<00:07,  5.96it/s]\u001b[A\r\n",
      "  8%|███▌                                        | 4/50 [00:00<00:06,  6.97it/s]\u001b[A\r\n",
      " 10%|████▍                                       | 5/50 [00:00<00:06,  7.45it/s]\u001b[A\r\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:06,  6.32it/s]\u001b[A\r\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:07,  5.73it/s]\u001b[A\r\n",
      " 16%|███████                                     | 8/50 [00:01<00:07,  5.37it/s]\u001b[A\r\n",
      " 20%|████████▌                                  | 10/50 [00:01<00:05,  7.59it/s]\u001b[A\r\n",
      " 22%|█████████▍                                 | 11/50 [00:01<00:05,  6.51it/s]\u001b[A\r\n",
      " 24%|██████████▎                                | 12/50 [00:01<00:06,  5.44it/s]\u001b[A\r\n",
      " 26%|███████████▏                               | 13/50 [00:02<00:06,  5.40it/s]\u001b[A\r\n",
      " 28%|████████████                               | 14/50 [00:02<00:06,  5.58it/s]\u001b[A\r\n",
      " 30%|████████████▉                              | 15/50 [00:02<00:06,  5.37it/s]\u001b[A\r\n",
      " 32%|█████████████▊                             | 16/50 [00:02<00:05,  5.82it/s]\u001b[A\r\n",
      " 36%|███████████████▍                           | 18/50 [00:02<00:05,  6.27it/s]\u001b[A\r\n",
      " 38%|████████████████▎                          | 19/50 [00:03<00:05,  5.80it/s]\u001b[A\r\n",
      " 42%|██████████████████                         | 21/50 [00:03<00:04,  6.33it/s]\u001b[A\r\n",
      " 44%|██████████████████▉                        | 22/50 [00:03<00:04,  5.82it/s]\u001b[A\r\n",
      " 46%|███████████████████▊                       | 23/50 [00:03<00:04,  5.66it/s]\u001b[A\r\n",
      " 48%|████████████████████▋                      | 24/50 [00:03<00:04,  5.78it/s]\u001b[A\r\n",
      " 52%|██████████████████████▎                    | 26/50 [00:04<00:03,  7.18it/s]\u001b[A\r\n",
      " 54%|███████████████████████▏                   | 27/50 [00:04<00:03,  6.48it/s]\u001b[A\r\n",
      " 56%|████████████████████████                   | 28/50 [00:04<00:03,  6.08it/s]\u001b[A\r\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:04<00:02,  6.79it/s]\u001b[A\r\n",
      " 62%|██████████████████████████▋                | 31/50 [00:05<00:03,  6.23it/s]\u001b[A\r\n",
      " 66%|████████████████████████████▍              | 33/50 [00:05<00:02,  6.73it/s]\u001b[A\r\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:05<00:02,  6.24it/s]\u001b[A\r\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:05<00:02,  6.49it/s]\u001b[A\r\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:05<00:01,  6.75it/s]\u001b[A\r\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:06<00:01,  6.87it/s]\u001b[A\r\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:06<00:01,  6.25it/s]\u001b[A\r\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:06<00:01,  5.84it/s]\u001b[A\r\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:06<00:01,  6.52it/s]\u001b[A\r\n",
      " 84%|████████████████████████████████████       | 42/50 [00:06<00:01,  6.10it/s]\u001b[A\r\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:06<00:01,  6.11it/s]\u001b[A\r\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:07<00:01,  5.84it/s]\u001b[A\r\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:07<00:00,  6.26it/s]\u001b[A\r\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:07<00:00,  5.58it/s]\u001b[A\r\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:07<00:00,  5.13it/s]\u001b[A\r\n",
      "\r\n",
      "\u001b[A{'eval_loss': 1.421971321105957, 'eval_accuracy': 0.7227982336028508, 'eval_runtime': 8.3643, 'eval_samples_per_second': 11.956, 'eval_steps_per_second': 5.978, 'epoch': 4.8, 'num_input_tokens_seen': 2147856}\r\n",
      "100%|███████████████████████████████████████████| 90/90 [23:17<00:00, 15.01s/it]\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  5.58it/s]\u001b[A\r\n",
      "                                                                                \u001b[A[INFO|trainer.py:2329] 2024-07-13 06:07:37,245 >> \r\n",
      "\r\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\r\n",
      "\r\n",
      "\r\n",
      "{'train_runtime': 1397.4951, 'train_samples_per_second': 3.22, 'train_steps_per_second': 0.064, 'train_loss': 1.638108311759101, 'epoch': 4.8, 'num_input_tokens_seen': 2147856}\r\n",
      "100%|███████████████████████████████████████████| 90/90 [23:17<00:00, 15.53s/it]\r\n",
      "[INFO|trainer.py:3410] 2024-07-13 06:07:37,247 >> Saving model checkpoint to savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 06:07:37,510 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 06:07:37,512 >> Model config Qwen2Config {\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-07-13 06:07:37,564 >> tokenizer config file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/tokenizer_config.json\r\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-07-13 06:07:37,564 >> Special tokens file saved in savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/special_tokens_map.json\r\n",
      "***** train metrics *****\r\n",
      "  epoch                    =        4.8\r\n",
      "  num_input_tokens_seen    =    2147856\r\n",
      "  total_flos               =  4348320GF\r\n",
      "  train_loss               =     1.6381\r\n",
      "  train_runtime            = 0:23:17.49\r\n",
      "  train_samples_per_second =       3.22\r\n",
      "  train_steps_per_second   =      0.064\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_loss.png\r\n",
      "Figure saved at: savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27/training_eval_accuracy.png\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 06:07:38,255 >> ***** Running Evaluation *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 06:07:38,255 >>   Num examples = 100\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 06:07:38,255 >>   Batch size = 2\r\n",
      "100%|███████████████████████████████████████████| 50/50 [00:08<00:00,  6.12it/s]\r\n",
      "***** eval metrics *****\r\n",
      "  epoch                   =        4.8\r\n",
      "  eval_accuracy           =     0.7228\r\n",
      "  eval_loss               =      1.422\r\n",
      "  eval_runtime            = 0:00:08.34\r\n",
      "  eval_samples_per_second =     11.986\r\n",
      "  eval_steps_per_second   =      5.993\r\n",
      "  num_input_tokens_seen   =    2147856\r\n",
      "[INFO|modelcard.py:450] 2024-07-13 06:07:46,601 >> Dropping the following result as it does not have all the necessary fields:\r\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.7227982336028508}]}\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-04 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bbb645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T06:07:48.851722Z",
     "iopub.status.busy": "2024-07-13T06:07:48.851356Z",
     "iopub.status.idle": "2024-07-13T06:10:07.242449Z",
     "shell.execute_reply": "2024-07-13T06:10:07.241347Z"
    },
    "papermill": {
     "duration": 138.485609,
     "end_time": "2024-07-13T06:10:07.244862",
     "exception": false,
     "start_time": "2024-07-13T06:07:48.759253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-13 06:07:53.957431: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2024-07-13 06:07:53.957505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2024-07-13 06:07:53.959064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "07/13/2024 06:08:02 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/vocab.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/merges.txt\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer.json\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file added_tokens.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file special_tokens_map.json from cache at None\r\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-07-13 06:08:03,099 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/tokenizer_config.json\r\n",
      "[WARNING|logging.py:314] 2024-07-13 06:08:03,407 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n",
      "07/13/2024 06:08:03 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\r\n",
      "07/13/2024 06:08:03 - INFO - llamafactory.data.loader - Loading dataset MedQA/test.json...\r\n",
      "07/13/2024 06:08:03 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_test_set.json...\r\n",
      "input_ids:\r\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 714, 5274, 432, 700, 315, 279, 63785, 1895, 11, 425, 25, 11735, 1469, 279, 1465, 311, 279, 8720, 323, 2182, 432, 304, 279, 63785, 1895, 11, 356, 25, 24647, 279, 23218, 429, 566, 4157, 3690, 311, 35233, 419, 16523, 11, 422, 25, 8259, 279, 27279, 311, 279, 30908, 12801, 11, 468, 25, 8550, 810, 311, 61874, 279, 63785, 1895, 151645, 198, 151644, 77091, 198]\r\n",
      "inputs:\r\n",
      "<|im_start|>system\r\n",
      "You are a helpful assistant.<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "Answer the following multiple choice question\r\n",
      "A: Disclose the error to the patient but leave it out of the operative report, B: Disclose the error to the patient and put it in the operative report, C: Tell the attending that he cannot fail to disclose this mistake, D: Report the physician to the ethics committee, E: Refuse to dictate the operative report<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "\r\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n",
      "  warnings.warn(\r\n",
      "[INFO|configuration_utils.py:733] 2024-07-13 06:08:04,350 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/config.json\r\n",
      "[INFO|configuration_utils.py:796] 2024-07-13 06:08:04,352 >> Model config Qwen2Config {\r\n",
      "  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\r\n",
      "  \"architectures\": [\r\n",
      "    \"Qwen2ForCausalLM\"\r\n",
      "  ],\r\n",
      "  \"attention_dropout\": 0.0,\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645,\r\n",
      "  \"hidden_act\": \"silu\",\r\n",
      "  \"hidden_size\": 896,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 4864,\r\n",
      "  \"max_position_embeddings\": 32768,\r\n",
      "  \"max_window_layers\": 24,\r\n",
      "  \"model_type\": \"qwen2\",\r\n",
      "  \"num_attention_heads\": 14,\r\n",
      "  \"num_hidden_layers\": 24,\r\n",
      "  \"num_key_value_heads\": 2,\r\n",
      "  \"rms_norm_eps\": 1e-06,\r\n",
      "  \"rope_theta\": 1000000.0,\r\n",
      "  \"sliding_window\": 32768,\r\n",
      "  \"tie_word_embeddings\": true,\r\n",
      "  \"torch_dtype\": \"bfloat16\",\r\n",
      "  \"transformers_version\": \"4.41.2\",\r\n",
      "  \"use_cache\": true,\r\n",
      "  \"use_sliding_window\": false,\r\n",
      "  \"vocab_size\": 151936\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 06:08:04 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\r\n",
      "[INFO|modeling_utils.py:3474] 2024-07-13 06:08:04,394 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/model.safetensors\r\n",
      "[INFO|modeling_utils.py:1519] 2024-07-13 06:08:04,406 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 06:08:04,409 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"eos_token_id\": 151645\r\n",
      "}\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4280] 2024-07-13 06:08:06,357 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\r\n",
      "\r\n",
      "[INFO|modeling_utils.py:4288] 2024-07-13 06:08:06,357 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\r\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\r\n",
      "[INFO|configuration_utils.py:917] 2024-07-13 06:08:06,450 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c291d6fce4804a1d39305f388dd32897d1f7acc4/generation_config.json\r\n",
      "[INFO|configuration_utils.py:962] 2024-07-13 06:08:06,451 >> Generate config GenerationConfig {\r\n",
      "  \"bos_token_id\": 151643,\r\n",
      "  \"do_sample\": true,\r\n",
      "  \"eos_token_id\": [\r\n",
      "    151645,\r\n",
      "    151643\r\n",
      "  ],\r\n",
      "  \"pad_token_id\": 151643,\r\n",
      "  \"repetition_penalty\": 1.1,\r\n",
      "  \"temperature\": 0.7,\r\n",
      "  \"top_k\": 20,\r\n",
      "  \"top_p\": 0.8\r\n",
      "}\r\n",
      "\r\n",
      "07/13/2024 06:08:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\r\n",
      "07/13/2024 06:08:07 - INFO - llamafactory.model.adapter - Merged 1 adapter(s).\r\n",
      "07/13/2024 06:08:07 - INFO - llamafactory.model.adapter - Loaded adapter(s): savesQwen2-0.5B-Instructloratrain_2024-07-08-20-40-27\r\n",
      "07/13/2024 06:08:07 - INFO - llamafactory.model.loader - all params: 494,032,768\r\n",
      "[INFO|trainer.py:3719] 2024-07-13 06:08:07,090 >> ***** Running Prediction *****\r\n",
      "[INFO|trainer.py:3721] 2024-07-13 06:08:07,090 >>   Num examples = 200\r\n",
      "[INFO|trainer.py:3724] 2024-07-13 06:08:07,090 >>   Batch size = 2\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:55<00:00,  1.99s/it]Building prefix dict from the default dictionary ...\r\n",
      "Loading model from cache /tmp/jieba.cache\r\n",
      "Loading model cost 0.917 seconds.\r\n",
      "Prefix dict has been built successfully.\r\n",
      "100%|█████████████████████████████████████████| 100/100 [01:56<00:00,  1.17s/it]\r\n",
      "***** predict metrics *****\r\n",
      "  predict_bleu-4             =    30.6168\r\n",
      "  predict_rouge-1            =    37.9136\r\n",
      "  predict_rouge-2            =    16.4099\r\n",
      "  predict_rouge-l            =    33.4206\r\n",
      "  predict_runtime            = 0:01:57.53\r\n",
      "  predict_samples_per_second =      1.702\r\n",
      "  predict_steps_per_second   =      0.851\r\n",
      "07/13/2024 06:10:04 - INFO - llamafactory.train.sft.trainer - Saving prediction results to savesQwen2-0.5B-Instructloraeval_2024-07-09-00-25-36/generated_predictions.jsonl\r\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --model_name_or_path Qwen/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_test,PubMedQA_pqal_test \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --max_samples 100 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --predict_with_generate True \\\n",
    "    --max_new_tokens 512 \\\n",
    "    --top_p 0.7 \\\n",
    "    --temperature 0.95 \\\n",
    "    --output_dir saves\\Qwen2-0.5B-Instruct\\lora\\eval_2024-07-09-00-25-36 \\\n",
    "    --do_predict True \\\n",
    "    --adapter_name_or_path saves\\Qwen2-0.5B-Instruct\\lora\\train_2024-07-08-20-40-27"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5295003,
     "sourceId": 8869298,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351712,
     "sourceId": 8902036,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351730,
     "sourceId": 8902062,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2421.074284,
   "end_time": "2024-07-13T06:10:07.570636",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-13T05:29:46.496352",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
