{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /root/miniconda3/lib/python3.10/site-packages (4.42.4)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /root/miniconda3/lib/python3.10/site-packages (0.32.1)\n",
      "Requirement already satisfied: peft>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (0.11.1)\n",
      "Requirement already satisfied: trl>=0.8.6 in /root/miniconda3/lib/python3.10/site-packages (0.9.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/miniconda3/lib/python3.10/site-packages (from trl>=0.8.6) (0.8.5)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (1.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=46bb0f24403b0b752105640efe762f5b24dab6d169ce4b7153fa77be1d58ee82\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7jqhldo1/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.8.2.dev0\n",
      "    Uninstalling llamafactory-0.8.2.dev0:\n",
      "      Successfully uninstalled llamafactory-0.8.2.dev0\n",
      "Successfully installed llamafactory-0.8.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (16.1.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: optimum in /root/miniconda3/lib/python3.10/site-packages (1.21.2)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.23.4)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping apex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-14 13:00:33,959] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/14/2024 13:00:43 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:00:43,227 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-14 13:00:43,541 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/14/2024 13:00:43 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/14/2024 13:00:43 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/14/2024 13:00:49 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-14 13:00:50,126 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-14 13:00:50,128 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-14 13:00:50,164 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-14 13:00:50,618 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:00:50,621 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-14 13:01:07,697 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-14 13:01:07,698 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-14 13:01:07,705 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:01:07,705 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/14/2024 13:01:07 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/14/2024 13:01:07 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/14/2024 13:01:07 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/14/2024 13:01:07 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\n",
      "07/14/2024 13:01:07 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-14 13:01:07,880 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-14 13:01:08,268 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-14 13:01:08,268 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-14 13:01:08,269 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-14 13:01:08,269 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-14 13:01:08,269 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-14 13:01:08,269 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-14 13:01:08,269 >>   Total optimization steps = 112\n",
      "[INFO|trainer.py:2137] 2024-07-14 13:01:08,270 >>   Number of trainable parameters = 494,032,768\n",
      "{'loss': 1.9123, 'grad_norm': 29.53398895263672, 'learning_rate': 9.982307470588096e-07, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5557, 'grad_norm': 17.891132354736328, 'learning_rate': 9.874639560909118e-07, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:19<02:11,  1.29s/it][INFO|trainer.py:3788] 2024-07-14 13:01:27,726 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:01:27,727 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:01:27,727 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.43it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.22it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.42it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.50it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.11it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.69it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 32.41it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 32.03it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.91it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5556182861328125, 'eval_runtime': 1.624, 'eval_samples_per_second': 61.578, 'eval_steps_per_second': 30.789, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:21<02:11,  1.29s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.53it/s]\u001b[A\n",
      "{'loss': 1.6201, 'grad_norm': 14.651189804077148, 'learning_rate': 9.671244701472997e-07, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.4347, 'grad_norm': 15.660823822021484, 'learning_rate': 9.376117109543768e-07, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:32<01:48,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:01:40,676 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:01:40,676 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:01:40,676 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.54it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.61it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.72it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.85it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 27.25it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:00, 28.21it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:00<00:00, 30.03it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 30.62it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 30.90it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 31.26it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 31.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.468807578086853, 'eval_runtime': 1.75, 'eval_samples_per_second': 57.144, 'eval_steps_per_second': 28.572, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:34<01:48,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.60it/s]\u001b[A\n",
      "{'loss': 1.2972, 'grad_norm': 13.177186965942383, 'learning_rate': 8.995052426791245e-07, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3497, 'grad_norm': 13.163229942321777, 'learning_rate': 8.535533905932737e-07, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:44<01:30,  1.10s/it][INFO|trainer.py:3788] 2024-07-14 13:01:53,181 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:01:53,182 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:01:53,182 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.43it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.11it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.25it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.19it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.55it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.78it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.39it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.21it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.67it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.59it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4489156007766724, 'eval_runtime': 1.6393, 'eval_samples_per_second': 61.003, 'eval_steps_per_second': 30.501, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:46<01:30,  1.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.27it/s]\u001b[A\n",
      "{'loss': 1.5853, 'grad_norm': 13.811386108398438, 'learning_rate': 8.006585456492029e-07, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.2159, 'grad_norm': 11.906137466430664, 'learning_rate': 7.418594435526199e-07, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [00:57<01:22,  1.14s/it][INFO|trainer.py:3788] 2024-07-14 13:02:06,171 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:02:06,171 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:02:06,171 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.86it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.49it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 26.66it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.96it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.80it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.71it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.99it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.97it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.89it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 24.60it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 26.52it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 28.02it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 28.40it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4370324611663818, 'eval_runtime': 1.9513, 'eval_samples_per_second': 51.248, 'eval_steps_per_second': 25.624, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [00:59<01:22,  1.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 28.33it/s]\u001b[A\n",
      "{'loss': 1.1827, 'grad_norm': 15.056358337402344, 'learning_rate': 6.783107663311564e-07, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.4316, 'grad_norm': 12.114006042480469, 'learning_rate': 6.112604669781572e-07, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:11<01:18,  1.26s/it][INFO|trainer.py:3788] 2024-07-14 13:02:20,092 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:02:20,092 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:02:20,092 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.96it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 28.16it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 27.72it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 25.32it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.80it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.21it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.39it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.70it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.63it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.48it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.38it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.33it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 22.91it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 23.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4305413961410522, 'eval_runtime': 2.0891, 'eval_samples_per_second': 47.868, 'eval_steps_per_second': 23.934, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:13<01:18,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.83it/s]\u001b[A\n",
      "{'loss': 1.5122, 'grad_norm': 14.086160659790039, 'learning_rate': 5.420252624646237e-07, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.4736, 'grad_norm': 12.648924827575684, 'learning_rate': 4.71964776381404e-07, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:25<00:59,  1.15s/it][INFO|trainer.py:3788] 2024-07-14 13:02:33,315 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:02:33,316 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:02:33,316 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.16it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.34it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.41it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.82it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.00it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.52it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.32it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.74it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.67it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4244674444198608, 'eval_runtime': 1.6362, 'eval_samples_per_second': 61.117, 'eval_steps_per_second': 30.558, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:26<00:59,  1.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.03it/s]\u001b[A\n",
      "{'loss': 1.439, 'grad_norm': 13.408970832824707, 'learning_rate': 4.0245483899193586e-07, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.3617, 'grad_norm': 12.730737686157227, 'learning_rate': 3.348604690224166e-07, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [01:40<00:57,  1.38s/it][INFO|trainer.py:3788] 2024-07-14 13:02:49,163 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:02:49,163 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:02:49,163 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.51it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.19it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.92it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.91it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.67it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.09it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.54it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.59it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.53it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.51it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.43it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.26it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 24.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4233589172363281, 'eval_runtime': 2.1044, 'eval_samples_per_second': 47.519, 'eval_steps_per_second': 23.76, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [01:42<00:57,  1.38s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 25.58it/s]\u001b[A\n",
      "{'loss': 1.4645, 'grad_norm': 12.157377243041992, 'learning_rate': 2.7050906776623104e-07, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.3317, 'grad_norm': 13.8226900100708, 'learning_rate': 2.1066435191009713e-07, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [01:53<00:35,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:03:02,214 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:03:02,214 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:03:02,214 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 35.89it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.87it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.87it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 30.65it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 30.96it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.20it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 29.91it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 28.52it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 28.85it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 29.30it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 29.93it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4228280782699585, 'eval_runtime': 1.7132, 'eval_samples_per_second': 58.372, 'eval_steps_per_second': 29.186, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [01:55<00:35,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.23it/s]\u001b[A\n",
      "{'loss': 1.3138, 'grad_norm': 11.527843475341797, 'learning_rate': 1.5650153698254913e-07, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.4291, 'grad_norm': 12.613454818725586, 'learning_rate': 1.090842587659851e-07, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [02:06<00:25,  1.16s/it][INFO|trainer.py:3788] 2024-07-14 13:03:15,026 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:03:15,026 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:03:15,026 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.74it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.20it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.20it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.10it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.23it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.96it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.42it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.94it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 24.02it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.99it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.93it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.48it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 23.35it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.422878384590149, 'eval_runtime': 2.1197, 'eval_samples_per_second': 47.177, 'eval_steps_per_second': 23.588, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [02:08<00:25,  1.16s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.79it/s]\u001b[A\n",
      "{'loss': 1.2501, 'grad_norm': 8.991713523864746, 'learning_rate': 6.934368588379552e-08, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.2992, 'grad_norm': 10.905558586120605, 'learning_rate': 3.806023374435663e-08, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:19<00:13,  1.16s/it][INFO|trainer.py:3788] 2024-07-14 13:03:28,196 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:03:28,196 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:03:28,196 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.52it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.23it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 24.58it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.53it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.41it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.30it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 22.96it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.12it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.03it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.12it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 22.88it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 22.72it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 22.30it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 22.33it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4224437475204468, 'eval_runtime': 2.1969, 'eval_samples_per_second': 45.519, 'eval_steps_per_second': 22.759, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:22<00:13,  1.16s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.10it/s]\u001b[A\n",
      "{'loss': 1.2452, 'grad_norm': 12.158559799194336, 'learning_rate': 1.584823893886933e-08, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.598, 'grad_norm': 11.862238883972168, 'learning_rate': 3.1438950533786977e-09, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [02:34<00:02,  1.34s/it][INFO|trainer.py:3788] 2024-07-14 13:03:42,894 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:03:42,894 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:03:42,894 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.76it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.69it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 26.67it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.94it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.83it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.40it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.66it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 24.08it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 24.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.94it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.85it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.38it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 23.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4223663806915283, 'eval_runtime': 2.0954, 'eval_samples_per_second': 47.723, 'eval_steps_per_second': 23.862, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [02:36<00:02,  1.34s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.99it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 112/112 [02:38<00:00,  1.67s/it][INFO|trainer.py:3478] 2024-07-14 13:03:47,229 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:03:47,244 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:03:47,256 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:03:52,704 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:03:52,717 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:03:52,727 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/checkpoint-112/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-14 13:04:03,454 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 175.1852, 'train_samples_per_second': 10.275, 'train_steps_per_second': 0.639, 'train_loss': 1.423734894820622, 'epoch': 1.99, 'num_input_tokens_seen': 631296}\n",
      "100%|█████████████████████████████████████████| 112/112 [02:55<00:00,  1.56s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-14 13:04:03,465 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_epoch=2\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:04:03,478 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:04:03,488 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:04:10,519 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:04:10,533 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:04:10,543 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9911\n",
      "  num_input_tokens_seen    =     631296\n",
      "  total_flos               =  1262536GF\n",
      "  train_loss               =     1.4237\n",
      "  train_runtime            = 0:02:55.18\n",
      "  train_samples_per_second =     10.275\n",
      "  train_steps_per_second   =      0.639\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_epoch=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_epoch=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-14 13:04:11,270 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:04:11,270 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:04:11,270 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.72it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9911\n",
      "  eval_loss               =     1.4223\n",
      "  eval_runtime            = 0:00:02.07\n",
      "  eval_samples_per_second =     48.183\n",
      "  eval_steps_per_second   =     24.092\n",
      "  num_input_tokens_seen   =     631296\n",
      "[INFO|modelcard.py:449] 2024-07-14 13:04:13,378 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type full \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-06 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/full/train_epoch=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e2e1734-8df5-4e92-876c-e3f806dc83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-14 13:04:28,978] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/14/2024 13:04:33 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:04:33,674 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-14 13:04:33,968 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/14/2024 13:04:33 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/14/2024 13:04:33 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/14/2024 13:04:39 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-14 13:04:40,794 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-14 13:04:40,796 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-14 13:04:40,839 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-14 13:04:41,304 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:04:41,308 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-14 13:04:56,760 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-14 13:04:56,761 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-14 13:04:56,768 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:04:56,769 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/14/2024 13:04:56 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/14/2024 13:04:56 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/14/2024 13:04:56 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/14/2024 13:04:56 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\n",
      "07/14/2024 13:04:56 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-14 13:04:56,930 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-14 13:04:57,254 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-14 13:04:57,254 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-14 13:04:57,255 >>   Num Epochs = 5\n",
      "[INFO|trainer.py:2131] 2024-07-14 13:04:57,255 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-14 13:04:57,255 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-14 13:04:57,255 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-14 13:04:57,255 >>   Total optimization steps = 280\n",
      "[INFO|trainer.py:2137] 2024-07-14 13:04:57,256 >>   Number of trainable parameters = 494,032,768\n",
      "{'loss': 1.9121, 'grad_norm': 29.55918312072754, 'learning_rate': 9.997167791667667e-07, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5555, 'grad_norm': 17.882728576660156, 'learning_rate': 9.979871469976195e-07, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  4%|█▌                                        | 10/280 [00:19<05:51,  1.30s/it][INFO|trainer.py:3788] 2024-07-14 13:05:16,729 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:05:16,729 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:05:16,729 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.72it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.30it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 24.85it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.52it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.37it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.13it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.58it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 25.05it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 27.06it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 28.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 29.51it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.02it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 29.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5550532341003418, 'eval_runtime': 1.8982, 'eval_samples_per_second': 52.682, 'eval_steps_per_second': 26.341, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  4%|█▌                                        | 10/280 [00:21<05:51,  1.30s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.11it/s]\u001b[A\n",
      "{'loss': 1.6195, 'grad_norm': 14.618163108825684, 'learning_rate': 9.946906630265184e-07, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.4338, 'grad_norm': 15.675799369812012, 'learning_rate': 9.898376992116177e-07, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      "  7%|███                                       | 20/280 [00:32<04:58,  1.15s/it][INFO|trainer.py:3788] 2024-07-14 13:05:29,572 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:05:29,572 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:05:29,572 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.52it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.64it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.92it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.37it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.29it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.97it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.31it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.47it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.52it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.47it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.50it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.41it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 24.44it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.61it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4679638147354126, 'eval_runtime': 2.0696, 'eval_samples_per_second': 48.319, 'eval_steps_per_second': 24.16, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      "  7%|███                                       | 20/280 [00:34<04:58,  1.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 26.26it/s]\u001b[A\n",
      "{'loss': 1.2963, 'grad_norm': 13.149949073791504, 'learning_rate': 9.83443524772503e-07, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3484, 'grad_norm': 13.18998908996582, 'learning_rate': 9.755282581475767e-07, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 11%|████▌                                     | 30/280 [00:45<04:37,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:05:42,428 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:05:42,428 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:05:42,428 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.27it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.28it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.97it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.54it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.38it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.96it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.79it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.67it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4473263025283813, 'eval_runtime': 1.6315, 'eval_samples_per_second': 61.294, 'eval_steps_per_second': 30.647, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 11%|████▌                                     | 30/280 [00:46<04:37,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.37it/s]\u001b[A\n",
      "{'loss': 1.584, 'grad_norm': 13.845182418823242, 'learning_rate': 9.661168036940071e-07, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.2143, 'grad_norm': 11.830839157104492, 'learning_rate': 9.552387733294078e-07, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 14%|██████                                    | 40/280 [00:57<04:19,  1.08s/it][INFO|trainer.py:3788] 2024-07-14 13:05:54,895 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:05:54,895 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:05:54,895 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 35.94it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.10it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.61it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.86it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.47it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.31it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.90it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.68it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.43560791015625, 'eval_runtime': 1.6367, 'eval_samples_per_second': 61.098, 'eval_steps_per_second': 30.549, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 14%|██████                                    | 40/280 [00:59<04:19,  1.08s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.38it/s]\u001b[A\n",
      "{'loss': 1.1821, 'grad_norm': 14.762059211730957, 'learning_rate': 9.429283933617899e-07, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.4288, 'grad_norm': 11.905611991882324, 'learning_rate': 9.29224396800933e-07, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 18%|███████▌                                  | 50/280 [01:10<04:23,  1.15s/it][INFO|trainer.py:3788] 2024-07-14 13:06:07,623 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:06:07,623 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:06:07,623 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.61it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.31it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.39it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.45it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.85it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.49it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.23it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.69it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.65it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4276310205459595, 'eval_runtime': 1.6315, 'eval_samples_per_second': 61.293, 'eval_steps_per_second': 30.647, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 18%|███████▌                                  | 50/280 [01:11<04:23,  1.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.29it/s]\u001b[A\n",
      "{'loss': 1.5116, 'grad_norm': 14.211236953735352, 'learning_rate': 9.141699014900082e-07, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.4642, 'grad_norm': 11.942851066589355, 'learning_rate': 8.978122744408905e-07, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 21%|█████████                                 | 60/280 [01:23<04:09,  1.13s/it][INFO|trainer.py:3788] 2024-07-14 13:06:20,409 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:06:20,409 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:06:20,409 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.17it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.12it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.26it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.30it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.72it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.86it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.39it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.21it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.49it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.29it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4220147132873535, 'eval_runtime': 1.6493, 'eval_samples_per_second': 60.633, 'eval_steps_per_second': 30.316, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 21%|█████████                                 | 60/280 [01:24<04:09,  1.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.41it/s]\u001b[A\n",
      "{'loss': 1.4255, 'grad_norm': 13.284761428833008, 'learning_rate': 8.802029828000155e-07, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.3443, 'grad_norm': 12.917094230651855, 'learning_rate': 8.613974319136957e-07, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 25%|██████████▌                               | 70/280 [01:37<04:15,  1.22s/it][INFO|trainer.py:3788] 2024-07-14 13:06:34,761 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:06:34,761 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:06:34,761 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.96it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.11it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.20it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.68it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.41it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.18it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.61it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.02it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.06it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 22.96it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 22.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 22.86it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.69it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.06it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.421665906906128, 'eval_runtime': 2.0951, 'eval_samples_per_second': 47.73, 'eval_steps_per_second': 23.865, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 25%|██████████▌                               | 70/280 [01:39<04:15,  1.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 26.47it/s]\u001b[A\n",
      "{'loss': 1.4511, 'grad_norm': 11.681509971618652, 'learning_rate': 8.414547910024035e-07, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.3146, 'grad_norm': 13.974505424499512, 'learning_rate': 8.20437806992512e-07, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 29%|████████████                              | 80/280 [01:50<03:44,  1.12s/it][INFO|trainer.py:3788] 2024-07-14 13:06:47,928 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:06:47,928 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:06:47,928 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.38it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.05it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.01it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 30.81it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 30.63it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 29.84it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.58it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.56it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.32it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4209712743759155, 'eval_runtime': 1.6593, 'eval_samples_per_second': 60.267, 'eval_steps_per_second': 30.134, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 29%|████████████                              | 80/280 [01:52<03:44,  1.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.98it/s]\u001b[A\n",
      "{'loss': 1.3008, 'grad_norm': 11.139114379882812, 'learning_rate': 7.984126070912518e-07, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.4143, 'grad_norm': 12.519010543823242, 'learning_rate': 7.754484907260512e-07, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 32%|█████████████▌                            | 90/280 [02:04<03:44,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:07:01,492 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:07:01,492 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:07:01,492 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.05it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.10it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.16it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.53it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.69it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.67it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.37it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4199835062026978, 'eval_runtime': 1.6524, 'eval_samples_per_second': 60.517, 'eval_steps_per_second': 30.259, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 32%|█████████████▌                            | 90/280 [02:05<03:44,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.97it/s]\u001b[A\n",
      "{'loss': 1.2311, 'grad_norm': 9.008787155151367, 'learning_rate': 7.516177115029001e-07, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.2772, 'grad_norm': 10.506690979003906, 'learning_rate': 7.269952498697734e-07, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 36%|██████████████▋                          | 100/280 [02:17<03:27,  1.15s/it][INFO|trainer.py:3788] 2024-07-14 13:07:14,349 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:07:14,349 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:07:14,349 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.31it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.34it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.01it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.14it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 26.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 27.72it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:00<00:00, 29.61it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 30.33it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 30.68it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 31.23it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 31.15it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4185906648635864, 'eval_runtime': 1.7724, 'eval_samples_per_second': 56.419, 'eval_steps_per_second': 28.21, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 36%|██████████████▋                          | 100/280 [02:18<03:27,  1.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.40it/s]\u001b[A\n",
      "{'loss': 1.2279, 'grad_norm': 11.94983196258545, 'learning_rate': 7.016585772004026e-07, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.5779, 'grad_norm': 12.310819625854492, 'learning_rate': 6.756874120406714e-07, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 39%|████████████████                         | 110/280 [02:30<03:12,  1.13s/it][INFO|trainer.py:3788] 2024-07-14 13:07:27,280 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:07:27,280 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:07:27,280 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 35.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.78it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.22it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.65it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.81it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.43it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.18it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.58it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.421880841255188, 'eval_runtime': 1.6385, 'eval_samples_per_second': 61.033, 'eval_steps_per_second': 30.517, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 39%|████████████████                         | 110/280 [02:31<03:12,  1.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.21it/s]\u001b[A\n",
      "{'loss': 1.3546, 'grad_norm': 11.20162296295166, 'learning_rate': 6.49163469284578e-07, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.3172, 'grad_norm': 9.034092903137207, 'learning_rate': 6.22170203068947e-07, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 43%|█████████████████▌                       | 120/280 [02:43<03:06,  1.16s/it][INFO|trainer.py:3788] 2024-07-14 13:07:40,337 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:07:40,338 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:07:40,338 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.22it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.66it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.27it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.96it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 26.77it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 27.57it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 29.24it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 30.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 30.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 30.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.12it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4216206073760986, 'eval_runtime': 1.7842, 'eval_samples_per_second': 56.047, 'eval_steps_per_second': 28.024, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 43%|█████████████████▌                       | 120/280 [02:44<03:06,  1.16s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.96it/s]\u001b[A\n",
      "{'loss': 1.0891, 'grad_norm': 11.045059204101562, 'learning_rate': 5.947925441958393e-07, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.3467, 'grad_norm': 10.885007858276367, 'learning_rate': 5.671166329088277e-07, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 46%|███████████████████                      | 130/280 [02:55<02:45,  1.10s/it][INFO|trainer.py:3788] 2024-07-14 13:07:53,117 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:07:53,117 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:07:53,117 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.17it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.26it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.34it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.70it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.83it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.42it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.20it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.56it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.421879768371582, 'eval_runtime': 1.6366, 'eval_samples_per_second': 61.104, 'eval_steps_per_second': 30.552, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 46%|███████████████████                      | 130/280 [02:57<02:45,  1.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.22it/s]\u001b[A\n",
      "{'loss': 1.3624, 'grad_norm': 11.73864459991455, 'learning_rate': 5.392295478639225e-07, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.3229, 'grad_norm': 11.057682037353516, 'learning_rate': 5.112190321479025e-07, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 50%|████████████████████▌                    | 140/280 [03:08<02:35,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:08:05,622 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:08:05,623 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:08:05,623 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.52it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.18it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.26it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.34it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.73it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.89it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.42it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.47it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.37it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4228394031524658, 'eval_runtime': 1.6394, 'eval_samples_per_second': 60.997, 'eval_steps_per_second': 30.499, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 50%|████████████████████▌                    | 140/280 [03:10<02:35,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.10it/s]\u001b[A\n",
      "{'loss': 1.158, 'grad_norm': 10.884917259216309, 'learning_rate': 4.831732172061032e-07, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.6028, 'grad_norm': 11.559328079223633, 'learning_rate': 4.5518034554828327e-07, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 54%|█████████████████████▉                   | 150/280 [03:21<02:33,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:08:18,741 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:08:18,741 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:08:18,741 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.10it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 24.71it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.46it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.45it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.19it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.58it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.03it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.07it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.01it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.00it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 22.93it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 22.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 22.52it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 22.39it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4219326972961426, 'eval_runtime': 2.2038, 'eval_samples_per_second': 45.377, 'eval_steps_per_second': 22.688, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 54%|█████████████████████▉                   | 150/280 [03:23<02:33,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 22.69it/s]\u001b[A\n",
      "{'loss': 1.4228, 'grad_norm': 11.223444938659668, 'learning_rate': 4.273284931050438e-07, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 1.0498, 'grad_norm': 9.096519470214844, 'learning_rate': 3.997052921083636e-07, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 57%|███████████████████████▍                 | 160/280 [03:34<02:06,  1.06s/it][INFO|trainer.py:3788] 2024-07-14 13:08:31,563 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:08:31,563 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:08:31,563 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 34.90it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.67it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.97it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.09it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.39it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.39it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.98it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.71it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.37it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.30it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4205982685089111, 'eval_runtime': 1.6564, 'eval_samples_per_second': 60.373, 'eval_steps_per_second': 30.187, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 57%|███████████████████████▍                 | 160/280 [03:35<02:06,  1.06s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.00it/s]\u001b[A\n",
      "{'loss': 1.177, 'grad_norm': 12.579253196716309, 'learning_rate': 3.723976553681787e-07, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "{'loss': 1.4052, 'grad_norm': 13.382055282592773, 'learning_rate': 3.454915028125263e-07, 'epoch': 3.02, 'num_input_tokens_seen': 960784}\n",
      " 61%|████████████████████████▉                | 170/280 [03:47<02:02,  1.12s/it][INFO|trainer.py:3788] 2024-07-14 13:08:44,278 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:08:44,279 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:08:44,279 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.84it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.88it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.42it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.08it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.94it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 25.02it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 27.72it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 29.03it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 30.50it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 30.59it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 30.86it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4203369617462158, 'eval_runtime': 1.8293, 'eval_samples_per_second': 54.666, 'eval_steps_per_second': 27.333, 'epoch': 3.02, 'num_input_tokens_seen': 960784}\n",
      " 61%|████████████████████████▉                | 170/280 [03:48<02:02,  1.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.27it/s]\u001b[A\n",
      "{'loss': 1.4053, 'grad_norm': 15.723217964172363, 'learning_rate': 3.1907149115166397e-07, 'epoch': 3.11, 'num_input_tokens_seen': 993408}\n",
      "{'loss': 1.049, 'grad_norm': 11.196394920349121, 'learning_rate': 2.9322074751673974e-07, 'epoch': 3.2, 'num_input_tokens_seen': 1017488}\n",
      " 64%|██████████████████████████▎              | 180/280 [04:00<02:06,  1.27s/it][INFO|trainer.py:3788] 2024-07-14 13:08:58,260 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:08:58,260 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:08:58,260 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.46it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.20it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.29it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.33it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.74it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.88it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.48it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.29it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.66it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.421868085861206, 'eval_runtime': 1.6353, 'eval_samples_per_second': 61.151, 'eval_steps_per_second': 30.575, 'epoch': 3.2, 'num_input_tokens_seen': 1017488}\n",
      " 64%|██████████████████████████▎              | 180/280 [04:02<02:06,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.26it/s]\u001b[A\n",
      "{'loss': 1.2845, 'grad_norm': 12.116119384765625, 'learning_rate': 2.68020607911083e-07, 'epoch': 3.29, 'num_input_tokens_seen': 1044544}\n",
      "{'loss': 1.377, 'grad_norm': 13.000490188598633, 'learning_rate': 2.4355036129704696e-07, 'epoch': 3.38, 'num_input_tokens_seen': 1073920}\n",
      " 68%|███████████████████████████▊             | 190/280 [04:15<01:50,  1.22s/it][INFO|trainer.py:3788] 2024-07-14 13:09:12,459 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:09:12,459 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:09:12,459 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.92it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 25.49it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 25.04it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 23.55it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 23.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 23.07it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 24.79it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 27.61it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 28.94it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 29.39it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 30.32it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 30.27it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 30.54it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4218125343322754, 'eval_runtime': 1.8536, 'eval_samples_per_second': 53.949, 'eval_steps_per_second': 26.975, 'epoch': 3.38, 'num_input_tokens_seen': 1073920}\n",
      " 68%|███████████████████████████▊             | 190/280 [04:17<01:50,  1.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.22it/s]\u001b[A\n",
      "{'loss': 1.0425, 'grad_norm': 10.146559715270996, 'learning_rate': 2.198870001235986e-07, 'epoch': 3.47, 'num_input_tokens_seen': 1100016}\n",
      "{'loss': 1.5385, 'grad_norm': 14.567554473876953, 'learning_rate': 1.971049780795901e-07, 'epoch': 3.56, 'num_input_tokens_seen': 1132672}\n",
      " 71%|█████████████████████████████▎           | 200/280 [04:28<01:31,  1.14s/it][INFO|trainer.py:3788] 2024-07-14 13:09:25,399 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:09:25,399 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:09:25,399 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.34it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.04it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.07it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.14it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.45it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.21it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.93it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.60it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.00it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4227904081344604, 'eval_runtime': 1.6503, 'eval_samples_per_second': 60.596, 'eval_steps_per_second': 30.298, 'epoch': 3.56, 'num_input_tokens_seen': 1132672}\n",
      " 71%|█████████████████████████████▎           | 200/280 [04:29<01:31,  1.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.86it/s]\u001b[A\n",
      "{'loss': 1.1027, 'grad_norm': 9.002562522888184, 'learning_rate': 1.7527597583490823e-07, 'epoch': 3.64, 'num_input_tokens_seen': 1159024}\n",
      "{'loss': 1.0709, 'grad_norm': 11.663177490234375, 'learning_rate': 1.5446867550656767e-07, 'epoch': 3.73, 'num_input_tokens_seen': 1185088}\n",
      " 75%|██████████████████████████████▊          | 210/280 [04:40<01:18,  1.12s/it][INFO|trainer.py:3788] 2024-07-14 13:09:37,908 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:09:37,908 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:09:37,908 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.20it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.10it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.13it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.47it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.66it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.17it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.97it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.58it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4229791164398193, 'eval_runtime': 1.6463, 'eval_samples_per_second': 60.742, 'eval_steps_per_second': 30.371, 'epoch': 3.73, 'num_input_tokens_seen': 1185088}\n",
      " 75%|██████████████████████████████▊          | 210/280 [04:42<01:18,  1.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.01it/s]\u001b[A\n",
      "{'loss': 1.2339, 'grad_norm': 11.633342742919922, 'learning_rate': 1.3474854455936123e-07, 'epoch': 3.82, 'num_input_tokens_seen': 1215392}\n",
      "{'loss': 1.2398, 'grad_norm': 11.27013874053955, 'learning_rate': 1.1617762982099444e-07, 'epoch': 3.91, 'num_input_tokens_seen': 1243904}\n",
      " 79%|████████████████████████████████▏        | 220/280 [04:53<01:08,  1.15s/it][INFO|trainer.py:3788] 2024-07-14 13:09:50,764 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:09:50,764 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:09:50,764 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.18it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.13it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.15it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.53it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.73it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.26it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 30.99it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4233832359313965, 'eval_runtime': 1.6515, 'eval_samples_per_second': 60.551, 'eval_steps_per_second': 30.275, 'epoch': 3.91, 'num_input_tokens_seen': 1243904}\n",
      " 79%|████████████████████████████████▏        | 220/280 [04:55<01:08,  1.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.83it/s]\u001b[A\n",
      "{'loss': 1.3666, 'grad_norm': 12.678227424621582, 'learning_rate': 9.881436225981104e-08, 'epoch': 4.0, 'num_input_tokens_seen': 1274304}\n",
      "{'loss': 1.1241, 'grad_norm': 10.40957260131836, 'learning_rate': 8.271337313934867e-08, 'epoch': 4.09, 'num_input_tokens_seen': 1301088}\n",
      " 82%|█████████████████████████████████▋       | 230/280 [05:06<00:55,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:10:03,514 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:10:03,515 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:10:03,515 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.24it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.08it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.14it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.68it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.25it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.59it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.42it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.24it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4233039617538452, 'eval_runtime': 1.6472, 'eval_samples_per_second': 60.709, 'eval_steps_per_second': 30.355, 'epoch': 4.09, 'num_input_tokens_seen': 1301088}\n",
      " 82%|█████████████████████████████████▋       | 230/280 [05:07<00:55,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.97it/s]\u001b[A\n",
      "{'loss': 1.1196, 'grad_norm': 9.82083797454834, 'learning_rate': 6.79253221281727e-08, 'epoch': 4.18, 'num_input_tokens_seen': 1328048}\n",
      "{'loss': 1.3645, 'grad_norm': 11.904823303222656, 'learning_rate': 5.44967379058161e-08, 'epoch': 4.27, 'num_input_tokens_seen': 1359856}\n",
      " 86%|███████████████████████████████████▏     | 240/280 [05:19<00:49,  1.23s/it][INFO|trainer.py:3788] 2024-07-14 13:10:16,939 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:10:16,939 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:10:16,939 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.86it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.20it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.43it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.07it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.79it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 24.02it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 26.32it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 27.90it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 29.49it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 29.73it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 30.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.423610806465149, 'eval_runtime': 1.8868, 'eval_samples_per_second': 53.001, 'eval_steps_per_second': 26.5, 'epoch': 4.27, 'num_input_tokens_seen': 1359856}\n",
      " 86%|███████████████████████████████████▏     | 240/280 [05:21<00:49,  1.23s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.10it/s]\u001b[A\n",
      "{'loss': 1.242, 'grad_norm': 13.843372344970703, 'learning_rate': 4.246987176634009e-08, 'epoch': 4.36, 'num_input_tokens_seen': 1387328}\n",
      "{'loss': 1.1197, 'grad_norm': 11.26338005065918, 'learning_rate': 3.188256468013139e-08, 'epoch': 4.44, 'num_input_tokens_seen': 1413728}\n",
      " 89%|████████████████████████████████████▌    | 250/280 [05:32<00:34,  1.14s/it][INFO|trainer.py:3788] 2024-07-14 13:10:29,752 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:10:29,752 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:10:29,752 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.60it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 24.76it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 24.47it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.85it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 22.75it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 22.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 22.52it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.70it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 22.60it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 22.71it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 22.75it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 23.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 24.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 26.56it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 27.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4237936735153198, 'eval_runtime': 2.0925, 'eval_samples_per_second': 47.791, 'eval_steps_per_second': 23.895, 'epoch': 4.44, 'num_input_tokens_seen': 1413728}\n",
      " 89%|████████████████████████████████████▌    | 250/280 [05:34<00:34,  1.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 27.34it/s]\u001b[A\n",
      "{'loss': 1.1512, 'grad_norm': 12.883172988891602, 'learning_rate': 2.276812823220964e-08, 'epoch': 4.53, 'num_input_tokens_seen': 1442064}\n",
      "{'loss': 1.1694, 'grad_norm': 10.507081031799316, 'learning_rate': 1.5155239811656562e-08, 'epoch': 4.62, 'num_input_tokens_seen': 1469424}\n",
      " 93%|██████████████████████████████████████   | 260/280 [05:45<00:22,  1.12s/it][INFO|trainer.py:3788] 2024-07-14 13:10:42,771 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:10:42,771 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:10:42,771 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.29it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.02it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.09it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.10it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.52it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.74it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.26it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.06it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.68it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.48it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.26it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4239543676376343, 'eval_runtime': 1.6443, 'eval_samples_per_second': 60.814, 'eval_steps_per_second': 30.407, 'epoch': 4.62, 'num_input_tokens_seen': 1469424}\n",
      " 93%|██████████████████████████████████████   | 260/280 [05:47<00:22,  1.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.00it/s]\u001b[A\n",
      "{'loss': 1.0903, 'grad_norm': 12.445756912231445, 'learning_rate': 9.067852381940799e-09, 'epoch': 4.71, 'num_input_tokens_seen': 1496944}\n",
      "{'loss': 1.2443, 'grad_norm': 13.848002433776855, 'learning_rate': 4.5251191160326495e-09, 'epoch': 4.8, 'num_input_tokens_seen': 1525456}\n",
      " 96%|███████████████████████████████████████▌ | 270/280 [05:58<00:11,  1.13s/it][INFO|trainer.py:3788] 2024-07-14 13:10:55,443 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:10:55,444 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:10:55,444 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.18it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.86it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.95it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.48it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.69it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.23it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.00it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.19it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4239219427108765, 'eval_runtime': 1.649, 'eval_samples_per_second': 60.644, 'eval_steps_per_second': 30.322, 'epoch': 4.8, 'num_input_tokens_seen': 1525456}\n",
      " 96%|███████████████████████████████████████▌ | 270/280 [05:59<00:11,  1.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.96it/s]\u001b[A\n",
      "{'loss': 1.3058, 'grad_norm': 10.99982738494873, 'learning_rate': 1.541333133436018e-09, 'epoch': 4.89, 'num_input_tokens_seen': 1554592}\n",
      "{'loss': 1.3562, 'grad_norm': 11.139357566833496, 'learning_rate': 1.2588252874673466e-10, 'epoch': 4.98, 'num_input_tokens_seen': 1582160}\n",
      "100%|█████████████████████████████████████████| 280/280 [06:12<00:00,  1.21s/it][INFO|trainer.py:3788] 2024-07-14 13:11:09,606 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:11:09,607 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:11:09,607 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.33it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.06it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.87it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 30.85it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.28it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.56it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.09it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.91it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.56it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.41it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.32it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4239602088928223, 'eval_runtime': 1.6492, 'eval_samples_per_second': 60.637, 'eval_steps_per_second': 30.319, 'epoch': 4.98, 'num_input_tokens_seen': 1582160}\n",
      "100%|█████████████████████████████████████████| 280/280 [06:13<00:00,  1.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.07it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3478] 2024-07-14 13:11:11,260 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:11:11,275 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:11:11,285 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:11:17,041 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:11:17,054 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:11:17,064 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/checkpoint-280/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-14 13:11:28,412 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 391.1563, 'train_samples_per_second': 11.504, 'train_steps_per_second': 0.716, 'train_loss': 1.3164633699825832, 'epoch': 4.98, 'num_input_tokens_seen': 1582160}\n",
      "100%|█████████████████████████████████████████| 280/280 [06:31<00:00,  1.40s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-14 13:11:28,425 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_epoch=5\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:11:28,439 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:11:28,450 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:11:34,866 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:11:34,890 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:11:34,900 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_epoch=5/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     4.9778\n",
      "  num_input_tokens_seen    =    1582160\n",
      "  total_flos               =  3164180GF\n",
      "  train_loss               =     1.3165\n",
      "  train_runtime            = 0:06:31.15\n",
      "  train_samples_per_second =     11.504\n",
      "  train_steps_per_second   =      0.716\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_epoch=5/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_epoch=5/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-14 13:11:35,687 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:11:35,687 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:11:35,687 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.44it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     4.9778\n",
      "  eval_loss               =      1.424\n",
      "  eval_runtime            = 0:00:02.19\n",
      "  eval_samples_per_second =     45.629\n",
      "  eval_steps_per_second   =     22.814\n",
      "  num_input_tokens_seen   =    1582160\n",
      "[INFO|modelcard.py:449] 2024-07-14 13:11:37,967 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type full \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 1e-06 \\\n",
    "    --num_train_epochs 5.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/full/train_epoch=5 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6d3cbe1-d752-4b16-9676-e5c6247562a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-14 13:14:15,824] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/14/2024 13:14:20 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-14 13:14:20,449 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-14 13:14:20,727 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/14/2024 13:14:20 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/14/2024 13:14:20 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/14/2024 13:14:26 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-14 13:14:27,981 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-14 13:14:27,983 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-14 13:14:28,018 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-14 13:14:28,418 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:14:28,423 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-14 13:14:46,353 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-14 13:14:46,353 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-14 13:14:46,360 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-14 13:14:46,361 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/14/2024 13:14:46 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/14/2024 13:14:46 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/14/2024 13:14:46 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/14/2024 13:14:46 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\n",
      "07/14/2024 13:14:46 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "[INFO|trainer.py:642] 2024-07-14 13:14:46,522 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-14 13:14:46,827 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-14 13:14:46,827 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-14 13:14:46,827 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-14 13:14:46,827 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-14 13:14:46,828 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-14 13:14:46,828 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-14 13:14:46,828 >>   Total optimization steps = 168\n",
      "[INFO|trainer.py:2137] 2024-07-14 13:14:46,829 >>   Number of trainable parameters = 494,032,768\n",
      "{'loss': 1.9679, 'grad_norm': 46.93043518066406, 'learning_rate': 4.996067037544542e-07, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.6606, 'grad_norm': 23.186885833740234, 'learning_rate': 4.972077065562821e-07, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:17<03:05,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:15:04,321 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:15:04,321 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:15:04,321 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.39it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.44it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.60it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.79it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.87it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.45it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.87it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.47it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.28it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6444923877716064, 'eval_runtime': 1.6413, 'eval_samples_per_second': 60.927, 'eval_steps_per_second': 30.463, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:19<03:05,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.05it/s]\u001b[A\n",
      "{'loss': 1.7024, 'grad_norm': 17.16386604309082, 'learning_rate': 4.926491418633478e-07, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.5117, 'grad_norm': 15.671197891235352, 'learning_rate': 4.859708325770919e-07, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [00:30<02:54,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:15:17,106 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:15:17,106 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:15:17,106 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.51it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 25.08it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 22.58it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.54it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 23.62it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 24.28it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 24.26it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 24.01it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 23.40it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 24.27it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 26.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 27.99it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 28.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 28.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.532676100730896, 'eval_runtime': 1.9695, 'eval_samples_per_second': 50.774, 'eval_steps_per_second': 25.387, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [00:32<02:54,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 28.68it/s]\u001b[A\n",
      "{'loss': 1.3528, 'grad_norm': 14.560193061828613, 'learning_rate': 4.772311193530527e-07, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3829, 'grad_norm': 13.112587928771973, 'learning_rate': 4.6650635094610966e-07, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [00:45<03:05,  1.34s/it][INFO|trainer.py:3788] 2024-07-14 13:15:32,036 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:15:32,037 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:15:32,037 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 35.97it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.73it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 30.62it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 30.75it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.19it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.49it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.19it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.00it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.31it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.08it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.07it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4827728271484375, 'eval_runtime': 1.659, 'eval_samples_per_second': 60.276, 'eval_steps_per_second': 30.138, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [00:46<03:05,  1.34s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.89it/s]\u001b[A\n",
      "{'loss': 1.6212, 'grad_norm': 13.935051918029785, 'learning_rate': 4.5389021723981503e-07, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.2385, 'grad_norm': 12.14964771270752, 'learning_rate': 4.394929307863632e-07, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [00:58<02:21,  1.10s/it][INFO|trainer.py:3788] 2024-07-14 13:15:44,985 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:15:44,985 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:15:44,985 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.27it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.16it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.17it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.27it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.67it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.82it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.40it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.77it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.61it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 30.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4636282920837402, 'eval_runtime': 1.6508, 'eval_samples_per_second': 60.576, 'eval_steps_per_second': 30.288, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [00:59<02:21,  1.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.71it/s]\u001b[A\n",
      "{'loss': 1.1995, 'grad_norm': 15.870516777038574, 'learning_rate': 4.234402640071354e-07, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.4565, 'grad_norm': 12.330008506774902, 'learning_rate': 4.058724504646834e-07, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [01:11<02:22,  1.21s/it][INFO|trainer.py:3788] 2024-07-14 13:15:58,367 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:15:58,368 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:15:58,368 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.35it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 23.79it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 23.33it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 21.52it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 20.96it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 21.51it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 21.91it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 22.30it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 22.64it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 22.82it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 22.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 22.61it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 25.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.453061580657959, 'eval_runtime': 2.1956, 'eval_samples_per_second': 45.545, 'eval_steps_per_second': 22.772, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [01:13<02:22,  1.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 25.68it/s]\u001b[A\n",
      "{'loss': 1.5293, 'grad_norm': 14.569416999816895, 'learning_rate': 3.8694295980446783e-07, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.5145, 'grad_norm': 14.106030464172363, 'learning_rate': 3.668171570682655e-07, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [01:25<02:09,  1.20s/it][INFO|trainer.py:3788] 2024-07-14 13:16:12,242 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:16:12,242 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:16:12,242 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.53it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.33it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.81it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.37it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.90it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.75it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.18it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.06it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.33it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.45it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 24.84it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 27.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4421751499176025, 'eval_runtime': 2.0404, 'eval_samples_per_second': 49.009, 'eval_steps_per_second': 24.504, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [01:27<02:09,  1.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 26.53it/s]\u001b[A\n",
      "{'loss': 1.481, 'grad_norm': 14.679883003234863, 'learning_rate': 3.4567085809127245e-07, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.4017, 'grad_norm': 13.921343803405762, 'learning_rate': 3.2368879360272606e-07, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [01:38<01:48,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:16:25,220 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:16:25,221 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:16:25,221 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.52it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.38it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.52it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.58it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.90it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.03it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.64it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 32.48it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.88it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.58it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.435279130935669, 'eval_runtime': 1.6292, 'eval_samples_per_second': 61.38, 'eval_steps_per_second': 30.69, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [01:40<01:48,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      "{'loss': 1.5116, 'grad_norm': 13.495931625366211, 'learning_rate': 3.010629954474201e-07, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.3646, 'grad_norm': 15.004172325134277, 'learning_rate': 2.7799111902582693e-07, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [01:51<01:50,  1.26s/it][INFO|trainer.py:3788] 2024-07-14 13:16:38,519 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:16:38,519 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:16:38,519 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 35.91it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.97it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.14it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.36it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.96it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.47it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.36it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.97it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.77it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.64it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4324368238449097, 'eval_runtime': 1.6322, 'eval_samples_per_second': 61.266, 'eval_steps_per_second': 30.633, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [01:53<01:50,  1.26s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.28it/s]\u001b[A\n",
      "{'loss': 1.347, 'grad_norm': 12.00020694732666, 'learning_rate': 2.5467471660772554e-07, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.4666, 'grad_norm': 13.176822662353516, 'learning_rate': 2.3131747660339394e-07, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [02:04<01:33,  1.20s/it][INFO|trainer.py:3788] 2024-07-14 13:16:51,504 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:16:51,504 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:16:51,504 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.74it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.37it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.51it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.59it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.83it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.63it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 30.55it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 31.47it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.34it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4316641092300415, 'eval_runtime': 1.647, 'eval_samples_per_second': 60.715, 'eval_steps_per_second': 30.357, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [02:06<01:33,  1.20s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 29.98it/s]\u001b[A\n",
      "{'loss': 1.2762, 'grad_norm': 9.513505935668945, 'learning_rate': 2.081234441738159e-07, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.3355, 'grad_norm': 12.04301929473877, 'learning_rate': 1.8529523872436977e-07, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [02:17<01:15,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:17:04,037 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:17:04,037 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:17:04,037 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.63it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.32it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.44it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.53it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.98it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.63it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 32.43it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.96it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.81it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.429413914680481, 'eval_runtime': 1.6247, 'eval_samples_per_second': 61.55, 'eval_steps_per_second': 30.775, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [02:18<01:15,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.39it/s]\u001b[A\n",
      "{'loss': 1.2752, 'grad_norm': 12.612893104553223, 'learning_rate': 1.6303228385369316e-07, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.6347, 'grad_norm': 12.692626953125, 'learning_rate': 1.4152906522061047e-07, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [02:30<01:05,  1.14s/it][INFO|trainer.py:3788] 2024-07-14 13:17:16,863 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:17:16,863 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:17:16,863 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.49it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 31.96it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.14it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 30.48it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.21it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.60it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.25it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 32.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 31.81it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.74it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4292619228363037, 'eval_runtime': 1.6385, 'eval_samples_per_second': 61.032, 'eval_steps_per_second': 30.516, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [02:31<01:05,  1.14s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.38it/s]\u001b[A\n",
      "{'loss': 1.4268, 'grad_norm': 11.829496383666992, 'learning_rate': 1.2097343154812331e-07, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.409, 'grad_norm': 10.137336730957031, 'learning_rate': 1.0154495360662463e-07, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [02:43<01:01,  1.28s/it][INFO|trainer.py:3788] 2024-07-14 13:17:30,737 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:17:30,737 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:17:30,737 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.48it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.05it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.45it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.83it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 30.97it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.59it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 32.42it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 32.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.81it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4291287660598755, 'eval_runtime': 1.6338, 'eval_samples_per_second': 61.206, 'eval_steps_per_second': 30.603, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [02:45<01:01,  1.28s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.25it/s]\u001b[A\n",
      "{'loss': 1.1862, 'grad_norm': 11.562135696411133, 'learning_rate': 8.341335551199902e-08, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.4399, 'grad_norm': 11.45812702178955, 'learning_rate': 6.673703204254347e-08, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [02:56<00:42,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:17:43,618 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:17:43,618 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:17:43,618 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.60it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 32.34it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 31.48it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 31.56it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:00, 31.98it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:00, 31.07it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:00<00:00, 31.66it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:00<00:00, 32.47it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 32.02it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 31.81it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 31.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4288479089736938, 'eval_runtime': 1.6258, 'eval_samples_per_second': 61.508, 'eval_steps_per_second': 30.754, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [02:58<00:42,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 30.29it/s]\u001b[A\n",
      "{'loss': 1.4501, 'grad_norm': 12.499624252319336, 'learning_rate': 5.166166492719124e-08, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.4149, 'grad_norm': 11.417293548583984, 'learning_rate': 3.831895019292897e-08, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [03:09<00:31,  1.11s/it][INFO|trainer.py:3788] 2024-07-14 13:17:56,129 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:17:56,129 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:17:56,129 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 36.48it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 30.98it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 28.29it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 26.40it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 25.63it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 24.61it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:01, 24.00it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 23.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 23.73it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.41it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 23.36it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 25.57it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.428594708442688, 'eval_runtime': 2.0146, 'eval_samples_per_second': 49.637, 'eval_steps_per_second': 24.818, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [03:11<00:31,  1.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 26.00it/s]\u001b[A\n",
      "{'loss': 1.251, 'grad_norm': 11.246050834655762, 'learning_rate': 2.682544768909717e-08, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.7248, 'grad_norm': 12.589554786682129, 'learning_rate': 1.7281562838948966e-08, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [03:22<00:21,  1.18s/it][INFO|trainer.py:3788] 2024-07-14 13:18:09,412 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:18:09,412 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:18:09,412 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.63it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.08it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.71it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 23.67it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.40it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.80it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.19it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.22it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.17it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.22it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 24.44it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 26.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.428450345993042, 'eval_runtime': 2.0656, 'eval_samples_per_second': 48.411, 'eval_steps_per_second': 24.205, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [03:24<00:21,  1.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 26.25it/s]\u001b[A\n",
      "{'loss': 1.5324, 'grad_norm': 11.969194412231445, 'learning_rate': 9.770669513725127e-09, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 1.1238, 'grad_norm': 9.871203422546387, 'learning_rate': 4.358381691677931e-09, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [03:36<00:09,  1.13s/it][INFO|trainer.py:3788] 2024-07-14 13:18:22,890 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:18:22,891 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:18:22,891 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 33.32it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.45it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.92it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.48it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.44it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.15it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.48it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.94it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.98it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.92it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.83it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.66it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 23.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.428290843963623, 'eval_runtime': 2.1016, 'eval_samples_per_second': 47.582, 'eval_steps_per_second': 23.791, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [03:38<00:09,  1.13s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.31it/s]\u001b[A\n",
      "{'loss': 1.2683, 'grad_norm': 15.175515174865723, 'learning_rate': 1.0919802647165465e-09, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "100%|█████████████████████████████████████████| 168/168 [03:47<00:00,  1.20s/it][INFO|trainer.py:3478] 2024-07-14 13:18:34,178 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:18:34,194 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:18:34,204 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:18:39,027 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:18:39,041 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:18:39,050 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/checkpoint-168/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-14 13:18:50,058 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 243.2296, 'train_samples_per_second': 11.101, 'train_steps_per_second': 0.691, 'train_loss': 1.444029572464171, 'epoch': 2.99, 'num_input_tokens_seen': 950512}\n",
      "100%|█████████████████████████████████████████| 168/168 [04:03<00:00,  1.45s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-14 13:18:50,083 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7\n",
      "[INFO|configuration_utils.py:472] 2024-07-14 13:18:50,096 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-14 13:18:50,106 >> Configuration saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-14 13:18:57,122 >> Model weights saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-14 13:18:57,136 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-14 13:18:57,146 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =     950512\n",
      "  total_flos               =  1900939GF\n",
      "  train_loss               =      1.444\n",
      "  train_runtime            = 0:04:03.22\n",
      "  train_samples_per_second =     11.101\n",
      "  train_steps_per_second   =      0.691\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-14 13:18:57,969 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-14 13:18:57,970 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-14 13:18:57,970 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.64it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.4283\n",
      "  eval_runtime            = 0:00:02.16\n",
      "  eval_samples_per_second =     46.104\n",
      "  eval_steps_per_second   =     23.052\n",
      "  num_input_tokens_seen   =     950512\n",
      "[INFO|modelcard.py:449] 2024-07-14 13:19:00,162 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type full \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-07 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/full/train_lr=5e-7 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867a2906-3d6b-432b-8ed0-1b9815209d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
