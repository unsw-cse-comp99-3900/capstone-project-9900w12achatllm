{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "750eafa2-80b7-4056-8f58-7f925002a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gemini/code/capstone-project-9900w12achatllm\n"
     ]
    }
   ],
   "source": [
    "%cd capstone-project-9900w12achatllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e18cda-3e01-4e11-bb98-a718d52cf46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///gemini/code/capstone-project-9900w12achatllm\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.41.2 in /root/miniconda3/lib/python3.10/site-packages (4.42.4)\n",
      "Requirement already satisfied: datasets>=2.16.0 in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: accelerate>=0.30.1 in /root/miniconda3/lib/python3.10/site-packages (0.33.0)\n",
      "Requirement already satisfied: peft>=0.11.1 in /root/miniconda3/lib/python3.10/site-packages (0.12.0)\n",
      "Requirement already satisfied: trl>=0.8.6 in /root/miniconda3/lib/python3.10/site-packages (0.9.6)\n",
      "Requirement already satisfied: gradio>=4.0.0 in /root/miniconda3/lib/python3.10/site-packages (4.7.1)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/lib/python3.10/site-packages (1.11.4)\n",
      "Requirement already satisfied: einops in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.10/site-packages (0.1.99)\n",
      "Requirement already satisfied: tiktoken in /root/miniconda3/lib/python3.10/site-packages (0.7.0)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (4.23.4)\n",
      "Requirement already satisfied: uvicorn in /root/miniconda3/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /root/miniconda3/lib/python3.10/site-packages (2.5.2)\n",
      "Requirement already satisfied: fastapi in /root/miniconda3/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: sse-starlette in /root/miniconda3/lib/python3.10/site-packages (1.8.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /root/miniconda3/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: fire in /root/miniconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (23.1)\n",
      "Requirement already satisfied: pyyaml in /root/miniconda3/lib/python3.10/site-packages (6.0.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: jieba in /root/miniconda3/lib/python3.10/site-packages (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /root/miniconda3/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: torch>=1.13.1 in /root/miniconda3/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (1.26.2)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (5.9.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.24.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/lib/python3.10/site-packages (from accelerate>=0.30.1) (0.4.1)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets>=2.16.0) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets>=2.16.0) (3.9.1)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (23.2.1)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (5.2.0)\n",
      "Requirement already satisfied: ffmpy in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.3.1)\n",
      "Requirement already satisfied: gradio-client==0.7.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.7.0)\n",
      "Requirement already satisfied: httpx in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.2)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (3.9.10)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (10.1.0)\n",
      "Requirement already satisfied: pydub in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (2.10.0)\n",
      "Requirement already satisfied: tomlkit==0.12.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio>=4.0.0) (4.8.0)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /root/miniconda3/lib/python3.10/site-packages (from gradio-client==0.7.0->gradio>=4.0.0) (11.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /root/miniconda3/lib/python3.10/site-packages (from matplotlib>=3.7.0) (2.8.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /root/miniconda3/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (1.12)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.13.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers>=4.41.2) (0.19.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /root/miniconda3/lib/python3.10/site-packages (from trl>=0.8.6) (0.8.5)\n",
      "Requirement already satisfied: click>=7.0 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: six in /root/miniconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /root/miniconda3/lib/python3.10/site-packages (from fire) (2.3.0)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (4.20.0)\n",
      "Requirement already satisfied: toolz in /root/miniconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio>=4.0.0) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets>=2.16.0) (4.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets>=2.16.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.16.0) (2023.7.22)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /root/miniconda3/lib/python3.10/site-packages (from typer[all]<1.0,>=0.9->gradio>=4.0.0) (13.7.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (0.16)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /root/miniconda3/lib/python3.10/site-packages (from tyro>=0.5.11->trl>=0.8.6) (1.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/lib/python3.10/site-packages (from httpx->gradio>=4.0.0) (1.0.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->torch>=1.13.1) (1.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (2023.11.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.31.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /root/miniconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio>=4.0.0) (0.13.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/lib/python3.10/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio>=4.0.0) (0.1.2)\n",
      "Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.8.2.dev0-0.editable-py3-none-any.whl size=6778 sha256=2068429de239c99b5e1faaf56049a6bf991974308413f86efe642ce7ede2668e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-09zeijzw/wheels/5f/7c/6d/29169cc8294fa806bb896a31b2bc295d0ff7b7c925c3a0809b\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.8.2.dev0\n",
      "    Uninstalling llamafactory-0.8.2.dev0:\n",
      "      Successfully uninstalled llamafactory-0.8.2.dev0\n",
      "Successfully installed llamafactory-0.8.2.dev0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/lib/python3.10/site-packages (17.0.0)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.32.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /root/miniconda3/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (0.24.2)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2023.7.22)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: optimum in /root/miniconda3/lib/python3.10/site-packages (1.21.2)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/lib/python3.10/site-packages (from optimum) (15.0.1)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.12)\n",
      "Requirement already satisfied: transformers<4.43.0,>=4.26.0 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.42.4)\n",
      "Requirement already satisfied: torch>=1.11 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.1.1)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/lib/python3.10/site-packages (from optimum) (23.1)\n",
      "Requirement already satisfied: numpy<2.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (1.26.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in /root/miniconda3/lib/python3.10/site-packages (from optimum) (0.24.2)\n",
      "Requirement already satisfied: datasets in /root/miniconda3/lib/python3.10/site-packages (from optimum) (2.20.0)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2024.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (6.0.1)\n",
      "Requirement already satisfied: requests in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/lib/python3.10/site-packages (from huggingface-hub>=0.8.0->optimum) (4.8.0)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /root/miniconda3/lib/python3.10/site-packages (from torch>=1.11->optimum) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /root/miniconda3/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->optimum) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.4.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from transformers<4.43.0,>=4.26.0->transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.19.1)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (4.23.4)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /root/miniconda3/lib/python3.10/site-packages (from transformers[sentencepiece]<4.43.0,>=4.26.0->optimum) (0.1.99)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.3.7)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /root/miniconda3/lib/python3.10/site-packages (from datasets->optimum) (3.9.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/lib/python3.10/site-packages (from sympy->optimum) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /root/miniconda3/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/lib/python3.10/site-packages (from requests->huggingface-hub>=0.8.0->optimum) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/lib/python3.10/site-packages (from pandas->datasets->optimum) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping apex as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -e \".[torch,metrics]\"\n",
    "!pip install --upgrade pandas pyarrow datasets\n",
    "!pip install auto_gptq>=0.5.0\n",
    "!pip install optimum\n",
    "!pip uninstall apex -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd41ddc7-fe57-45b6-87db-5ee83877d3a6",
   "metadata": {},
   "source": [
    "LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47387f9e-0017-4a97-b122-9d07ee22bdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 03:59:36,825] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 03:59:43 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 03:59:43,743 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 03:59:44,150 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 03:59:44 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 03:59:44 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 03:59:49 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 03:59:50,768 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 03:59:50,770 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 03:59:50,841 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 03:59:51,347 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 03:59:51,350 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:00:04,553 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:00:04,553 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:00:04,575 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:00:04,576 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:00:04 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:00:04 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:00:04 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:00:04 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 04:00:04 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,down_proj,v_proj,o_proj,q_proj,up_proj,k_proj\n",
      "07/26/2024 04:00:05 - INFO - llamafactory.model.loader - trainable params: 4399104 || all params: 498431872 || trainable%: 0.8826\n",
      "[INFO|trainer.py:642] 2024-07-26 04:00:05,247 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:00:05,669 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:00:05,669 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:00:05,670 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:00:05,670 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:00:05,670 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:00:05,670 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:00:05,670 >>   Total optimization steps = 168\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:00:05,673 >>   Number of trainable parameters = 4,399,104\n",
      "{'loss': 1.844, 'grad_norm': 1.7298327684402466, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5291, 'grad_norm': 1.527961254119873, 'learning_rate': 4.956416183083221e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:27<06:00,  2.28s/it][INFO|trainer.py:3788] 2024-07-26 04:00:33,385 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:00:33,385 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:00:33,385 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.70it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 17.28it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.49it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.28it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.11it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.11it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.07it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.86it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.89it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.99it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.69it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 15.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.48it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 14.89it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.32it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.39it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.57it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.68it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.90it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 15.58it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5484271049499512, 'eval_runtime': 3.2056, 'eval_samples_per_second': 31.196, 'eval_steps_per_second': 15.598, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [00:30<06:00,  2.28s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.76it/s]\u001b[A\n",
      "{'loss': 1.6202, 'grad_norm': 1.3011258840560913, 'learning_rate': 4.9022933048627496e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.4393, 'grad_norm': 1.4655195474624634, 'learning_rate': 4.827184371610511e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [00:51<05:20,  2.17s/it][INFO|trainer.py:3788] 2024-07-26 04:00:57,651 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:00:57,651 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:00:57,651 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 25.57it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 21.23it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 19.86it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 18.28it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 17.89it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 17.76it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 17.13it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.71it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 16.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 16.09it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.50it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.40it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.57it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.37it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 16.24it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.06it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.31it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.93it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.93it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.98it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 16.20it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4697474241256714, 'eval_runtime': 3.0817, 'eval_samples_per_second': 32.45, 'eval_steps_per_second': 16.225, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [00:55<05:20,  2.17s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.91it/s]\u001b[A\n",
      "{'loss': 1.296, 'grad_norm': 1.1275662183761597, 'learning_rate': 4.731745523109029e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3491, 'grad_norm': 1.1943882703781128, 'learning_rate': 4.6168104980707107e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [01:16<04:58,  2.16s/it][INFO|trainer.py:3788] 2024-07-26 04:01:21,889 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:01:21,889 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:01:21,889 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 20.60it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.50it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.89it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.75it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 15.94it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.01it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.21it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.94it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.88it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.01it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.90it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 15.99it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.84it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.93it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.71it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.77it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.86it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 16.06it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.41it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4470373392105103, 'eval_runtime': 3.2789, 'eval_samples_per_second': 30.498, 'eval_steps_per_second': 15.249, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [01:19<04:58,  2.16s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.67it/s]\u001b[A\n",
      "{'loss': 1.58, 'grad_norm': 1.1794564723968506, 'learning_rate': 4.4833833507280884e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.2147, 'grad_norm': 1.0084750652313232, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [01:41<04:39,  2.19s/it][INFO|trainer.py:3788] 2024-07-26 04:01:46,731 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:01:46,731 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:01:46,731 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.58it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 16.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.73it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 17.22it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.92it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.26it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.95it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.95it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.33it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.44it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.77it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.42it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.06it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.04it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.88it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.18it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.22it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.13it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.36it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.35it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 14.64it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 14.18it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.43534517288208, 'eval_runtime': 3.3035, 'eval_samples_per_second': 30.271, 'eval_steps_per_second': 15.135, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [01:44<04:39,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.56it/s]\u001b[A\n",
      "{'loss': 1.1833, 'grad_norm': 1.5220625400543213, 'learning_rate': 4.16586644488001e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.4274, 'grad_norm': 1.1121591329574585, 'learning_rate': 3.9845504639337535e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [02:06<04:20,  2.21s/it][INFO|trainer.py:3788] 2024-07-26 04:02:11,743 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:02:11,743 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:02:11,743 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 25.15it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 20.25it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 19.42it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 18.91it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 17.71it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 17.06it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 17.06it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:01, 16.43it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 16.52it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 14.71it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 15.13it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 15.04it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 15.22it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 14.21it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 14.39it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:00, 15.24it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 15.88it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 16.37it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 16.77it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 17.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 17.54it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 17.47it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4281582832336426, 'eval_runtime': 3.0641, 'eval_samples_per_second': 32.636, 'eval_steps_per_second': 16.318, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [02:09<04:20,  2.21s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 17.70it/s]\u001b[A\n",
      "{'loss': 1.5126, 'grad_norm': 1.268788456916809, 'learning_rate': 3.790265684518767e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.4877, 'grad_norm': 1.1511110067367554, 'learning_rate': 3.5847093477938956e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [02:30<03:56,  2.19s/it][INFO|trainer.py:3788] 2024-07-26 04:02:36,198 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:02:36,198 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:02:36,199 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.25it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.45it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.12it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.66it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 15.84it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.47it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 15.42it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.54it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.06it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.19it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.48it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.13it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.35it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.09it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.62it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 14.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 14.85it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 14.65it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.01it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.22it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.36it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 15.23it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4226105213165283, 'eval_runtime': 3.3312, 'eval_samples_per_second': 30.019, 'eval_steps_per_second': 15.009, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [02:33<03:56,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.43it/s]\u001b[A\n",
      "{'loss': 1.458, 'grad_norm': 1.3816213607788086, 'learning_rate': 3.369677161463068e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.3807, 'grad_norm': 1.3323551416397095, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [02:54<03:26,  2.11s/it][INFO|trainer.py:3788] 2024-07-26 04:03:00,417 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:03:00,417 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:03:00,417 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.49it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.33it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.39it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 15.44it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 14.89it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.12it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 15.38it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.54it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.65it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.48it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.39it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.70it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.81it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.96it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 16.00it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.30it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 14.65it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 16.44it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 16.97it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 17.21it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 16.65it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 15.34it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4211691617965698, 'eval_runtime': 3.2723, 'eval_samples_per_second': 30.559, 'eval_steps_per_second': 15.28, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [02:58<03:26,  2.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.47it/s]\u001b[A\n",
      "{'loss': 1.4921, 'grad_norm': 1.2308391332626343, 'learning_rate': 2.918765558261841e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.3513, 'grad_norm': 1.3814599514007568, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [03:18<03:04,  2.10s/it][INFO|trainer.py:3788] 2024-07-26 04:03:24,496 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:03:24,496 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:03:24,496 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.53it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.87it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.63it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.06it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.32it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.27it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.30it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.42it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.57it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.58it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.90it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.19it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.40it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.96it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.26it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.19it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 16.11it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 16.23it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.82it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.97it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4204484224319458, 'eval_runtime': 3.1587, 'eval_samples_per_second': 31.659, 'eval_steps_per_second': 15.83, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [03:21<03:04,  2.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 16.05it/s]\u001b[A\n",
      "{'loss': 1.3362, 'grad_norm': 1.0497275590896606, 'learning_rate': 2.4532528339227452e-05, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.4532, 'grad_norm': 1.221352458000183, 'learning_rate': 2.2200888097417307e-05, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [03:42<02:43,  2.09s/it][INFO|trainer.py:3788] 2024-07-26 04:03:48,349 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:03:48,349 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:03:48,349 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 23.15it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.60it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.03it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.42it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.33it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.53it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 16.27it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 16.32it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.73it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.63it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.88it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 17.47it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:00, 17.46it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 17.59it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 17.74it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 17.60it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 17.60it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 17.73it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 17.77it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 17.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.418949007987976, 'eval_runtime': 3.028, 'eval_samples_per_second': 33.025, 'eval_steps_per_second': 16.512, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [03:45<02:43,  2.09s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 17.65it/s]\u001b[A\n",
      "{'loss': 1.2641, 'grad_norm': 0.7938742637634277, 'learning_rate': 1.9893700455257996e-05, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.32, 'grad_norm': 1.1343640089035034, 'learning_rate': 1.7631120639727393e-05, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [04:06<02:25,  2.15s/it][INFO|trainer.py:3788] 2024-07-26 04:04:12,466 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:04:12,466 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:04:12,466 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 16.66it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.09it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 15.85it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 15.55it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.01it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 15.13it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.23it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.34it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.15it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.10it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.53it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.46it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.73it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.88it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.87it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.08it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.15it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.67it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.82it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.79it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 15.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4178613424301147, 'eval_runtime': 3.2854, 'eval_samples_per_second': 30.438, 'eval_steps_per_second': 15.219, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [04:10<02:25,  2.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.76it/s]\u001b[A\n",
      "{'loss': 1.2677, 'grad_norm': 1.1781986951828003, 'learning_rate': 1.5432914190872757e-05, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.6224, 'grad_norm': 1.2002079486846924, 'learning_rate': 1.331828429317345e-05, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [04:31<02:06,  2.19s/it][INFO|trainer.py:3788] 2024-07-26 04:04:37,101 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:04:37,102 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:04:37,102 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.08it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.47it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.44it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.73it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 15.80it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.64it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 15.49it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.55it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.72it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.40it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.28it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.16it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.10it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.44it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.54it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:00, 16.30it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.35it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.53it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 16.25it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.90it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 16.18it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.419083833694458, 'eval_runtime': 3.1837, 'eval_samples_per_second': 31.41, 'eval_steps_per_second': 15.705, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [04:34<02:06,  2.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 16.13it/s]\u001b[A\n",
      "{'loss': 1.4116, 'grad_norm': 1.1477397680282593, 'learning_rate': 1.130570401955322e-05, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.392, 'grad_norm': 0.7489234805107117, 'learning_rate': 9.412754953531663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [04:54<01:38,  2.05s/it][INFO|trainer.py:3788] 2024-07-26 04:05:00,348 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:05:00,348 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:05:00,348 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.51it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 15.75it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 15.33it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 15.89it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 15.31it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.47it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:01<00:02, 15.65it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.70it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.84it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.64it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.66it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.07it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.93it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.14it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:02<00:01, 15.93it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.79it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.88it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.95it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 16.05it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 16.93it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 16.73it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 17.32it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 17.08it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4196999073028564, 'eval_runtime': 3.1666, 'eval_samples_per_second': 31.579, 'eval_steps_per_second': 15.79, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [04:57<01:38,  2.05s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 17.08it/s]\u001b[A\n",
      "{'loss': 1.171, 'grad_norm': 1.1208434104919434, 'learning_rate': 7.65597359928646e-06, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.4249, 'grad_norm': 1.051795244216919, 'learning_rate': 6.050706921363672e-06, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [05:18<01:21,  2.15s/it][INFO|trainer.py:3788] 2024-07-26 04:05:24,292 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:05:24,293 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:05:24,293 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 23.21it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.73it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 17.86it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 18.40it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 18.02it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 18.04it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 17.98it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:01, 17.89it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 17.98it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 17.73it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 18.12it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 18.23it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:01, 18.64it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:01, 18.54it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 18.66it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 18.73it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 18.63it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 18.52it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 18.52it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 18.46it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 18.52it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 17.16it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4195654392242432, 'eval_runtime': 2.845, 'eval_samples_per_second': 35.149, 'eval_steps_per_second': 17.574, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [05:21<01:21,  2.15s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 17.34it/s]\u001b[A\n",
      "{'loss': 1.4342, 'grad_norm': 1.1387019157409668, 'learning_rate': 4.610978276018496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.402, 'grad_norm': 1.129263162612915, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [05:42<00:58,  2.10s/it][INFO|trainer.py:3788] 2024-07-26 04:05:48,059 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:05:48,060 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:05:48,060 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.37it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.85it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.24it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.74it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 15.83it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.57it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 15.77it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.98it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.80it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.78it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.50it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.10it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.06it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.46it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.38it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:00, 16.15it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.07it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.82it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.25it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.51it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.78it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 14.45it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 14.67it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4193403720855713, 'eval_runtime': 3.253, 'eval_samples_per_second': 30.741, 'eval_steps_per_second': 15.37, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [05:45<00:58,  2.10s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.14it/s]\u001b[A\n",
      "{'loss': 1.2357, 'grad_norm': 1.0263901948928833, 'learning_rate': 2.2768880646947268e-06, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.7082, 'grad_norm': 1.2331246137619019, 'learning_rate': 1.4029167422908107e-06, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [06:07<00:39,  2.18s/it][INFO|trainer.py:3788] 2024-07-26 04:06:12,727 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:06:12,727 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:06:12,728 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 17.00it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.49it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.76it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.67it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.53it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.50it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.18it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 16.20it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.40it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.32it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.08it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.66it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 15.79it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.74it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.96it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.00it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.81it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 16.40it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 16.14it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.73it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 14.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4193207025527954, 'eval_runtime': 3.1948, 'eval_samples_per_second': 31.301, 'eval_steps_per_second': 15.651, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [06:10<00:39,  2.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.31it/s]\u001b[A\n",
      "{'loss': 1.5185, 'grad_norm': 1.1225976943969727, 'learning_rate': 7.350858136652261e-07, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 1.1116, 'grad_norm': 0.9536842703819275, 'learning_rate': 2.7922934437178695e-07, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [06:31<00:17,  2.22s/it][INFO|trainer.py:3788] 2024-07-26 04:06:37,282 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:06:37,282 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:06:37,282 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 18.38it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:02, 17.55it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:02, 16.84it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 16.87it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 16.55it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 15.92it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:02, 15.45it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 15.78it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 15.46it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 15.34it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 15.22it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 15.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.419164776802063, 'eval_runtime': 3.2739, 'eval_samples_per_second': 30.545, 'eval_steps_per_second': 15.273, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [06:34<00:17,  2.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.52it/s]\u001b[A\n",
      "{'loss': 1.2515, 'grad_norm': 1.519592523574829, 'learning_rate': 3.9329624554584884e-08, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "100%|█████████████████████████████████████████| 168/168 [06:52<00:00,  2.23s/it][INFO|trainer.py:3478] 2024-07-26 04:06:58,078 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/checkpoint-168\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:06:58,122 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:06:58,123 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:06:58,317 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/checkpoint-168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:06:58,326 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/checkpoint-168/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 04:06:59,117 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 413.4441, 'train_samples_per_second': 6.531, 'train_steps_per_second': 0.406, 'train_loss': 1.4149029084614344, 'epoch': 2.99, 'num_input_tokens_seen': 950512}\n",
      "100%|█████████████████████████████████████████| 168/168 [06:53<00:00,  2.46s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 04:06:59,125 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:06:59,164 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:06:59,165 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:06:59,334 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:06:59,342 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =     950512\n",
      "  total_flos               =  1924305GF\n",
      "  train_loss               =     1.4149\n",
      "  train_runtime            = 0:06:53.44\n",
      "  train_samples_per_second =      6.531\n",
      "  train_steps_per_second   =      0.406\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 04:07:00,235 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:07:00,235 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:07:00,235 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.15it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.4191\n",
      "  eval_runtime            = 0:00:03.38\n",
      "  eval_samples_per_second =     29.507\n",
      "  eval_steps_per_second   =     14.753\n",
      "  num_input_tokens_seen   =     950512\n",
      "[INFO|modelcard.py:449] 2024-07-26 04:07:03,643 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb61405-dc34-4737-a316-306fd9cb7223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 04:07:23,765] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 04:07:30 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:07:30,523 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 04:07:30,826 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 04:07:30 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 04:07:30 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 04:07:36 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:07:37,462 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:07:37,464 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 04:07:37,552 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 04:07:38,035 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:07:38,041 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:07:48,945 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:07:48,945 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:07:48,950 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:07:48,950 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,gate_proj,down_proj,q_proj,up_proj,o_proj,k_proj\n",
      "07/26/2024 04:07:49 - INFO - llamafactory.model.loader - trainable params: 4399104 || all params: 498431872 || trainable%: 0.8826\n",
      "[INFO|trainer.py:642] 2024-07-26 04:07:49,763 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:07:50,218 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:07:50,218 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:07:50,218 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:07:50,218 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:07:50,218 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:07:50,218 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:07:50,218 >>   Total optimization steps = 54\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:07:50,223 >>   Number of trainable parameters = 4,399,104\n",
      "{'loss': 2.0501, 'grad_norm': 1.2575711011886597, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\n",
      "{'loss': 1.9023, 'grad_norm': 1.118385672569275, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\n",
      " 19%|███████▉                                   | 10/54 [00:36<02:21,  3.23s/it][INFO|trainer.py:3788] 2024-07-26 04:08:26,690 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:08:26,691 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:08:26,691 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 19.34it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:00<00:03, 12.20it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:03, 12.96it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:03, 11.66it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:03, 12.78it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:01<00:03, 10.67it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:01<00:02, 11.74it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:01<00:02, 12.70it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:02, 13.39it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:02, 13.82it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 13.94it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 13.72it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:02<00:01, 13.41it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:02<00:01, 13.90it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:02<00:01, 13.84it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:02<00:01, 13.47it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:02<00:01, 14.28it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:02<00:00, 14.72it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:02<00:00, 13.67it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:03<00:00, 13.83it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:03<00:00, 13.92it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:03<00:00, 14.39it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:03<00:00, 14.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.527052402496338, 'eval_runtime': 3.9596, 'eval_samples_per_second': 25.255, 'eval_steps_per_second': 12.627, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\n",
      " 19%|███████▉                                   | 10/54 [00:40<02:21,  3.23s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.12it/s]\u001b[A\n",
      "{'loss': 1.6379, 'grad_norm': 0.8996407985687256, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\n",
      "{'loss': 1.8207, 'grad_norm': 0.8165451884269714, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\n",
      " 37%|███████████████▉                           | 20/54 [01:12<01:49,  3.22s/it][INFO|trainer.py:3788] 2024-07-26 04:09:02,383 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:09:02,384 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:09:02,384 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.38it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.64it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.87it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 16.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.10it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.72it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 15.87it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 15.84it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.09it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.48it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 15.38it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.50it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 15.93it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.25it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.40it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.73it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.43it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.42it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.36it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.21it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 14.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4598666429519653, 'eval_runtime': 3.2596, 'eval_samples_per_second': 30.679, 'eval_steps_per_second': 15.339, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\n",
      " 37%|███████████████▉                           | 20/54 [01:15<01:49,  3.22s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.25it/s]\u001b[A\n",
      "{'loss': 1.763, 'grad_norm': 0.8163280487060547, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\n",
      "{'loss': 1.6429, 'grad_norm': 0.7104174494743347, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\n",
      " 56%|███████████████████████▉                   | 30/54 [01:46<01:14,  3.12s/it][INFO|trainer.py:3788] 2024-07-26 04:09:36,589 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:09:36,590 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:09:36,590 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.85it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 17.67it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 16.81it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.15it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.23it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 15.87it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.07it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:02, 16.00it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.02it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.59it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.67it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.26it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.02it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.37it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.49it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:00, 16.49it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 16.52it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 16.61it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 16.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 16.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 16.47it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 16.54it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4416289329528809, 'eval_runtime': 3.1379, 'eval_samples_per_second': 31.868, 'eval_steps_per_second': 15.934, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\n",
      " 56%|███████████████████████▉                   | 30/54 [01:49<01:14,  3.12s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 16.31it/s]\u001b[A\n",
      "{'loss': 1.6128, 'grad_norm': 0.7787975668907166, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\n",
      "{'loss': 1.7812, 'grad_norm': 0.8391226530075073, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\n",
      " 74%|███████████████████████████████▊           | 40/54 [02:20<00:43,  3.11s/it][INFO|trainer.py:3788] 2024-07-26 04:10:10,624 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:10:10,625 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:10:10,625 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 21.83it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.19it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 17.16it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.43it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.58it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.36it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.19it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.20it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.30it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 16.04it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.84it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.19it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 15.90it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.07it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 14.30it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 14.62it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 14.97it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.31it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.62it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:03<00:00, 15.45it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.434600830078125, 'eval_runtime': 3.2328, 'eval_samples_per_second': 30.933, 'eval_steps_per_second': 15.466, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\n",
      " 74%|███████████████████████████████▊           | 40/54 [02:23<00:43,  3.11s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.57it/s]\u001b[A\n",
      "{'loss': 1.6391, 'grad_norm': 0.8252052068710327, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\n",
      "{'loss': 1.6895, 'grad_norm': 0.8362146019935608, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [02:55<00:13,  3.26s/it][INFO|trainer.py:3788] 2024-07-26 04:10:45,536 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:10:45,537 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:10:45,537 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:02, 22.97it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 18.11it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:02, 17.08it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:02, 17.43it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 16.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:02, 16.36it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:02, 16.28it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:01<00:01, 16.34it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 15.64it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 15.32it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 15.50it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 16.08it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 16.04it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 16.18it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 16.20it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:02<00:01, 15.90it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 15.82it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 15.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 15.51it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 15.63it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 15.56it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 15.62it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:02<00:00, 15.46it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4329533576965332, 'eval_runtime': 3.209, 'eval_samples_per_second': 31.162, 'eval_steps_per_second': 15.581, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [02:58<00:13,  3.26s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 15.45it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 54/54 [03:10<00:00,  3.35s/it][INFO|trainer.py:3478] 2024-07-26 04:11:00,642 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/checkpoint-54\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:11:00,713 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:11:00,714 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:11:01,398 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/checkpoint-54/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:11:01,414 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/checkpoint-54/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 04:11:03,556 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 193.3337, 'train_samples_per_second': 13.965, 'train_steps_per_second': 0.279, 'train_loss': 1.7465273362618905, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\n",
      "100%|███████████████████████████████████████████| 54/54 [03:13<00:00,  3.58s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 04:11:03,571 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:11:03,632 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:11:03,635 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:11:04,294 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:11:04,307 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.88\n",
      "  num_input_tokens_seen    =    1278720\n",
      "  total_flos               =  2588760GF\n",
      "  train_loss               =     1.7465\n",
      "  train_runtime            = 0:03:13.33\n",
      "  train_samples_per_second =     13.965\n",
      "  train_steps_per_second   =      0.279\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 04:11:05,365 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:11:05,366 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:11:05,366 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:03<00:00, 14.67it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.88\n",
      "  eval_loss               =      1.433\n",
      "  eval_runtime            = 0:00:03.50\n",
      "  eval_samples_per_second =     28.526\n",
      "  eval_steps_per_second   =     14.263\n",
      "  num_input_tokens_seen   =    1278720\n",
      "[INFO|modelcard.py:449] 2024-07-26 04:11:08,915 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_lora_bz=6 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0d5d3a-8b66-484b-afe7-551fc0eb1ea8",
   "metadata": {},
   "source": [
    "QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f615a5-b235-4205-adad-1cf4014cdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 04:12:02,950] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 04:12:26 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/26/2024 04:12:26 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:12:26,955 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 04:12:27,459 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 04:12:27 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 04:12:27 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 04:12:33 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:12:34,313 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:12:34,322 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/26/2024 04:12:34 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 04:12:34,834 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 04:12:35,643 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:12:35,649 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:12:49,607 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:12:49,608 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:12:49,621 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:12:49,622 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:12:49 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:12:49 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:12:49 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:12:49 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 04:12:49 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,k_proj,gate_proj,q_proj,up_proj,v_proj,o_proj\n",
      "07/26/2024 04:12:50 - INFO - llamafactory.model.loader - trainable params: 4399104 || all params: 498431872 || trainable%: 0.8826\n",
      "[INFO|trainer.py:642] 2024-07-26 04:12:50,235 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:12:51,012 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:12:51,012 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:12:51,012 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:12:51,012 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:12:51,012 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:12:51,012 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:12:51,012 >>   Total optimization steps = 168\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:12:51,017 >>   Number of trainable parameters = 4,399,104\n",
      "{'loss': 1.8453, 'grad_norm': 1.6934832334518433, 'learning_rate': 4.989080197352834e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.5443, 'grad_norm': 1.5799267292022705, 'learning_rate': 4.96467754629559e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [01:27<19:39,  7.47s/it][INFO|trainer.py:3788] 2024-07-26 04:14:18,441 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:14:18,441 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:14:18,441 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.70it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.10it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.50it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.40it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.16it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  3.00it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.95it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:14,  2.88it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  2.90it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.78it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:14,  2.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.67it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.70it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:12,  2.72it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  2.84it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.92it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:11,  2.89it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  2.89it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:10,  2.99it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:09,  3.03it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:09,  3.04it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:08,  3.05it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:07<00:08,  3.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  3.09it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:07,  3.06it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:08<00:07,  3.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:07,  3.03it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:06,  3.14it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:09<00:06,  3.12it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:06,  3.10it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:05,  3.14it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:10<00:05,  3.14it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:05,  3.12it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:11<00:04,  3.07it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:11<00:04,  2.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:12<00:04,  2.95it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:12<00:03,  3.08it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:12<00:03,  3.14it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:13<00:03,  3.17it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:13<00:02,  3.35it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:13<00:02,  3.49it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:13<00:01,  3.66it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:14<00:01,  3.77it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:14<00:01,  3.84it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:14<00:01,  3.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:15<00:00,  3.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:15<00:00,  3.35it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:15<00:00,  3.17it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5643486976623535, 'eval_runtime': 16.4097, 'eval_samples_per_second': 6.094, 'eval_steps_per_second': 3.047, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  6%|██▌                                       | 10/168 [01:43<19:39,  7.47s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:16<00:00,  3.18it/s]\u001b[A\n",
      "{'loss': 1.6351, 'grad_norm': 1.3288559913635254, 'learning_rate': 4.914814565722671e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.4515, 'grad_norm': 1.4243974685668945, 'learning_rate': 4.8438561463599984e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [02:45<16:14,  6.58s/it][INFO|trainer.py:3788] 2024-07-26 04:15:36,455 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:15:36,455 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:15:36,455 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.68it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.11it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  3.88it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.62it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.37it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:13,  3.22it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.21it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.18it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:12,  3.09it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.82it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:13,  2.73it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.66it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:13,  2.66it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.69it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:12,  2.58it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.52it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:11,  2.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:11,  2.69it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:10,  2.74it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.71it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:09,  2.81it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:09,  2.79it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  2.90it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:08,  2.94it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:08,  2.79it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:08,  2.69it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:07,  2.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.85it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:06,  2.88it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:10<00:06,  3.00it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:05,  2.93it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:05,  2.75it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.59it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:12<00:05,  2.46it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:05,  2.53it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:04,  2.71it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.58it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.67it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:02,  2.76it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.84it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:02,  2.87it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  2.84it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.83it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:01,  2.69it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  2.72it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  2.79it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4756015539169312, 'eval_runtime': 18.0349, 'eval_samples_per_second': 5.545, 'eval_steps_per_second': 2.772, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 12%|█████                                     | 20/168 [03:03<16:14,  6.58s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  2.78it/s]\u001b[A\n",
      "{'loss': 1.3013, 'grad_norm': 1.1435946226119995, 'learning_rate': 4.752422169756048e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.3522, 'grad_norm': 1.2715978622436523, 'learning_rate': 4.641311388694629e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [04:20<16:47,  7.30s/it][INFO|trainer.py:3788] 2024-07-26 04:17:11,066 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:17:11,066 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:17:11,067 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.55it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.43it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  3.91it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.66it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.45it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:12,  3.38it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:12,  3.44it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.60it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.60it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:11,  3.50it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.47it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.55it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.61it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:09,  3.60it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.55it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:09,  3.55it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:09,  3.48it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.49it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.61it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:08,  3.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:08,  3.41it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.42it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:07,  3.34it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:07<00:07,  3.33it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:07,  3.38it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.61it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:06,  3.59it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.82it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.73it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:04,  3.94it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  4.00it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.14it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  4.10it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:03,  3.72it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.61it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:03,  3.44it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:02,  3.49it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  3.51it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.54it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  3.58it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  3.59it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  3.63it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:01,  3.64it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.56it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.48it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  3.41it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.450612187385559, 'eval_runtime': 14.2303, 'eval_samples_per_second': 7.027, 'eval_steps_per_second': 3.514, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 18%|███████▌                                  | 30/168 [04:34<16:47,  7.30s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.45it/s]\u001b[A\n",
      "{'loss': 1.58, 'grad_norm': 1.2307320833206177, 'learning_rate': 4.511494449416671e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.2165, 'grad_norm': 0.9912742376327515, 'learning_rate': 4.364105412207914e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [05:48<17:26,  8.18s/it][INFO|trainer.py:3788] 2024-07-26 04:18:39,222 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:18:39,222 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:18:39,222 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.58it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.50it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.15it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.77it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.55it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.53it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.49it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.52it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.43it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.35it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.30it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.40it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.44it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:14,  2.43it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:13,  2.51it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.44it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.41it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.51it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.51it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.47it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.34it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:12,  2.24it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:11,  2.31it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:10,  2.43it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.66it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.86it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:07,  2.95it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:06,  3.07it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:06,  3.16it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:05,  3.28it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:05,  3.37it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:05,  3.22it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  2.94it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:04,  3.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:04,  2.90it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:04,  2.81it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:04,  2.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:03,  2.83it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.86it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.83it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  2.83it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.88it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:16<00:02,  2.91it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  2.92it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.94it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:17<00:01,  2.83it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  2.82it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:18<00:00,  2.83it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4400235414505005, 'eval_runtime': 18.819, 'eval_samples_per_second': 5.314, 'eval_steps_per_second': 2.657, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 24%|██████████                                | 40/168 [06:06<17:26,  8.18s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  2.83it/s]\u001b[A\n",
      "{'loss': 1.1877, 'grad_norm': 1.5065380334854126, 'learning_rate': 4.2004318444272985e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.4318, 'grad_norm': 1.1113874912261963, 'learning_rate': 4.021903572521802e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [07:27<16:30,  8.39s/it][INFO|trainer.py:3788] 2024-07-26 04:20:18,099 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:20:18,099 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:20:18,099 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.72it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.21it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:15,  2.89it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.89it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:16,  2.72it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.66it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.67it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.68it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.41it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:15,  2.38it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:15,  2.37it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:15,  2.30it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.25it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.23it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.30it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.36it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:12,  2.46it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.48it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.48it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:11,  2.45it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.51it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.66it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.74it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.75it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:07,  2.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.78it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:07,  2.69it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:06,  2.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:06,  2.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:06,  2.62it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:05,  2.65it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:05,  2.56it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:05,  2.51it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:04,  2.55it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:15<00:04,  2.56it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:03,  2.50it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:16<00:03,  2.44it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:16<00:02,  2.44it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:17<00:02,  2.34it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:17<00:02,  2.38it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:18<00:01,  2.47it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:18<00:01,  2.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:18<00:00,  2.43it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:19<00:00,  2.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4288296699523926, 'eval_runtime': 20.0283, 'eval_samples_per_second': 4.993, 'eval_steps_per_second': 2.496, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 30%|████████████▌                             | 50/168 [07:47<16:30,  8.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:19<00:00,  2.52it/s]\u001b[A\n",
      "{'loss': 1.5129, 'grad_norm': 1.2713018655776978, 'learning_rate': 3.830080191288342e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.4891, 'grad_norm': 1.1689268350601196, 'learning_rate': 3.6266374394998634e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [09:06<14:52,  8.27s/it][INFO|trainer.py:3788] 2024-07-26 04:21:57,355 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:21:57,355 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:21:57,356 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  4.80it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.53it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.77it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.61it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.51it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.50it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:17,  2.47it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.59it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.61it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.45it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.29it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:16,  2.24it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:16,  2.12it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.24it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:15,  2.25it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.38it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.45it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.46it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.44it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:10,  2.50it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.45it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:10,  2.50it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.49it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:09,  2.54it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:08,  2.57it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:07,  2.63it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.61it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:12<00:07,  2.59it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:06,  2.65it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:13<00:06,  2.59it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:06,  2.56it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:05,  2.53it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:14<00:05,  2.47it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:05,  2.37it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:15<00:04,  2.41it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:15<00:04,  2.51it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:03,  2.58it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:16<00:03,  2.54it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:16<00:03,  2.31it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:17<00:03,  2.32it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:17<00:02,  2.40it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:18<00:02,  2.50it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:18<00:01,  2.54it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:18<00:01,  2.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:19<00:00,  2.44it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:19<00:00,  2.31it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.424973487854004, 'eval_runtime': 20.6753, 'eval_samples_per_second': 4.837, 'eval_steps_per_second': 2.418, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 36%|███████████████                           | 60/168 [09:26<14:52,  8.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:20<00:00,  2.28it/s]\u001b[A\n",
      "{'loss': 1.4581, 'grad_norm': 1.3719353675842285, 'learning_rate': 3.413352560915988e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 1.3854, 'grad_norm': 1.2564724683761597, 'learning_rate': 3.1920887785621235e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [10:47<13:35,  8.32s/it][INFO|trainer.py:3788] 2024-07-26 04:23:38,986 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:23:38,987 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:23:38,987 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.14it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.44it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.82it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.54it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.49it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:17,  2.43it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.36it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.55it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.52it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:15,  2.41it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.40it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.53it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.63it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:13,  2.59it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.50it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.54it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.58it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.53it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.54it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.60it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.58it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.59it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.53it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.56it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:09,  2.44it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:08,  2.41it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:08,  2.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:12<00:07,  2.38it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:07,  2.35it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:13<00:07,  2.24it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:07,  2.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:14<00:06,  2.22it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:14<00:06,  2.26it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:05,  2.29it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:15<00:05,  2.36it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:15<00:04,  2.56it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:03,  2.64it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:16<00:03,  2.57it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:16<00:03,  2.60it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:17<00:02,  2.54it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:17<00:02,  2.57it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:17<00:01,  2.59it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:18<00:01,  2.65it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:18<00:01,  2.65it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:19<00:00,  2.58it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:19<00:00,  2.51it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4224286079406738, 'eval_runtime': 20.2742, 'eval_samples_per_second': 4.932, 'eval_steps_per_second': 2.466, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 42%|█████████████████▌                        | 70/168 [11:08<13:35,  8.32s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:19<00:00,  2.52it/s]\u001b[A\n",
      "{'loss': 1.4944, 'grad_norm': 1.2455949783325195, 'learning_rate': 2.9647790179072872e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 1.3519, 'grad_norm': 1.3614048957824707, 'learning_rate': 2.7334090211323766e-05, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [12:23<10:45,  7.33s/it][INFO|trainer.py:3788] 2024-07-26 04:25:14,240 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:25:14,240 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:25:14,240 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.40it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.79it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.24it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.83it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:16,  2.74it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.75it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.67it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:14,  2.74it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.64it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:15,  2.55it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.37it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:15,  2.38it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.39it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:14,  2.37it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.35it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:14,  2.27it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.30it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:13,  2.29it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:12,  2.32it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.39it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.57it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:09,  2.70it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:08,  2.71it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.77it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.71it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:07,  2.76it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.77it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:07,  2.68it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:06,  2.68it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:06,  2.78it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  2.84it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:05,  2.81it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:04,  2.83it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:04,  2.60it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:04,  2.56it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:04,  2.55it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:03,  2.52it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.47it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:16<00:03,  2.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:16<00:02,  2.70it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:16<00:02,  2.68it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:17<00:01,  2.73it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:17<00:01,  2.61it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:17<00:01,  2.50it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:18<00:00,  2.46it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:18<00:00,  2.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4217660427093506, 'eval_runtime': 19.7291, 'eval_samples_per_second': 5.069, 'eval_steps_per_second': 2.534, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 48%|████████████████████                      | 80/168 [12:42<10:45,  7.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:19<00:00,  2.24it/s]\u001b[A\n",
      "{'loss': 1.3358, 'grad_norm': 1.0798351764678955, 'learning_rate': 2.5e-05, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 1.4553, 'grad_norm': 1.2285046577453613, 'learning_rate': 2.2665909788676237e-05, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [14:02<10:38,  8.19s/it][INFO|trainer.py:3788] 2024-07-26 04:26:53,286 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:26:53,286 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:26:53,286 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.30it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.77it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.27it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.05it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.80it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.79it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.76it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:14,  2.73it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.76it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.76it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.59it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.53it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:14,  2.48it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.54it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.60it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:12,  2.59it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:13,  2.40it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.32it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:12,  2.42it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.52it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:10,  2.56it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.49it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.52it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.63it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:08,  2.68it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.63it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.63it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:07,  2.79it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.84it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:06,  2.92it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:05,  3.12it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:05,  3.06it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  3.05it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:04,  3.14it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:04,  3.00it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:04,  2.92it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.85it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:03,  2.98it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.97it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.91it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  3.06it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  3.09it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:01,  3.26it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  3.43it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  3.42it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:00,  3.34it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  3.34it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  3.38it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4201228618621826, 'eval_runtime': 17.8692, 'eval_samples_per_second': 5.596, 'eval_steps_per_second': 2.798, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 54%|██████████████████████▌                   | 90/168 [14:20<10:38,  8.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  3.31it/s]\u001b[A\n",
      "{'loss': 1.2689, 'grad_norm': 0.8121336102485657, 'learning_rate': 2.0352209820927137e-05, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 1.3221, 'grad_norm': 1.1392475366592407, 'learning_rate': 1.8079112214378768e-05, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [15:21<07:13,  6.38s/it][INFO|trainer.py:3788] 2024-07-26 04:28:12,921 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:28:12,921 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:28:12,921 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.76it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.58it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:10,  4.19it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:11,  3.98it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:11,  3.87it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:11,  3.87it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:10,  3.82it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:10,  3.89it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.89it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:09,  3.90it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.64it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.67it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.69it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:09,  3.75it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.73it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  3.75it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.73it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:08,  3.76it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  3.84it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  3.86it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:07,  3.84it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:07,  3.83it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.84it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:06,  3.74it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.61it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:06,  3.57it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  3.60it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:05,  3.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.57it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.64it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.64it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:04,  3.65it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:04,  3.66it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  3.65it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  3.69it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.72it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:02,  3.78it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  3.80it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  3.65it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.47it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:02,  3.43it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  3.49it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  3.53it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:01,  3.50it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.34it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  3.33it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  3.36it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4185845851898193, 'eval_runtime': 13.8469, 'eval_samples_per_second': 7.222, 'eval_steps_per_second': 3.611, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 60%|████████████████████████▍                | 100/168 [15:35<07:13,  6.38s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  3.38it/s]\u001b[A\n",
      "{'loss': 1.2691, 'grad_norm': 1.1756587028503418, 'learning_rate': 1.5866474390840125e-05, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 1.6241, 'grad_norm': 1.2155174016952515, 'learning_rate': 1.3733625605001365e-05, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [16:43<07:25,  7.69s/it][INFO|trainer.py:3788] 2024-07-26 04:29:34,661 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:29:34,661 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:29:34,661 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.24it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.46it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:15,  2.91it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.73it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.58it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.56it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.60it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.55it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.57it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.59it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.53it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.52it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:13,  2.59it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.61it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.63it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:12,  2.56it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.54it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.55it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.52it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.48it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.42it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.46it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.51it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.66it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.69it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.58it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:08,  2.51it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:08,  2.46it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:07,  2.47it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:06,  2.61it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:06,  2.63it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:06,  2.55it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:06,  2.46it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:05,  2.43it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:05,  2.31it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:05,  2.27it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:15<00:04,  2.39it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:04,  2.45it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:03,  2.50it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:16<00:03,  2.61it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:16<00:02,  2.65it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:17<00:02,  2.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:17<00:01,  2.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:17<00:01,  2.58it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:18<00:01,  2.58it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:18<00:00,  2.64it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:18<00:00,  2.62it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4220696687698364, 'eval_runtime': 19.7896, 'eval_samples_per_second': 5.053, 'eval_steps_per_second': 2.527, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 65%|██████████████████████████▊              | 110/168 [17:03<07:25,  7.69s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:19<00:00,  2.54it/s]\u001b[A\n",
      "{'loss': 1.4111, 'grad_norm': 1.151536464691162, 'learning_rate': 1.1699198087116589e-05, 'epoch': 2.04, 'num_input_tokens_seen': 648528}\n",
      "{'loss': 1.3927, 'grad_norm': 0.7659660577774048, 'learning_rate': 9.780964274781984e-06, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [18:21<06:28,  8.09s/it][INFO|trainer.py:3788] 2024-07-26 04:31:12,284 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:31:12,284 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:31:12,284 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.06it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:14,  3.33it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.09it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.43it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.30it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.38it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.42it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.50it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:15,  2.44it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:17,  2.21it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.34it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.40it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:14,  2.34it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.35it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.29it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.27it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.39it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.62it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:09,  2.81it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:09,  2.87it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:08,  2.91it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:08,  3.07it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:07,  3.19it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:07,  3.09it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:07,  3.11it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:06,  3.22it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:06,  3.09it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:06,  3.07it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:05,  3.17it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:05,  3.13it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:04,  3.20it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:04,  3.25it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:04,  3.22it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:04,  3.15it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:03,  3.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:03,  3.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  3.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:02,  3.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  3.11it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  3.12it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:01,  3.12it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  3.32it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  3.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:00,  3.46it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  3.46it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  3.55it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4226455688476562, 'eval_runtime': 17.7099, 'eval_samples_per_second': 5.647, 'eval_steps_per_second': 2.823, 'epoch': 2.13, 'num_input_tokens_seen': 675168}\n",
      " 71%|█████████████████████████████▎           | 120/168 [18:38<06:28,  8.09s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  3.48it/s]\u001b[A\n",
      "{'loss': 1.1758, 'grad_norm': 1.136338233947754, 'learning_rate': 7.99568155572701e-06, 'epoch': 2.22, 'num_input_tokens_seen': 704912}\n",
      "{'loss': 1.4245, 'grad_norm': 1.0480098724365234, 'learning_rate': 6.358945877920861e-06, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [19:50<05:06,  8.06s/it][INFO|trainer.py:3788] 2024-07-26 04:32:41,235 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:32:41,235 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:32:41,235 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.32it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:13,  3.52it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.87it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.61it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.58it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:16,  2.56it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.55it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:15,  2.62it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.70it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.66it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.67it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.78it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:12,  2.78it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.65it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.53it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.48it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:12,  2.50it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.57it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.68it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:10,  2.66it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:10,  2.59it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.53it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.46it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.56it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:09,  2.59it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:08,  2.59it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.53it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:08,  2.58it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.57it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:07,  2.62it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:06,  2.85it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:05,  3.00it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:05,  3.14it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:04,  3.33it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:04,  3.41it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:03,  3.43it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:03,  3.45it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:03,  3.54it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:02,  3.62it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:02,  3.63it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:02,  3.62it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:01,  3.62it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:01,  3.55it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  3.60it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:15<00:01,  3.63it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:00,  3.67it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  3.76it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:16<00:00,  3.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4226710796356201, 'eval_runtime': 17.4199, 'eval_samples_per_second': 5.741, 'eval_steps_per_second': 2.87, 'epoch': 2.31, 'num_input_tokens_seen': 732944}\n",
      " 77%|███████████████████████████████▋         | 130/168 [20:07<05:06,  8.06s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:16<00:00,  3.63it/s]\u001b[A\n",
      "{'loss': 1.4361, 'grad_norm': 1.1590654850006104, 'learning_rate': 4.885055505833291e-06, 'epoch': 2.4, 'num_input_tokens_seen': 760832}\n",
      "{'loss': 1.4051, 'grad_norm': 1.119056224822998, 'learning_rate': 3.586886113053717e-06, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [21:20<03:43,  8.00s/it][INFO|trainer.py:3788] 2024-07-26 04:34:11,783 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:34:11,783 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:34:11,783 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:10,  4.78it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.68it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:15,  2.90it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.52it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:18,  2.34it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:18,  2.36it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:16,  2.50it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:16,  2.52it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:15,  2.52it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.40it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.29it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:16,  2.27it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.32it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.32it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.41it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:13,  2.37it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:13,  2.36it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:08<00:12,  2.37it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:12,  2.33it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.40it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:10,  2.46it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.41it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:10<00:09,  2.60it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:08,  2.82it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:07,  3.06it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:07,  3.09it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:06,  3.29it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:05,  3.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:05,  3.43it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:05,  3.53it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:12<00:04,  3.61it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:04,  3.64it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:04,  3.70it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:13<00:03,  3.65it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:03,  3.74it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:03,  3.51it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:14<00:03,  3.27it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  3.14it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.98it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  2.95it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.90it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:02,  2.91it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  2.97it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.90it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:01,  2.73it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  2.56it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  2.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4227534532546997, 'eval_runtime': 18.4966, 'eval_samples_per_second': 5.406, 'eval_steps_per_second': 2.703, 'epoch': 2.49, 'num_input_tokens_seen': 788928}\n",
      " 83%|██████████████████████████████████▏      | 140/168 [21:39<03:43,  8.00s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  2.56it/s]\u001b[A\n",
      "{'loss': 1.2391, 'grad_norm': 1.0313140153884888, 'learning_rate': 2.475778302439524e-06, 'epoch': 2.58, 'num_input_tokens_seen': 818384}\n",
      "{'loss': 1.7095, 'grad_norm': 1.239406704902649, 'learning_rate': 1.5614385364000228e-06, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [22:58<02:31,  8.39s/it][INFO|trainer.py:3788] 2024-07-26 04:35:49,200 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:35:49,201 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:35:49,201 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:09,  5.16it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.63it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:16,  2.86it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:17,  2.62it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:02<00:17,  2.54it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:17,  2.45it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:03<00:17,  2.36it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:03<00:17,  2.39it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:16,  2.36it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:04<00:16,  2.35it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:16,  2.28it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:05<00:15,  2.34it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:05<00:15,  2.30it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:06<00:15,  2.28it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:06<00:14,  2.31it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:14,  2.33it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:07<00:12,  2.50it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:11,  2.64it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.63it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:08<00:11,  2.56it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:11,  2.54it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:09<00:10,  2.58it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:09,  2.61it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:09,  2.58it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:10<00:09,  2.55it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:09,  2.47it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:11<00:09,  2.36it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:11<00:08,  2.44it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:07,  2.51it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:12<00:07,  2.59it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:12<00:06,  2.59it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:13<00:06,  2.57it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:13<00:06,  2.59it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:13<00:05,  2.66it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:14<00:05,  2.64it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:14<00:04,  2.76it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:14<00:04,  2.93it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:15<00:03,  3.11it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:15<00:03,  3.29it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:15<00:02,  3.41it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:15<00:02,  3.55it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:16<00:01,  3.55it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:16<00:01,  3.65it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:16<00:01,  3.68it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  3.65it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:17<00:00,  3.68it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  3.65it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  3.69it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4210102558135986, 'eval_runtime': 18.5052, 'eval_samples_per_second': 5.404, 'eval_steps_per_second': 2.702, 'epoch': 2.67, 'num_input_tokens_seen': 851456}\n",
      " 89%|████████████████████████████████████▌    | 150/168 [23:16<02:31,  8.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  3.64it/s]\u001b[A\n",
      "{'loss': 1.5216, 'grad_norm': 1.1500983238220215, 'learning_rate': 8.51854342773295e-07, 'epoch': 2.76, 'num_input_tokens_seen': 879696}\n",
      "{'loss': 1.1141, 'grad_norm': 0.9756461977958679, 'learning_rate': 3.5322453704410286e-07, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [24:31<00:59,  7.38s/it][INFO|trainer.py:3788] 2024-07-26 04:37:22,389 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:37:22,389 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:37:22,389 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:06,  6.94it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:09,  4.85it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:10,  4.35it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:10,  4.09it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:11,  3.99it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:10,  3.98it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:01<00:10,  3.92it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:10,  3.88it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.93it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:10,  3.87it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:02<00:10,  3.71it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.60it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.64it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:03<00:09,  3.71it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.73it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  3.75it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.64it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:04<00:08,  3.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.68it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  3.68it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:07,  3.63it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:05<00:07,  3.77it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:06,  3.83it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  3.84it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:06,  3.91it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:06<00:05,  3.92it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:05,  3.90it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  3.77it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:07<00:05,  3.59it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.58it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.72it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.75it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:08<00:04,  3.74it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  3.76it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  3.74it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  3.57it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.33it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:03,  3.26it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:03,  3.22it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  3.10it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.22it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:02,  3.41it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  3.45it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  3.36it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:01,  3.18it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  3.09it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:13<00:00,  3.11it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:13<00:00,  3.06it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4217712879180908, 'eval_runtime': 14.196, 'eval_samples_per_second': 7.044, 'eval_steps_per_second': 3.522, 'epoch': 2.84, 'num_input_tokens_seen': 902976}\n",
      " 95%|███████████████████████████████████████  | 160/168 [24:45<00:59,  7.38s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  2.99it/s]\u001b[A\n",
      "{'loss': 1.253, 'grad_norm': 1.524792194366455, 'learning_rate': 6.990507047049676e-08, 'epoch': 2.93, 'num_input_tokens_seen': 930944}\n",
      "100%|█████████████████████████████████████████| 168/168 [25:53<00:00,  8.89s/it][INFO|trainer.py:3478] 2024-07-26 04:38:44,229 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:38:44,299 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:38:44,300 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:38:45,113 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:38:45,129 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/checkpoint-168/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 04:38:47,491 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1556.4748, 'train_samples_per_second': 1.735, 'train_steps_per_second': 0.108, 'train_loss': 1.4180742104848225, 'epoch': 2.99, 'num_input_tokens_seen': 950512}\n",
      "100%|█████████████████████████████████████████| 168/168 [25:56<00:00,  9.26s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 04:38:47,505 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:38:47,651 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:38:47,653 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:38:48,383 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:38:48,394 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     2.9867\n",
      "  num_input_tokens_seen    =     950512\n",
      "  total_flos               =  1924305GF\n",
      "  train_loss               =     1.4181\n",
      "  train_runtime            = 0:25:56.47\n",
      "  train_samples_per_second =      1.735\n",
      "  train_steps_per_second   =      0.108\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 04:38:49,989 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:38:49,989 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:38:49,989 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:18<00:00,  2.75it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     2.9867\n",
      "  eval_loss               =     1.4213\n",
      "  eval_runtime            = 0:00:18.57\n",
      "  eval_samples_per_second =      5.383\n",
      "  eval_steps_per_second   =      2.692\n",
      "  num_input_tokens_seen   =     950512\n",
      "[INFO|modelcard.py:449] 2024-07-26 04:39:08,647 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51dbb0-061b-4ac3-947d-ec031f2f19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 04:40:07,810] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 04:40:29 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "07/26/2024 04:40:29 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:40:29,335 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 04:40:29,747 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 04:40:29 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 04:40:29 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 04:40:35 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:40:36,589 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:40:36,599 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "07/26/2024 04:40:36 - INFO - llamafactory.model.model_utils.quantization - Quantizing model to 8 bit.\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 04:40:36,958 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 04:40:37,567 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:40:37,572 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:40:51,455 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:40:51,455 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:40:51,480 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:40:51,481 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:40:51 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:40:51 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:40:51 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:40:51 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "07/26/2024 04:40:51 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,down_proj,o_proj,gate_proj,up_proj,v_proj\n",
      "07/26/2024 04:40:52 - INFO - llamafactory.model.loader - trainable params: 4399104 || all params: 498431872 || trainable%: 0.8826\n",
      "[INFO|trainer.py:642] 2024-07-26 04:40:52,183 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:40:53,330 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:40:53,330 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:40:53,330 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:40:53,330 >>   Instantaneous batch size per device = 6\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:40:53,330 >>   Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:40:53,330 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:40:53,330 >>   Total optimization steps = 54\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:40:53,336 >>   Number of trainable parameters = 4,399,104\n",
      "{'loss': 2.0523, 'grad_norm': 1.24844229221344, 'learning_rate': 4.894973780788722e-05, 'epoch': 0.27, 'num_input_tokens_seen': 118128}\n",
      "{'loss': 1.9091, 'grad_norm': 1.1065477132797241, 'learning_rate': 4.588719528532342e-05, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\n",
      " 19%|███████▉                                   | 10/54 [01:35<06:15,  8.53s/it][INFO|trainer.py:3788] 2024-07-26 04:42:29,075 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:42:29,075 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:42:29,075 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.65it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.16it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:12,  3.57it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.30it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:13,  3.25it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:13,  3.24it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.16it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  3.05it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.05it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.79it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:15,  2.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:14,  2.61it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:13,  2.70it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:12,  2.73it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:12,  2.70it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.80it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:10,  2.91it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  3.01it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:09,  3.02it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:10,  2.88it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.68it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:09,  2.75it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:08,  2.90it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:08,  2.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:08,  2.99it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:07,  2.91it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:08,  2.70it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:07,  2.78it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:07,  2.67it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:06,  2.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:06,  2.74it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:05,  2.79it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.80it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:12<00:05,  2.74it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:12<00:04,  2.60it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.60it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:04,  2.61it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.51it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.58it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:03,  2.61it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  2.64it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:02,  2.70it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  2.65it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:16<00:01,  2.66it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:01,  2.55it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:17<00:00,  2.45it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:17<00:00,  2.45it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.5301928520202637, 'eval_runtime': 18.7041, 'eval_samples_per_second': 5.346, 'eval_steps_per_second': 2.673, 'epoch': 0.53, 'num_input_tokens_seen': 237552}\n",
      " 19%|███████▉                                   | 10/54 [01:54<06:15,  8.53s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  2.48it/s]\u001b[A\n",
      "{'loss': 1.6421, 'grad_norm': 0.9165357947349548, 'learning_rate': 4.1069690242163484e-05, 'epoch': 0.8, 'num_input_tokens_seen': 350880}\n",
      "{'loss': 1.8243, 'grad_norm': 0.8573377132415771, 'learning_rate': 3.490199415097892e-05, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\n",
      " 37%|███████████████▉                           | 20/54 [03:20<04:55,  8.68s/it][INFO|trainer.py:3788] 2024-07-26 04:44:13,697 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:44:13,697 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:44:13,697 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.32it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:11,  4.08it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:13,  3.48it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:14,  3.21it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.05it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:15,  2.82it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:15,  2.68it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:14,  2.75it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:14,  2.61it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:15,  2.41it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:14,  2.56it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:05<00:13,  2.54it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:13,  2.48it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:06<00:13,  2.42it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:06<00:13,  2.32it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:07<00:12,  2.46it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:07<00:11,  2.55it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:07<00:11,  2.50it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:08<00:10,  2.48it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:09<00:10,  2.48it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:09<00:10,  2.38it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:09<00:09,  2.42it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:10<00:09,  2.52it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:10<00:08,  2.55it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:10<00:07,  2.84it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:11<00:06,  3.03it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:11<00:06,  3.16it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:05,  3.30it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:04,  3.41it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:12<00:04,  3.60it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:04,  3.53it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:12<00:04,  3.43it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:03,  3.27it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:03,  3.17it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:03,  3.21it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  3.20it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:02,  3.16it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:02,  3.18it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  3.29it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:01,  3.26it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  3.17it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:15<00:01,  3.20it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:00,  3.09it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  3.16it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:16<00:00,  3.22it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.460422396659851, 'eval_runtime': 17.6148, 'eval_samples_per_second': 5.677, 'eval_steps_per_second': 2.839, 'epoch': 1.07, 'num_input_tokens_seen': 476208}\n",
      " 37%|███████████████▉                           | 20/54 [03:37<04:55,  8.68s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:17<00:00,  3.17it/s]\u001b[A\n",
      "{'loss': 1.7642, 'grad_norm': 0.8137511610984802, 'learning_rate': 2.7902322853130757e-05, 'epoch': 1.33, 'num_input_tokens_seen': 597600}\n",
      "{'loss': 1.6443, 'grad_norm': 0.7176933884620667, 'learning_rate': 2.0658795558326743e-05, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\n",
      " 56%|███████████████████████▉                   | 30/54 [04:55<03:19,  8.32s/it][INFO|trainer.py:3788] 2024-07-26 04:45:48,806 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:45:48,807 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:45:48,807 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:08,  5.87it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:12,  3.78it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:01<00:14,  3.28it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:15,  2.99it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:15,  2.90it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.96it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:13,  3.19it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:12,  3.37it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:11,  3.53it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:10,  3.69it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.79it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:09,  3.94it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.99it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:08,  3.99it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:08,  4.07it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:07,  4.14it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.94it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.78it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:07,  3.81it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:08,  3.53it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:06<00:07,  3.53it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.46it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:07,  3.50it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:07,  3.53it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:07<00:06,  3.59it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.66it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:06,  3.66it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:08<00:05,  3.57it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.37it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.24it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:09<00:05,  3.31it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:09<00:05,  3.35it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  3.31it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:04,  3.31it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:10<00:04,  3.32it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:10<00:04,  3.16it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.28it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:11<00:03,  3.39it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:11<00:03,  3.31it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:11<00:02,  3.30it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:12<00:02,  3.21it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:12<00:02,  3.25it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:12<00:01,  3.18it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:13<00:01,  3.08it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:13<00:01,  2.96it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:13<00:01,  2.76it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:14<00:00,  2.70it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:14<00:00,  2.61it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4454376697540283, 'eval_runtime': 15.456, 'eval_samples_per_second': 6.47, 'eval_steps_per_second': 3.235, 'epoch': 1.6, 'num_input_tokens_seen': 714048}\n",
      " 56%|███████████████████████▉                   | 30/54 [05:10<03:19,  8.32s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  2.44it/s]\u001b[A\n",
      "{'loss': 1.615, 'grad_norm': 0.8330691456794739, 'learning_rate': 1.3780020494988446e-05, 'epoch': 1.87, 'num_input_tokens_seen': 824496}\n",
      "{'loss': 1.7834, 'grad_norm': 0.8355152010917664, 'learning_rate': 7.843959053281663e-06, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\n",
      " 74%|███████████████████████████████▊           | 40/54 [06:21<01:40,  7.19s/it][INFO|trainer.py:3788] 2024-07-26 04:47:14,406 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:47:14,406 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:47:14,406 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:06,  7.08it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:10,  4.61it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:12,  3.81it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:13,  3.39it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:14,  3.04it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:02<00:14,  2.89it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:14,  2.92it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:13,  2.96it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:03<00:13,  3.06it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:03<00:13,  2.97it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:13,  2.88it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:04<00:13,  2.82it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:04<00:12,  2.88it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:11,  2.98it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:05<00:11,  3.03it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:05<00:11,  2.98it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:05<00:10,  3.01it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:06<00:10,  3.06it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:06<00:09,  3.07it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:06<00:09,  2.94it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:07<00:10,  2.79it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:07<00:10,  2.55it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:08<00:10,  2.43it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:08<00:10,  2.47it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:08<00:09,  2.57it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:09<00:08,  2.72it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:09<00:07,  2.76it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:09<00:07,  2.73it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:10<00:07,  2.61it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:10<00:07,  2.53it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:11<00:07,  2.56it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:11<00:06,  2.61it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:11<00:05,  2.67it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:12<00:05,  2.75it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:12<00:05,  2.63it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:13<00:05,  2.55it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:13<00:04,  2.53it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:13<00:04,  2.49it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:14<00:03,  2.52it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:14<00:03,  2.63it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:14<00:02,  2.69it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:15<00:02,  3.03it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:15<00:01,  3.28it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:15<00:01,  3.49it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:15<00:01,  3.64it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:16<00:00,  3.82it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:16<00:00,  3.97it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:16<00:00,  4.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4364080429077148, 'eval_runtime': 17.1419, 'eval_samples_per_second': 5.834, 'eval_steps_per_second': 2.917, 'epoch': 2.13, 'num_input_tokens_seen': 947040}\n",
      " 74%|███████████████████████████████▊           | 40/54 [06:38<01:40,  7.19s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:16<00:00,  4.11it/s]\u001b[A\n",
      "{'loss': 1.6418, 'grad_norm': 0.8318832516670227, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1065840}\n",
      "{'loss': 1.6919, 'grad_norm': 0.8282485604286194, 'learning_rate': 6.738782355044049e-07, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [07:48<00:29,  7.42s/it][INFO|trainer.py:3788] 2024-07-26 04:48:41,766 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:48:41,766 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:48:41,766 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  4%|█▊                                          | 2/50 [00:00<00:07,  6.81it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:09,  4.70it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:11,  3.99it/s]\u001b[A\n",
      " 10%|████▍                                       | 5/50 [00:01<00:12,  3.66it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:01<00:12,  3.46it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:01<00:12,  3.44it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:02<00:11,  3.54it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:02<00:11,  3.62it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:02<00:10,  3.64it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:02<00:10,  3.64it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:03<00:10,  3.65it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:03<00:10,  3.67it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:03<00:09,  3.67it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:04<00:09,  3.67it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:04<00:09,  3.70it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:04<00:08,  3.74it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:04<00:08,  3.66it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:05<00:08,  3.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:05<00:08,  3.66it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:05<00:07,  3.69it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:05<00:07,  3.61it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:06<00:07,  3.66it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:06<00:07,  3.70it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:06<00:06,  3.70it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:06<00:06,  3.71it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:07<00:06,  3.66it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:07<00:06,  3.64it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:07<00:05,  3.70it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:08<00:05,  3.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:08<00:05,  3.74it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:08<00:04,  3.86it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:08<00:04,  3.92it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:09<00:04,  3.99it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:09<00:03,  4.01it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:09<00:03,  3.93it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:09<00:03,  3.81it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:10<00:03,  3.75it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:10<00:03,  3.63it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:10<00:02,  3.59it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:10<00:02,  3.65it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:11<00:02,  3.67it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:11<00:01,  3.75it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:11<00:01,  3.81it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:12<00:01,  3.88it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:12<00:00,  4.12it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:12<00:00,  4.11it/s]\u001b[A\n",
      " 96%|█████████████████████████████████████████▎ | 48/50 [00:12<00:00,  4.07it/s]\u001b[A\n",
      " 98%|██████████████████████████████████████████▏| 49/50 [00:12<00:00,  4.04it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.4346227645874023, 'eval_runtime': 13.5182, 'eval_samples_per_second': 7.397, 'eval_steps_per_second': 3.699, 'epoch': 2.67, 'num_input_tokens_seen': 1189104}\n",
      " 93%|███████████████████████████████████████▊   | 50/54 [08:01<00:29,  7.42s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:13<00:00,  4.02it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████| 54/54 [08:27<00:00,  8.04s/it][INFO|trainer.py:3478] 2024-07-26 04:49:20,941 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/checkpoint-54\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:49:20,984 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:49:20,985 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:49:21,150 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/checkpoint-54/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:49:21,161 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/checkpoint-54/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 04:49:21,842 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 508.5068, 'train_samples_per_second': 5.31, 'train_steps_per_second': 0.106, 'train_loss': 1.749418020248413, 'epoch': 2.88, 'num_input_tokens_seen': 1278720}\n",
      "100%|███████████████████████████████████████████| 54/54 [08:28<00:00,  9.42s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 04:49:21,849 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:49:21,889 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:49:21,890 >> Model config Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:49:22,050 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:49:22,162 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =       2.88\n",
      "  num_input_tokens_seen    =    1278720\n",
      "  total_flos               =  2588760GF\n",
      "  train_loss               =     1.7494\n",
      "  train_runtime            = 0:08:28.50\n",
      "  train_samples_per_second =       5.31\n",
      "  train_steps_per_second   =      0.106\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 04:49:22,839 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:49:22,839 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:49:22,839 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:15<00:00,  3.15it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =       2.88\n",
      "  eval_loss               =     1.4343\n",
      "  eval_runtime            = 0:00:16.19\n",
      "  eval_samples_per_second =      6.176\n",
      "  eval_steps_per_second   =      3.088\n",
      "  num_input_tokens_seen   =    1278720\n",
      "[INFO|modelcard.py:449] 2024-07-26 04:49:39,122 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type lora \\\n",
    "    --quantization_bit 8 \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 6 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_qlora_bz=6 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0 \\\n",
    "    --lora_target all \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f89eaab-2d26-4c99-a2c5-9aafbfe709fb",
   "metadata": {},
   "source": [
    "full-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719ba66-c0b7-4012-b47e-812699f754e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 04:50:24,348] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 04:50:32 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:50:32,865 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 04:50:33,170 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 04:50:33 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 04:50:33 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 04:50:39 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:50:40,084 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:50:40,093 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 04:50:40,229 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 04:50:40,781 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:50:40,786 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:50:52,712 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:50:52,713 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:50:52,722 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:50:52,723 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:50:52 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:50:52 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:50:52 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:50:52 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\n",
      "07/26/2024 04:50:53 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\n",
      "[INFO|trainer.py:642] 2024-07-26 04:50:53,213 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:50:53,681 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:50:53,681 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:50:53,681 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:50:53,681 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:50:53,681 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:50:53,681 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:50:53,681 >>   Total optimization steps = 224\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:50:53,682 >>   Number of trainable parameters = 494,032,768\n",
      "{'loss': 1.5686, 'grad_norm': 19.236539840698242, 'learning_rate': 4.9977874547522175e-05, 'epoch': 0.04, 'num_input_tokens_seen': 9456}\n",
      "{'loss': 1.3648, 'grad_norm': 18.7326602935791, 'learning_rate': 4.984280524733107e-05, 'epoch': 0.09, 'num_input_tokens_seen': 19936}\n",
      "  4%|█▉                                        | 10/224 [00:29<05:18,  1.49s/it][INFO|trainer.py:3788] 2024-07-26 04:51:22,722 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:51:22,723 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:51:22,723 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 28.01it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 26.54it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 25.39it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 25.13it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 23.87it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 24.76it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 24.57it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:00<00:01, 23.55it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 23.56it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 24.24it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 24.65it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 24.75it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 24.57it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 24.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 24.89it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.707553744316101, 'eval_runtime': 2.2092, 'eval_samples_per_second': 45.266, 'eval_steps_per_second': 22.633, 'epoch': 0.09, 'num_input_tokens_seen': 19936}\n",
      "  4%|█▉                                        | 10/224 [00:31<05:18,  1.49s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.38it/s]\u001b[A\n",
      "{'loss': 1.2015, 'grad_norm': 22.45205307006836, 'learning_rate': 4.9585621689151216e-05, 'epoch': 0.13, 'num_input_tokens_seen': 29072}\n",
      "{'loss': 1.301, 'grad_norm': 14.346552848815918, 'learning_rate': 4.9207588053056545e-05, 'epoch': 0.18, 'num_input_tokens_seen': 39056}\n",
      "  9%|███▊                                      | 20/224 [00:44<04:43,  1.39s/it][INFO|trainer.py:3788] 2024-07-26 04:51:38,190 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:51:38,190 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:51:38,190 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.54it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 23.87it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 22.51it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 21.77it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 21.43it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 21.82it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 19.50it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 18.48it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 19.24it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 18.57it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 18.84it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 19.37it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 19.17it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 18.78it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:02<00:00, 19.25it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 19.97it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7463680505752563, 'eval_runtime': 2.5383, 'eval_samples_per_second': 39.397, 'eval_steps_per_second': 19.698, 'epoch': 0.18, 'num_input_tokens_seen': 39056}\n",
      "  9%|███▊                                      | 20/224 [00:47<04:43,  1.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 19.50it/s]\u001b[A\n",
      "{'loss': 1.546, 'grad_norm': 11.316555976867676, 'learning_rate': 4.871056255499757e-05, 'epoch': 0.22, 'num_input_tokens_seen': 49584}\n",
      "{'loss': 1.2103, 'grad_norm': 13.934386253356934, 'learning_rate': 4.8096988312782174e-05, 'epoch': 0.27, 'num_input_tokens_seen': 58976}\n",
      " 13%|█████▋                                    | 30/224 [01:01<04:53,  1.51s/it][INFO|trainer.py:3788] 2024-07-26 04:51:55,235 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:51:55,235 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:51:55,235 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 28.17it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 23.15it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 22.85it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 22.09it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 20.90it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 21.84it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 22.80it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 23.11it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 22.88it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 23.44it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 23.17it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.44it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 23.55it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 22.76it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 21.19it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7307214736938477, 'eval_runtime': 2.3415, 'eval_samples_per_second': 42.707, 'eval_steps_per_second': 21.353, 'epoch': 0.27, 'num_input_tokens_seen': 58976}\n",
      " 13%|█████▋                                    | 30/224 [01:03<04:53,  1.51s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 19.82it/s]\u001b[A\n",
      "{'loss': 1.3155, 'grad_norm': 11.967639923095703, 'learning_rate': 4.7369881336984153e-05, 'epoch': 0.31, 'num_input_tokens_seen': 68880}\n",
      "{'loss': 1.5216, 'grad_norm': 14.748614311218262, 'learning_rate': 4.653281570581023e-05, 'epoch': 0.36, 'num_input_tokens_seen': 79536}\n",
      " 18%|███████▌                                  | 40/224 [01:18<04:35,  1.50s/it][INFO|trainer.py:3788] 2024-07-26 04:52:12,320 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:52:12,321 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:52:12,321 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 28.95it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 24.34it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 18.43it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 19.63it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 20.39it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 20.69it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 20.28it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 19.91it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 20.00it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 19.20it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 19.53it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 18.73it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 19.73it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:02<00:00, 19.79it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 20.62it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 19.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7377315759658813, 'eval_runtime': 2.5574, 'eval_samples_per_second': 39.102, 'eval_steps_per_second': 19.551, 'epoch': 0.36, 'num_input_tokens_seen': 79536}\n",
      " 18%|███████▌                                  | 40/224 [01:21<04:35,  1.50s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 20.87it/s]\u001b[A\n",
      "{'loss': 1.25, 'grad_norm': 12.282526016235352, 'learning_rate': 4.558990599679787e-05, 'epoch': 0.4, 'num_input_tokens_seen': 89136}\n",
      "{'loss': 1.1755, 'grad_norm': 7.637223720550537, 'learning_rate': 4.454578706170075e-05, 'epoch': 0.44, 'num_input_tokens_seen': 98072}\n",
      " 22%|█████████▍                                | 50/224 [01:36<04:26,  1.53s/it][INFO|trainer.py:3788] 2024-07-26 04:52:30,203 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:52:30,204 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:52:30,204 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 27.63it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 20.04it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 20.25it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:02, 18.02it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 18.16it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 18.53it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 18.05it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:01<00:01, 16.99it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 17.75it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 19.21it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 19.15it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 18.79it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 19.37it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 18.70it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 17.94it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 18.95it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 19.13it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 19.76it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 19.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.756385087966919, 'eval_runtime': 2.8334, 'eval_samples_per_second': 35.294, 'eval_steps_per_second': 17.647, 'epoch': 0.44, 'num_input_tokens_seen': 98072}\n",
      " 22%|█████████▍                                | 50/224 [01:39<04:26,  1.53s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 19.02it/s]\u001b[A\n",
      "{'loss': 1.3622, 'grad_norm': 8.223040580749512, 'learning_rate': 4.3405591243977736e-05, 'epoch': 0.49, 'num_input_tokens_seen': 107808}\n",
      "{'loss': 1.4456, 'grad_norm': 13.597573280334473, 'learning_rate': 4.2174923150872544e-05, 'epoch': 0.53, 'num_input_tokens_seen': 117464}\n",
      " 27%|███████████▎                              | 60/224 [01:53<03:51,  1.41s/it][INFO|trainer.py:3788] 2024-07-26 04:52:47,633 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:52:47,634 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:52:47,634 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.97it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 22.70it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 23.99it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 23.38it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 23.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 22.72it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 21.67it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 21.24it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 21.53it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 21.44it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 21.61it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 22.75it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 22.53it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.40it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 23.90it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7723463773727417, 'eval_runtime': 2.3246, 'eval_samples_per_second': 43.018, 'eval_steps_per_second': 21.509, 'epoch': 0.53, 'num_input_tokens_seen': 117464}\n",
      " 27%|███████████▎                              | 60/224 [01:56<03:51,  1.41s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.04it/s]\u001b[A\n",
      "{'loss': 1.9646, 'grad_norm': 13.856877326965332, 'learning_rate': 4.085983210409114e-05, 'epoch': 0.58, 'num_input_tokens_seen': 131088}\n",
      "{'loss': 1.1132, 'grad_norm': 9.365686416625977, 'learning_rate': 3.946678240449515e-05, 'epoch': 0.62, 'num_input_tokens_seen': 139944}\n",
      " 31%|█████████████▏                            | 70/224 [02:08<03:17,  1.28s/it][INFO|trainer.py:3788] 2024-07-26 04:53:02,359 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:53:02,359 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:53:02,359 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.68it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.36it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 27.66it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 25.19it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 25.80it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.47it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.97it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.21it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.62it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 24.23it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 24.72it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 25.12it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 25.38it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.33it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.03it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7205430269241333, 'eval_runtime': 2.047, 'eval_samples_per_second': 48.851, 'eval_steps_per_second': 24.426, 'epoch': 0.62, 'num_input_tokens_seen': 139944}\n",
      " 31%|█████████████▏                            | 70/224 [02:10<03:17,  1.28s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 25.03it/s]\u001b[A\n",
      "{'loss': 1.1283, 'grad_norm': 11.306074142456055, 'learning_rate': 3.8002621556974367e-05, 'epoch': 0.67, 'num_input_tokens_seen': 148720}\n",
      "{'loss': 1.2376, 'grad_norm': 10.105889320373535, 'learning_rate': 3.6474546611688445e-05, 'epoch': 0.71, 'num_input_tokens_seen': 157864}\n",
      " 36%|███████████████                           | 80/224 [02:23<03:00,  1.25s/it][INFO|trainer.py:3788] 2024-07-26 04:53:16,924 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:53:16,924 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:53:16,924 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 23.67it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 23.90it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 24.87it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 23.92it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 22.74it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 23.62it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 24.12it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 24.08it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 24.85it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 25.38it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 25.73it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 25.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 25.46it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 24.92it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 25.11it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7430386543273926, 'eval_runtime': 2.1043, 'eval_samples_per_second': 47.521, 'eval_steps_per_second': 23.76, 'epoch': 0.71, 'num_input_tokens_seen': 157864}\n",
      " 36%|███████████████                           | 80/224 [02:25<03:00,  1.25s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.13it/s]\u001b[A\n",
      "{'loss': 1.0965, 'grad_norm': 10.15137004852295, 'learning_rate': 3.489006878712647e-05, 'epoch': 0.76, 'num_input_tokens_seen': 166368}\n",
      "{'loss': 1.2109, 'grad_norm': 10.311064720153809, 'learning_rate': 3.358710195908653e-05, 'epoch': 0.8, 'num_input_tokens_seen': 176504}\n",
      " 40%|████████████████▉                         | 90/224 [02:37<02:49,  1.27s/it][INFO|trainer.py:3788] 2024-07-26 04:53:31,395 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:53:31,395 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:53:31,395 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 29.92it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 27.23it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 24.90it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 23.62it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 22.56it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 23.51it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 24.01it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 22.93it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 24.17it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 25.05it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 23.04it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.51it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 23.89it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 23.95it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 23.46it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.758541226387024, 'eval_runtime': 2.1648, 'eval_samples_per_second': 46.194, 'eval_steps_per_second': 23.097, 'epoch': 0.8, 'num_input_tokens_seen': 176504}\n",
      " 40%|████████████████▉                         | 90/224 [02:39<02:49,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 22.50it/s]\u001b[A\n",
      "{'loss': 1.5354, 'grad_norm': 21.98957633972168, 'learning_rate': 3.1920887785621235e-05, 'epoch': 0.84, 'num_input_tokens_seen': 187960}\n",
      "{'loss': 1.4653, 'grad_norm': 11.157663345336914, 'learning_rate': 3.022065414180425e-05, 'epoch': 0.89, 'num_input_tokens_seen': 199240}\n",
      " 45%|██████████████████▎                      | 100/224 [02:53<02:57,  1.43s/it][INFO|trainer.py:3788] 2024-07-26 04:53:46,735 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:53:46,735 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:53:46,736 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 26.65it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:02, 19.72it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:02, 16.96it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:02, 17.68it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:02, 17.79it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 18.14it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 18.50it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:01<00:01, 17.99it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 17.45it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 16.67it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 16.56it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 17.19it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:01, 17.68it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:01, 17.99it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 18.09it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:02<00:00, 18.08it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:02<00:00, 18.01it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:02<00:00, 18.14it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 17.89it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:02<00:00, 17.26it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 17.85it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7235772609710693, 'eval_runtime': 2.8598, 'eval_samples_per_second': 34.968, 'eval_steps_per_second': 17.484, 'epoch': 0.89, 'num_input_tokens_seen': 199240}\n",
      " 45%|██████████████████▎                      | 100/224 [02:55<02:57,  1.43s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 18.35it/s]\u001b[A\n",
      "{'loss': 1.6783, 'grad_norm': 10.421029090881348, 'learning_rate': 2.849475848838749e-05, 'epoch': 0.93, 'num_input_tokens_seen': 212032}\n",
      "{'loss': 1.3041, 'grad_norm': 9.643231391906738, 'learning_rate': 2.6751684427161683e-05, 'epoch': 0.98, 'num_input_tokens_seen': 222608}\n",
      " 49%|████████████████████▏                    | 110/224 [03:09<02:38,  1.39s/it][INFO|trainer.py:3788] 2024-07-26 04:54:03,122 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:54:03,122 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:54:03,122 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.58it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 25.52it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 24.37it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.42it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 21.22it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 20.58it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 21.49it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.53it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 23.55it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 23.78it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 22.23it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 22.90it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 22.24it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 22.99it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 22.10it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7023245096206665, 'eval_runtime': 2.3407, 'eval_samples_per_second': 42.723, 'eval_steps_per_second': 21.361, 'epoch': 0.98, 'num_input_tokens_seen': 222608}\n",
      " 49%|████████████████████▏                    | 110/224 [03:11<02:38,  1.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 19.81it/s]\u001b[A\n",
      "{'loss': 1.0173, 'grad_norm': 33.992496490478516, 'learning_rate': 2.5e-05, 'epoch': 1.02, 'num_input_tokens_seen': 232120}\n",
      "{'loss': 0.6973, 'grad_norm': 7.975135803222656, 'learning_rate': 2.3248315572838316e-05, 'epoch': 1.07, 'num_input_tokens_seen': 243576}\n",
      " 54%|█████████████████████▉                   | 120/224 [03:24<02:18,  1.33s/it][INFO|trainer.py:3788] 2024-07-26 04:54:18,343 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:54:18,343 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:54:18,343 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 30.84it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.30it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 21.79it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 21.19it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 20.93it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 20.67it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:01<00:01, 21.10it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 20.98it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 21.25it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 21.83it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 22.31it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 22.91it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.30it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.80it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 23.56it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.737117886543274, 'eval_runtime': 2.2791, 'eval_samples_per_second': 43.878, 'eval_steps_per_second': 21.939, 'epoch': 1.07, 'num_input_tokens_seen': 243576}\n",
      " 54%|█████████████████████▉                   | 120/224 [03:26<02:18,  1.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.41it/s]\u001b[A\n",
      "{'loss': 0.6093, 'grad_norm': 7.592122554779053, 'learning_rate': 2.1505241511612522e-05, 'epoch': 1.11, 'num_input_tokens_seen': 253248}\n",
      "{'loss': 0.6522, 'grad_norm': 7.880817890167236, 'learning_rate': 1.977934585819576e-05, 'epoch': 1.16, 'num_input_tokens_seen': 263960}\n",
      " 58%|███████████████████████▊                 | 130/224 [03:39<02:02,  1.30s/it][INFO|trainer.py:3788] 2024-07-26 04:54:33,241 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:54:33,241 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:54:33,241 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.78it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 24.61it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 25.48it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 23.71it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 20.76it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 21.68it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 19.18it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 20.79it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 21.74it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 21.77it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 22.64it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 22.80it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 21.80it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 21.42it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 21.73it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.858147144317627, 'eval_runtime': 2.3384, 'eval_samples_per_second': 42.764, 'eval_steps_per_second': 21.382, 'epoch': 1.16, 'num_input_tokens_seen': 263960}\n",
      " 58%|███████████████████████▊                 | 130/224 [03:41<02:02,  1.30s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 21.83it/s]\u001b[A\n",
      "{'loss': 0.6291, 'grad_norm': 9.017938613891602, 'learning_rate': 1.8079112214378768e-05, 'epoch': 1.2, 'num_input_tokens_seen': 273448}\n",
      "{'loss': 0.7455, 'grad_norm': 18.690349578857422, 'learning_rate': 1.641289804091347e-05, 'epoch': 1.24, 'num_input_tokens_seen': 282776}\n",
      " 62%|█████████████████████████▋               | 140/224 [03:55<01:51,  1.33s/it][INFO|trainer.py:3788] 2024-07-26 04:54:49,411 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:54:49,412 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:54:49,412 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.11it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 23.73it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 24.20it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.94it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.62it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.67it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.68it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 24.20it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.50it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 24.36it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.91it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 24.23it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 24.13it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 24.41it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 24.53it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8089975118637085, 'eval_runtime': 2.1272, 'eval_samples_per_second': 47.01, 'eval_steps_per_second': 23.505, 'epoch': 1.24, 'num_input_tokens_seen': 282776}\n",
      " 62%|█████████████████████████▋               | 140/224 [03:57<01:51,  1.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.84it/s]\u001b[A\n",
      "{'loss': 0.612, 'grad_norm': 8.322758674621582, 'learning_rate': 1.4788893576600099e-05, 'epoch': 1.29, 'num_input_tokens_seen': 294080}\n",
      "{'loss': 0.6887, 'grad_norm': 11.127881050109863, 'learning_rate': 1.3215081579350058e-05, 'epoch': 1.33, 'num_input_tokens_seen': 304632}\n",
      " 67%|███████████████████████████▍             | 150/224 [04:10<01:35,  1.29s/it][INFO|trainer.py:3788] 2024-07-26 04:55:04,380 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:55:04,380 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:55:04,380 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 29.19it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 22.69it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 21.87it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 20.71it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 20.72it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 22.38it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 23.08it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 22.53it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:00, 23.27it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 23.83it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 22.99it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.59it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 24.00it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 22.21it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 22.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8047587871551514, 'eval_runtime': 2.2522, 'eval_samples_per_second': 44.401, 'eval_steps_per_second': 22.201, 'epoch': 1.33, 'num_input_tokens_seen': 304632}\n",
      " 67%|███████████████████████████▍             | 150/224 [04:12<01:35,  1.29s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 22.65it/s]\u001b[A\n",
      "{'loss': 0.5072, 'grad_norm': 6.981425762176514, 'learning_rate': 1.1699198087116589e-05, 'epoch': 1.38, 'num_input_tokens_seen': 315632}\n",
      "{'loss': 0.5278, 'grad_norm': 8.06922721862793, 'learning_rate': 1.0248694391571801e-05, 'epoch': 1.42, 'num_input_tokens_seen': 324680}\n",
      " 71%|█████████████████████████████▎           | 160/224 [04:25<01:21,  1.27s/it][INFO|trainer.py:3788] 2024-07-26 04:55:19,210 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:55:19,210 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:55:19,210 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.54it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 23.77it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 23.80it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 21.79it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 21.11it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 21.88it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 21.69it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.02it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 21.35it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 21.70it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 22.79it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 23.26it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 22.63it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.50it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 23.89it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8345295190811157, 'eval_runtime': 2.2731, 'eval_samples_per_second': 43.993, 'eval_steps_per_second': 21.997, 'epoch': 1.42, 'num_input_tokens_seen': 324680}\n",
      " 71%|█████████████████████████████▎           | 160/224 [04:27<01:21,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 22.44it/s]\u001b[A\n",
      "{'loss': 0.6558, 'grad_norm': 9.861849784851074, 'learning_rate': 8.870700411447816e-06, 'epoch': 1.47, 'num_input_tokens_seen': 335656}\n",
      "{'loss': 0.4324, 'grad_norm': 7.068021297454834, 'learning_rate': 7.571989645579419e-06, 'epoch': 1.51, 'num_input_tokens_seen': 343224}\n",
      " 76%|███████████████████████████████          | 170/224 [04:40<01:11,  1.33s/it][INFO|trainer.py:3788] 2024-07-26 04:55:34,186 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:55:34,186 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:55:34,186 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 30.54it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.32it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.15it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 23.33it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 22.87it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 23.42it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 22.02it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 22.38it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.01it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 22.52it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 22.79it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 23.06it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 22.53it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.36it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 23.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8506081104278564, 'eval_runtime': 2.2013, 'eval_samples_per_second': 45.429, 'eval_steps_per_second': 22.714, 'epoch': 1.51, 'num_input_tokens_seen': 343224}\n",
      " 76%|███████████████████████████████          | 170/224 [04:42<01:11,  1.33s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.95it/s]\u001b[A\n",
      "{'loss': 0.6144, 'grad_norm': 13.28073787689209, 'learning_rate': 6.358945877920861e-06, 'epoch': 1.56, 'num_input_tokens_seen': 353536}\n",
      "{'loss': 0.587, 'grad_norm': 9.330925941467285, 'learning_rate': 5.237531798197415e-06, 'epoch': 1.6, 'num_input_tokens_seen': 364320}\n",
      " 80%|████████████████████████████████▉        | 180/224 [04:55<00:55,  1.27s/it][INFO|trainer.py:3788] 2024-07-26 04:55:49,007 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:55:49,007 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:55:49,007 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 30.94it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.52it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 26.44it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.96it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 25.55it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 25.62it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 25.02it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 25.85it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 25.89it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 26.02it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 26.30it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 26.31it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 26.19it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.68it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.25it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8344694375991821, 'eval_runtime': 2.0133, 'eval_samples_per_second': 49.67, 'eval_steps_per_second': 24.835, 'epoch': 1.6, 'num_input_tokens_seen': 364320}\n",
      " 80%|████████████████████████████████▉        | 180/224 [04:57<00:55,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 24.64it/s]\u001b[A\n",
      "{'loss': 0.4814, 'grad_norm': 5.994381904602051, 'learning_rate': 4.213259692436367e-06, 'epoch': 1.64, 'num_input_tokens_seen': 373800}\n",
      "{'loss': 0.4893, 'grad_norm': 6.605400562286377, 'learning_rate': 3.2911643474473646e-06, 'epoch': 1.69, 'num_input_tokens_seen': 382768}\n",
      " 85%|██████████████████████████████████▊      | 190/224 [05:11<00:47,  1.39s/it][INFO|trainer.py:3788] 2024-07-26 04:56:05,023 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:56:05,023 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:56:05,023 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 31.35it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.53it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.50it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.65it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 25.21it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 25.25it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 24.93it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 25.70it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 26.08it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 25.92it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 25.49it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 25.13it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 25.07it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.00it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 24.87it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8294169902801514, 'eval_runtime': 2.0331, 'eval_samples_per_second': 49.186, 'eval_steps_per_second': 24.593, 'epoch': 1.69, 'num_input_tokens_seen': 382768}\n",
      " 85%|██████████████████████████████████▊      | 190/224 [05:13<00:47,  1.39s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 24.75it/s]\u001b[A\n",
      "{'loss': 0.4922, 'grad_norm': 8.18988037109375, 'learning_rate': 2.475778302439524e-06, 'epoch': 1.73, 'num_input_tokens_seen': 392456}\n",
      "{'loss': 0.6036, 'grad_norm': 7.984795570373535, 'learning_rate': 1.771109569425547e-06, 'epoch': 1.78, 'num_input_tokens_seen': 402192}\n",
      " 89%|████████████████████████████████████▌    | 200/224 [05:25<00:31,  1.30s/it][INFO|trainer.py:3788] 2024-07-26 04:56:19,670 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:56:19,670 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:56:19,670 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.81it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.80it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 28.10it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 26.37it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 25.79it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 25.89it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 25.36it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:00<00:00, 26.18it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 26.51it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 26.63it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 26.75it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 26.81it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 26.27it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.73it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.52it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8287664651870728, 'eval_runtime': 1.9576, 'eval_samples_per_second': 51.083, 'eval_steps_per_second': 25.542, 'epoch': 1.78, 'num_input_tokens_seen': 402192}\n",
      " 89%|████████████████████████████████████▌    | 200/224 [05:27<00:31,  1.30s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 25.32it/s]\u001b[A\n",
      "{'loss': 0.4476, 'grad_norm': 10.56456184387207, 'learning_rate': 1.180621931927592e-06, 'epoch': 1.82, 'num_input_tokens_seen': 411768}\n",
      "{'loss': 0.5649, 'grad_norm': 11.559589385986328, 'learning_rate': 7.072179188262251e-07, 'epoch': 1.87, 'num_input_tokens_seen': 421440}\n",
      " 94%|██████████████████████████████████████▍  | 210/224 [05:40<00:17,  1.27s/it][INFO|trainer.py:3788] 2024-07-26 04:56:33,967 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:56:33,967 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:56:33,967 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.93it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.77it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 27.77it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 26.64it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 26.96it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 26.36it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 25.64it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:00<00:00, 26.00it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 25.95it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 26.09it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 26.23it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 26.18it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 26.08it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 26.10it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.76it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8308138847351074, 'eval_runtime': 1.9522, 'eval_samples_per_second': 51.224, 'eval_steps_per_second': 25.612, 'epoch': 1.87, 'num_input_tokens_seen': 421440}\n",
      " 94%|██████████████████████████████████████▍  | 210/224 [05:42<00:17,  1.27s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 25.64it/s]\u001b[A\n",
      "{'loss': 0.7165, 'grad_norm': 8.98772144317627, 'learning_rate': 3.5322453704410286e-07, 'epoch': 1.91, 'num_input_tokens_seen': 433784}\n",
      "{'loss': 0.5738, 'grad_norm': 55.54294204711914, 'learning_rate': 1.2038183319507955e-07, 'epoch': 1.96, 'num_input_tokens_seen': 443192}\n",
      " 98%|████████████████████████████████████████▎| 220/224 [05:54<00:05,  1.30s/it][INFO|trainer.py:3788] 2024-07-26 04:56:48,492 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:56:48,492 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:56:48,492 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 30.69it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 25.65it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 25.87it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 24.24it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.88it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 25.17it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 24.78it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:00, 25.54it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 25.84it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 25.97it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 26.16it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 26.24it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 25.58it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 25.32it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 25.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.8307888507843018, 'eval_runtime': 2.0263, 'eval_samples_per_second': 49.352, 'eval_steps_per_second': 24.676, 'epoch': 1.96, 'num_input_tokens_seen': 443192}\n",
      " 98%|████████████████████████████████████████▎| 220/224 [05:56<00:05,  1.30s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:01<00:00, 25.02it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 224/224 [06:01<00:00,  1.48s/it]\u001b[A[INFO|trainer.py:3478] 2024-07-26 04:56:55,639 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 04:56:55,655 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 04:56:55,664 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 04:57:04,736 >> Model weights saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:57:04,748 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:57:04,755 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/checkpoint-224/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 04:57:20,680 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 386.9986, 'train_samples_per_second': 4.651, 'train_steps_per_second': 0.579, 'train_loss': 0.978430052953107, 'epoch': 1.99, 'num_input_tokens_seen': 452168}\n",
      "100%|█████████████████████████████████████████| 224/224 [06:27<00:00,  1.73s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 04:57:20,691 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 04:57:20,715 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 04:57:20,726 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 04:57:33,338 >> Model weights saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 04:57:33,352 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 04:57:33,366 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9911\n",
      "  num_input_tokens_seen    =     452168\n",
      "  total_flos               =   904296GF\n",
      "  train_loss               =     0.9784\n",
      "  train_runtime            = 0:06:26.99\n",
      "  train_samples_per_second =      4.651\n",
      "  train_steps_per_second   =      0.579\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 04:57:34,669 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:57:34,670 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:57:34,670 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 22.20it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9911\n",
      "  eval_loss               =     1.8306\n",
      "  eval_runtime            = 0:00:02.32\n",
      "  eval_samples_per_second =      43.07\n",
      "  eval_steps_per_second   =     21.535\n",
      "  num_input_tokens_seen   =     452168\n",
      "[INFO|modelcard.py:449] 2024-07-26 04:57:37,019 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type full \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_full_bz=1 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b8de4-5757-4496-a21a-ab4029fe2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-26 04:58:26,089] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Using config file: /etc/orion/env/env.conf\n",
      "07/26/2024 04:58:45 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2159] 2024-07-26 04:58:45,737 >> loading file tokenizer_config.json\n",
      "[WARNING|logging.py:313] 2024-07-26 04:58:46,129 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "07/26/2024 04:58:46 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\n",
      "07/26/2024 04:58:46 - INFO - llamafactory.data.loader - Loading dataset MedQA/train.json...\n",
      "07/26/2024 04:58:52 - INFO - llamafactory.data.loader - Loading dataset PubMedQA/pqal_train_set.json...\n",
      "input_ids:\n",
      "[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16141, 279, 2701, 5248, 5754, 3405, 198, 32, 25, 53687, 292, 60497, 11, 425, 25, 356, 823, 376, 685, 87, 603, 11, 356, 25, 356, 48789, 69, 55728, 39388, 11, 422, 25, 3155, 87, 3337, 6179, 482, 11, 468, 25, 49516, 299, 82901, 517, 1961, 151645, 198, 151644, 77091, 198, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "inputs:\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the following multiple choice question\n",
      "A: Ampicillin, B: Ceftriaxone, C: Ciprofloxacin, D: Doxycycline, E: Nitrofurantoin<|im_end|>\n",
      "<|im_start|>assistant\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 36, 25, 49516, 299, 82901, 517, 1961, 151645]\n",
      "labels:\n",
      "E: Nitrofurantoin<|im_end|>\n",
      "[INFO|configuration_utils.py:731] 2024-07-26 04:58:53,101 >> loading configuration file model/Qwen2-0.5B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-07-26 04:58:53,109 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"model/Qwen2-0.5B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 896,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4864,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 24,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 14,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.42.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3553] 2024-07-26 04:58:53,536 >> loading weights file model/Qwen2-0.5B-Instruct/model.safetensors\n",
      "[INFO|modeling_utils.py:1531] 2024-07-26 04:58:54,211 >> Instantiating Qwen2ForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:58:54,217 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:4364] 2024-07-26 04:59:06,295 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4372] 2024-07-26 04:59:06,295 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at model/Qwen2-0.5B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:953] 2024-07-26 04:59:06,306 >> loading configuration file model/Qwen2-0.5B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1000] 2024-07-26 04:59:06,306 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.1,\n",
      "  \"temperature\": 0.7,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.8\n",
      "}\n",
      "\n",
      "07/26/2024 04:59:06 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "07/26/2024 04:59:06 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "07/26/2024 04:59:06 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "07/26/2024 04:59:06 - INFO - llamafactory.model.adapter - Fine-tuning method: Full\n",
      "07/26/2024 04:59:06 - INFO - llamafactory.model.loader - trainable params: 494032768 || all params: 494032768 || trainable%: 100.0000\n",
      "[INFO|trainer.py:642] 2024-07-26 04:59:06,606 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2128] 2024-07-26 04:59:07,365 >> ***** Running training *****\n",
      "[INFO|trainer.py:2129] 2024-07-26 04:59:07,365 >>   Num examples = 900\n",
      "[INFO|trainer.py:2130] 2024-07-26 04:59:07,365 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:2131] 2024-07-26 04:59:07,365 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2134] 2024-07-26 04:59:07,365 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2135] 2024-07-26 04:59:07,365 >>   Gradient Accumulation steps = 8\n",
      "[INFO|trainer.py:2136] 2024-07-26 04:59:07,365 >>   Total optimization steps = 112\n",
      "[INFO|trainer.py:2137] 2024-07-26 04:59:07,367 >>   Number of trainable parameters = 494,032,768\n",
      "{'loss': 2.0505, 'grad_norm': 30.88935661315918, 'learning_rate': 4.991153735294049e-05, 'epoch': 0.09, 'num_input_tokens_seen': 28048}\n",
      "{'loss': 1.584, 'grad_norm': 16.853227615356445, 'learning_rate': 4.937319780454559e-05, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:23<02:36,  1.53s/it][INFO|trainer.py:3788] 2024-07-26 04:59:30,952 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:59:30,952 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:59:30,952 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 27.06it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 23.03it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 22.34it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 21.78it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 18.82it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 19.33it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 19.73it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 19.37it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 19.94it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 20.30it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 20.62it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 21.16it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 21.33it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:02<00:00, 21.09it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:02<00:00, 21.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6240826845169067, 'eval_runtime': 2.4557, 'eval_samples_per_second': 40.721, 'eval_steps_per_second': 20.361, 'epoch': 0.18, 'num_input_tokens_seen': 56096}\n",
      "  9%|███▊                                      | 10/112 [00:26<02:36,  1.53s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 21.65it/s]\u001b[A\n",
      "{'loss': 1.7394, 'grad_norm': 13.999152183532715, 'learning_rate': 4.8356223507364996e-05, 'epoch': 0.27, 'num_input_tokens_seen': 84720}\n",
      "{'loss': 1.6118, 'grad_norm': 12.423261642456055, 'learning_rate': 4.6880585547718845e-05, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:39<02:04,  1.36s/it][INFO|trainer.py:3788] 2024-07-26 04:59:46,588 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 04:59:46,589 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 04:59:46,589 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.18it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 24.59it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 23.26it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.43it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 22.33it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 20.98it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:01<00:01, 20.02it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 20.56it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:01, 21.68it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 21.61it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 21.91it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 22.61it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 22.97it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.52it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:02<00:00, 23.91it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.619268774986267, 'eval_runtime': 2.2815, 'eval_samples_per_second': 43.831, 'eval_steps_per_second': 21.915, 'epoch': 0.36, 'num_input_tokens_seen': 112400}\n",
      " 18%|███████▌                                  | 20/112 [00:41<02:04,  1.36s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.78it/s]\u001b[A\n",
      "{'loss': 1.4737, 'grad_norm': 15.460563659667969, 'learning_rate': 4.497526213395623e-05, 'epoch': 0.44, 'num_input_tokens_seen': 138464}\n",
      "{'loss': 1.5053, 'grad_norm': 10.68559455871582, 'learning_rate': 4.267766952966369e-05, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:56<02:00,  1.47s/it][INFO|trainer.py:3788] 2024-07-26 05:00:03,575 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:00:03,576 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:00:03,576 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 28.40it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 22.37it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 21.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 21.53it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 21.53it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 22.85it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 22.99it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 22.10it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 21.82it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 21.71it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 22.65it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 22.98it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 23.38it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 23.93it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 24.50it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.624289870262146, 'eval_runtime': 2.3345, 'eval_samples_per_second': 42.836, 'eval_steps_per_second': 21.418, 'epoch': 0.53, 'num_input_tokens_seen': 163552}\n",
      " 27%|███████████▎                              | 30/112 [00:58<02:00,  1.47s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.55it/s]\u001b[A\n",
      "{'loss': 1.8156, 'grad_norm': 11.386603355407715, 'learning_rate': 4.0032927282460146e-05, 'epoch': 0.62, 'num_input_tokens_seen': 194368}\n",
      "{'loss': 1.3631, 'grad_norm': 8.228558540344238, 'learning_rate': 3.7092972177631e-05, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [01:12<01:41,  1.41s/it][INFO|trainer.py:3788] 2024-07-26 05:00:20,173 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:00:20,174 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:00:20,174 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 32.32it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 27.44it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 27.05it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 25.71it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 24.19it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 22.07it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 20.85it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 22.52it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.11it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.37it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 23.88it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 24.34it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 24.32it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 23.63it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:01<00:00, 23.71it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.638051152229309, 'eval_runtime': 2.1569, 'eval_samples_per_second': 46.363, 'eval_steps_per_second': 23.182, 'epoch': 0.71, 'num_input_tokens_seen': 218544}\n",
      " 36%|███████████████                           | 40/112 [01:14<01:41,  1.41s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.28it/s]\u001b[A\n",
      "{'loss': 1.3717, 'grad_norm': 11.68851089477539, 'learning_rate': 3.391553831655782e-05, 'epoch': 0.8, 'num_input_tokens_seen': 243872}\n",
      "{'loss': 1.623, 'grad_norm': 9.283585548400879, 'learning_rate': 3.056302334890786e-05, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:29<01:33,  1.52s/it][INFO|trainer.py:3788] 2024-07-26 05:00:36,904 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:00:36,904 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:00:36,904 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 29.95it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 24.95it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 23.45it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 22.41it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 21.55it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 21.37it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 21.73it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 21.40it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 21.86it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 22.18it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 23.19it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 24.09it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 24.44it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 24.59it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 24.43it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6173748970031738, 'eval_runtime': 2.263, 'eval_samples_per_second': 44.19, 'eval_steps_per_second': 22.095, 'epoch': 0.89, 'num_input_tokens_seen': 274544}\n",
      " 45%|██████████████████▊                       | 50/112 [01:31<01:33,  1.52s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.04it/s]\u001b[A\n",
      "{'loss': 1.7099, 'grad_norm': 10.152897834777832, 'learning_rate': 2.710126312323119e-05, 'epoch': 0.98, 'num_input_tokens_seen': 305584}\n",
      "{'loss': 1.1595, 'grad_norm': 7.220686912536621, 'learning_rate': 2.3598238819070202e-05, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:46<01:18,  1.51s/it][INFO|trainer.py:3788] 2024-07-26 05:00:54,229 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:00:54,229 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:00:54,229 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 28.84it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 24.80it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 24.42it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.10it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 21.92it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 22.74it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 23.01it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.65it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 22.51it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 23.48it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 23.94it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 23.51it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 23.33it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.75it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 23.28it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.609095811843872, 'eval_runtime': 2.2076, 'eval_samples_per_second': 45.298, 'eval_steps_per_second': 22.649, 'epoch': 1.07, 'num_input_tokens_seen': 335024}\n",
      " 54%|██████████████████████▌                   | 60/112 [01:49<01:18,  1.51s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.68it/s]\u001b[A\n",
      "{'loss': 0.856, 'grad_norm': 8.586162567138672, 'learning_rate': 2.0122741949596797e-05, 'epoch': 1.16, 'num_input_tokens_seen': 365600}\n",
      "{'loss': 0.7551, 'grad_norm': 9.922621726989746, 'learning_rate': 1.6743023451120832e-05, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [02:03<01:01,  1.47s/it][INFO|trainer.py:3788] 2024-07-26 05:01:11,049 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:01:11,049 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:01:11,049 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 25.87it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 24.10it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 24.53it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 24.06it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 23.61it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 24.02it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:00<00:01, 23.55it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 22.26it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 21.82it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 22.76it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 23.33it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.05it/s]\u001b[A\n",
      " 78%|█████████████████████████████████▌         | 39/50 [00:01<00:00, 22.89it/s]\u001b[A\n",
      " 84%|████████████████████████████████████       | 42/50 [00:01<00:00, 23.50it/s]\u001b[A\n",
      " 90%|██████████████████████████████████████▋    | 45/50 [00:01<00:00, 23.43it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7808483839035034, 'eval_runtime': 2.2128, 'eval_samples_per_second': 45.192, 'eval_steps_per_second': 22.596, 'epoch': 1.24, 'num_input_tokens_seen': 392224}\n",
      " 62%|██████████████████████████▎               | 70/112 [02:05<01:01,  1.47s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 23.12it/s]\u001b[A\n",
      "{'loss': 0.7733, 'grad_norm': 26.285140991210938, 'learning_rate': 1.3525453388311554e-05, 'epoch': 1.33, 'num_input_tokens_seen': 421712}\n",
      "{'loss': 0.7111, 'grad_norm': 7.562676906585693, 'learning_rate': 1.0533217595504858e-05, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [02:20<00:48,  1.50s/it][INFO|trainer.py:3788] 2024-07-26 05:01:28,158 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:01:28,158 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:01:28,158 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.02it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 24.04it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 23.57it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 22.83it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 23.54it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 23.51it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 22.62it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:01<00:01, 22.45it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 22.83it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 23.88it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 24.48it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 24.67it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 24.08it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 22.83it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.7083507776260376, 'eval_runtime': 2.3174, 'eval_samples_per_second': 43.153, 'eval_steps_per_second': 21.576, 'epoch': 1.42, 'num_input_tokens_seen': 451744}\n",
      " 71%|██████████████████████████████            | 80/112 [02:22<00:48,  1.50s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 21.81it/s]\u001b[A\n",
      "{'loss': 0.7243, 'grad_norm': 7.27877950668335, 'learning_rate': 7.825076849127458e-06, 'epoch': 1.51, 'num_input_tokens_seen': 478672}\n",
      "{'loss': 0.7325, 'grad_norm': 8.994869232177734, 'learning_rate': 5.454212938299255e-06, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [02:38<00:34,  1.58s/it][INFO|trainer.py:3788] 2024-07-26 05:01:45,702 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:01:45,702 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:01:45,702 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 29.77it/s]\u001b[A\n",
      " 14%|██████▏                                     | 7/50 [00:00<00:01, 27.04it/s]\u001b[A\n",
      " 20%|████████▌                                  | 10/50 [00:00<00:01, 27.38it/s]\u001b[A\n",
      " 26%|███████████▏                               | 13/50 [00:00<00:01, 25.40it/s]\u001b[A\n",
      " 32%|█████████████▊                             | 16/50 [00:00<00:01, 25.16it/s]\u001b[A\n",
      " 38%|████████████████▎                          | 19/50 [00:00<00:01, 24.55it/s]\u001b[A\n",
      " 44%|██████████████████▉                        | 22/50 [00:00<00:01, 24.90it/s]\u001b[A\n",
      " 50%|█████████████████████▌                     | 25/50 [00:00<00:00, 25.14it/s]\u001b[A\n",
      " 56%|████████████████████████                   | 28/50 [00:01<00:00, 24.79it/s]\u001b[A\n",
      " 62%|██████████████████████████▋                | 31/50 [00:01<00:00, 24.62it/s]\u001b[A\n",
      " 68%|█████████████████████████████▏             | 34/50 [00:01<00:00, 23.92it/s]\u001b[A\n",
      " 74%|███████████████████████████████▊           | 37/50 [00:01<00:00, 23.57it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 23.72it/s]\u001b[A\n",
      " 86%|████████████████████████████████████▉      | 43/50 [00:01<00:00, 24.32it/s]\u001b[A\n",
      " 92%|███████████████████████████████████████▌   | 46/50 [00:01<00:00, 24.58it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.699965476989746, 'eval_runtime': 2.0947, 'eval_samples_per_second': 47.74, 'eval_steps_per_second': 23.87, 'epoch': 1.6, 'num_input_tokens_seen': 507008}\n",
      " 80%|█████████████████████████████████▊        | 90/112 [02:40<00:34,  1.58s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 24.22it/s]\u001b[A\n",
      "{'loss': 0.7125, 'grad_norm': 5.933475971221924, 'learning_rate': 3.4671842941897765e-06, 'epoch': 1.69, 'num_input_tokens_seen': 533296}\n",
      "{'loss': 0.6719, 'grad_norm': 6.530492305755615, 'learning_rate': 1.9030116872178316e-06, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:55<00:18,  1.55s/it][INFO|trainer.py:3788] 2024-07-26 05:02:02,910 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:02:02,910 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:02:02,910 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|██▋                                         | 3/50 [00:00<00:01, 25.70it/s]\u001b[A\n",
      " 12%|█████▎                                      | 6/50 [00:00<00:01, 22.57it/s]\u001b[A\n",
      " 18%|███████▉                                    | 9/50 [00:00<00:01, 20.92it/s]\u001b[A\n",
      " 24%|██████████▎                                | 12/50 [00:00<00:01, 20.11it/s]\u001b[A\n",
      " 30%|████████████▉                              | 15/50 [00:00<00:01, 19.42it/s]\u001b[A\n",
      " 36%|███████████████▍                           | 18/50 [00:00<00:01, 19.84it/s]\u001b[A\n",
      " 42%|██████████████████                         | 21/50 [00:01<00:01, 19.90it/s]\u001b[A\n",
      " 48%|████████████████████▋                      | 24/50 [00:01<00:01, 20.11it/s]\u001b[A\n",
      " 54%|███████████████████████▏                   | 27/50 [00:01<00:01, 20.66it/s]\u001b[A\n",
      " 60%|█████████████████████████▊                 | 30/50 [00:01<00:00, 21.74it/s]\u001b[A\n",
      " 66%|████████████████████████████▍              | 33/50 [00:01<00:00, 22.78it/s]\u001b[A\n",
      " 72%|██████████████████████████████▉            | 36/50 [00:01<00:00, 23.31it/s]\u001b[A\n",
      " 80%|██████████████████████████████████▍        | 40/50 [00:01<00:00, 25.35it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 26.70it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 26.70it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.698246955871582, 'eval_runtime': 2.2559, 'eval_samples_per_second': 44.328, 'eval_steps_per_second': 22.164, 'epoch': 1.78, 'num_input_tokens_seen': 559536}\n",
      " 89%|████████████████████████████████████▌    | 100/112 [02:57<00:18,  1.55s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 26.03it/s]\u001b[A\n",
      "{'loss': 0.624, 'grad_norm': 9.534947395324707, 'learning_rate': 7.924119469434665e-07, 'epoch': 1.87, 'num_input_tokens_seen': 587040}\n",
      "{'loss': 0.84, 'grad_norm': 10.372885704040527, 'learning_rate': 1.571947526689349e-07, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [03:12<00:03,  1.52s/it][INFO|trainer.py:3788] 2024-07-26 05:02:19,834 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:02:19,834 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:02:19,834 >>   Batch size = 2\n",
      "\n",
      "  0%|                                                    | 0/50 [00:00<?, ?it/s]\u001b[A\n",
      "  8%|███▌                                        | 4/50 [00:00<00:01, 30.78it/s]\u001b[A\n",
      " 16%|███████                                     | 8/50 [00:00<00:01, 26.34it/s]\u001b[A\n",
      " 22%|█████████▍                                 | 11/50 [00:00<00:01, 26.77it/s]\u001b[A\n",
      " 28%|████████████                               | 14/50 [00:00<00:01, 25.49it/s]\u001b[A\n",
      " 34%|██████████████▌                            | 17/50 [00:00<00:01, 25.30it/s]\u001b[A\n",
      " 40%|█████████████████▏                         | 20/50 [00:00<00:01, 24.38it/s]\u001b[A\n",
      " 46%|███████████████████▊                       | 23/50 [00:00<00:01, 23.47it/s]\u001b[A\n",
      " 52%|██████████████████████▎                    | 26/50 [00:01<00:01, 23.58it/s]\u001b[A\n",
      " 58%|████████████████████████▉                  | 29/50 [00:01<00:00, 23.35it/s]\u001b[A\n",
      " 64%|███████████████████████████▌               | 32/50 [00:01<00:00, 23.17it/s]\u001b[A\n",
      " 70%|██████████████████████████████             | 35/50 [00:01<00:00, 22.63it/s]\u001b[A\n",
      " 76%|████████████████████████████████▋          | 38/50 [00:01<00:00, 22.61it/s]\u001b[A\n",
      " 82%|███████████████████████████████████▎       | 41/50 [00:01<00:00, 23.02it/s]\u001b[A\n",
      " 88%|█████████████████████████████████████▊     | 44/50 [00:01<00:00, 22.46it/s]\u001b[A\n",
      " 94%|████████████████████████████████████████▍  | 47/50 [00:02<00:00, 21.74it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 1.6999499797821045, 'eval_runtime': 2.2052, 'eval_samples_per_second': 45.348, 'eval_steps_per_second': 22.674, 'epoch': 1.96, 'num_input_tokens_seen': 618448}\n",
      " 98%|████████████████████████████████████████▎| 110/112 [03:14<00:03,  1.52s/it]\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 21.82it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 112/112 [03:17<00:00,  1.94s/it]\u001b[A[INFO|trainer.py:3478] 2024-07-26 05:02:24,891 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 05:02:24,919 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 05:02:24,933 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 05:02:37,165 >> Model weights saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:02:37,180 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:02:37,193 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/checkpoint-112/special_tokens_map.json\n",
      "[INFO|trainer.py:2383] 2024-07-26 05:03:00,539 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 233.173, 'train_samples_per_second': 7.72, 'train_steps_per_second': 0.48, 'train_loss': 1.1932241288678986, 'epoch': 1.99, 'num_input_tokens_seen': 631296}\n",
      "100%|█████████████████████████████████████████| 112/112 [03:53<00:00,  2.08s/it]\n",
      "[INFO|trainer.py:3478] 2024-07-26 05:03:00,552 >> Saving model checkpoint to saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2\n",
      "[INFO|configuration_utils.py:472] 2024-07-26 05:03:00,576 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/config.json\n",
      "[INFO|configuration_utils.py:769] 2024-07-26 05:03:00,590 >> Configuration saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/generation_config.json\n",
      "[INFO|modeling_utils.py:2690] 2024-07-26 05:03:14,052 >> Model weights saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/model.safetensors\n",
      "[INFO|tokenization_utils_base.py:2574] 2024-07-26 05:03:14,088 >> tokenizer config file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2583] 2024-07-26 05:03:14,102 >> Special tokens file saved in saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =     1.9911\n",
      "  num_input_tokens_seen    =     631296\n",
      "  total_flos               =  1262536GF\n",
      "  train_loss               =     1.1932\n",
      "  train_runtime            = 0:03:53.17\n",
      "  train_samples_per_second =       7.72\n",
      "  train_steps_per_second   =       0.48\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/training_loss.png\n",
      "Figure saved at: saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2/training_eval_loss.png\n",
      "[INFO|trainer.py:3788] 2024-07-26 05:03:15,322 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:3790] 2024-07-26 05:03:15,322 >>   Num examples = 100\n",
      "[INFO|trainer.py:3793] 2024-07-26 05:03:15,322 >>   Batch size = 2\n",
      "100%|███████████████████████████████████████████| 50/50 [00:02<00:00, 21.29it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =     1.9911\n",
      "  eval_loss               =        1.7\n",
      "  eval_runtime            = 0:00:02.40\n",
      "  eval_samples_per_second =      41.56\n",
      "  eval_steps_per_second   =      20.78\n",
      "  num_input_tokens_seen   =     631296\n",
      "[INFO|modelcard.py:449] 2024-07-26 05:03:17,744 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli train \\\n",
    "    --stage sft \\\n",
    "    --do_train True \\\n",
    "    --model_name_or_path model/Qwen2-0.5B-Instruct \\\n",
    "    --preprocessing_num_workers 16 \\\n",
    "    --finetuning_type full \\\n",
    "    --template qwen \\\n",
    "    --flash_attn auto \\\n",
    "    --dataset_dir data \\\n",
    "    --dataset MedQA_train,PubMedQA_pqal_train \\\n",
    "    --cutoff_len 1024 \\\n",
    "    --learning_rate 5e-05 \\\n",
    "    --num_train_epochs 2.0 \\\n",
    "    --max_samples 500 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 8 \\\n",
    "    --lr_scheduler_type cosine \\\n",
    "    --max_grad_norm 1.0 \\\n",
    "    --logging_steps 5 \\\n",
    "    --save_steps 1000 \\\n",
    "    --warmup_steps 0 \\\n",
    "    --optim adamw_torch \\\n",
    "    --packing False \\\n",
    "    --report_to none \\\n",
    "    --output_dir saves/Qwen2-0.5B-Instruct/bz/train_full_bz=2 \\\n",
    "    --fp16 True \\\n",
    "    --plot_loss True \\\n",
    "    --ddp_timeout 180000000 \\\n",
    "    --include_num_input_tokens_seen True \\\n",
    "    --val_size 0.1 \\\n",
    "    --eval_strategy steps \\\n",
    "    --eval_steps 10 \\\n",
    "    --per_device_eval_batch_size 2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
